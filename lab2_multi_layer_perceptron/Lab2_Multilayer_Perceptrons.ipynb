{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvFXBRxWElea"
      },
      "source": [
        "## Objectives:\n",
        "\n",
        "- Quickly Understand Backpropagation in a Single Neuron\n",
        "- Implement Multi-Layer Perceptrons with Backpropagation algorithm using PyTorch\n",
        "\n",
        "\\\n",
        "\n",
        "## Prerequisites:\n",
        "\n",
        "1. Perceptrons (Single neuron)\n",
        "\n",
        "2. Backpropagation and Computational Graphs\n",
        "\n",
        "3. Basic Knowledge of PyTorch (and/or TensorFlow)\n",
        "\n",
        "\n",
        "    Reference: https://github.com/SunlightWings/Machine-Learning-and-Data-Mining-/blob/main/ANN/PyTorch_ANN.ipynb\n",
        "\n",
        "\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGbLojKhMmeh"
      },
      "source": [
        "# Part A: Understanding Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnrk9UD-GKtn"
      },
      "source": [
        "## Computational Graphs:\n",
        "\n",
        "Components:\n",
        "  * Nodes: Represent operations or variables.\n",
        "  * Edges: Represent the flow of data and carry weights.\n",
        "\n",
        "**Illustration**:\n",
        "\n",
        "Lets consider a simple equation, that is represented in the form of  computational graph as below:\n",
        "\n",
        "<p align=\"center\">s = 2x</p>\n",
        "<p align=\"center\">y = s + a</p>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://doc.google.com/a/fusemachines.com/uc?id=1_WwWjWXvT6b31TS402a9tVe7EjZw_Exu\">\n",
        "\n",
        "Figure 1: Computational graph\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvOqz7MCJSg_"
      },
      "source": [
        "**Illustration**:\n",
        "Problem: Logistic regression with two inputs variables $x_1$ and $x_2$.\n",
        "\n",
        "* $z$ is weighted sum of input plus bias,\n",
        "* $\\hat{y}$ is the output of a sigmoid function and\n",
        "* $y$ is the label for a training example which are finally feed to Binary cross entropy loss.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=14MXS74SqI0ouBzvtwgyLTA8v93g66mRG\" alt=\"ford_prop\" height=\"400\" width = \"900\">\n",
        "\n",
        "Figure 3: Computational Graph for Logistic regression\n",
        "</center>\n",
        "\n",
        "We have,\n",
        "\n",
        "$$z = 1 * w_0 + x_1 * w_1 + x_2 * w_2$$\n",
        "\n",
        "$$\\hat{y} = \\frac{1}{1 + e^{-(1 * w_0 + x_1 * w_1 + x_2 * w_2)}}$$\n",
        "\n",
        "$$Loss (L) = - ( y \\log{\\hat{y}} + (1 - y) \\log{(1-\\hat{y})})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O6cTfq5JrZW"
      },
      "source": [
        "## Backprop Using Chain Rule:\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://doc.google.com/a/fusemachines.com/uc?export=download&id=1RFe8TVQyTZcg41W1Gc64rbE3u_VX2GfU\" alt=\"chain_rule\" height=\"350\">\n",
        "\n",
        "Figure 5: Chain Rule for Logistic Regression.\n",
        "</center>\n",
        "\n",
        "\n",
        "1. **Known Gradients** = Local Gradients = $\\frac{\\partial{L}}{\\partial{ŷ}}$, $\\frac{\\partial{ŷ}}{\\partial{Z}}$, ( $\\frac{\\partial{Z}}{\\partial{w0}}$, $\\frac{\\partial{Z}}{\\partial{w1}}$, $\\frac{\\partial{Z}}{\\partial{w2}}$ )\n",
        "\n",
        "2. **To Find** = Final Loss with respect to local inputs (weights) = $\\frac{\\partial{L}}{\\partial{ŷ}}$, $\\frac{\\partial{L}}{\\partial{Z}}$, ( $\\frac{\\partial{L}}{\\partial{w0}}$, $\\frac{\\partial{L}}{\\partial{w1}}$, $\\frac{\\partial{L}}{\\partial{w2}}$ )\n",
        "\n",
        "3. **Chain Rule** = Use point (1) to calculate point (2) as shown in Figure (5).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xovarRHKkMX"
      },
      "source": [
        "## Calculation:\n",
        "\n",
        "**Task1: Calculate the gradients**\n",
        "\n",
        "Lets calculate all the local gradients.\n",
        "\n",
        "1. Loss w.r.t. output:\n",
        "\n",
        "$\\frac{\\partial{L}}{\\partial{\\hat{y}}}=\\frac{\\partial{(-(y\\log{\\hat{y}}+(1-y))\\log{(1-\\hat{y})})}}{\\partial{\\hat{y}}}$\n",
        "\n",
        "$\\therefore \\frac{\\partial{L}}{\\partial{\\hat{y}}}=-(\\frac{y}{\\hat{y}}-\\frac{(1-y)}{(1-\\hat{y})})$\n",
        "\n",
        "\\\n",
        "\n",
        "2. Output w.r.t. z:\n",
        "\n",
        "$\\frac{\\partial{\\hat{y}}}{\\partial{z}}=\\frac{\\partial({\\frac{1}{1+e^{-z}}})}{\\partial{z}}$        (Since, $\\hat{y}=\\frac{1}{1+e^{-z}}$)\n",
        "\n",
        "$\\frac{\\partial{\\hat{y}}}{\\partial{z}}=\\frac{(1+e^{-z})*\\frac{\\partial{(1)}}{\\partial{z}}-\\frac{\\partial{(1+e^{-z})}}{\\partial{z}}*1}{(1+e^{-z})^2}$\n",
        "\n",
        "$\\frac{\\partial{\\hat{y}}}{\\partial{z}}=\\frac{0-(-e^{-z})}{(1+e^{-z})^2}$\n",
        "\n",
        "$\\frac{\\partial{\\hat{y}}}{\\partial{z}}=\\frac{e^{-z}}{(1+e^{-z})^2}$\n",
        "\n",
        "$\\frac{\\partial{\\hat{y}}}{\\partial{z}}=\\frac{1}{(1+e^{-z})}*\\frac{e^{-z}}{(1+e^{-z})}$\n",
        "\n",
        "$\\frac{\\partial{\\hat{y}}}{\\partial{z}}=\\hat{y}*(1-\\frac{1}{(1+e^{-z})})$\n",
        "\n",
        "$\\therefore \\frac{\\partial{\\hat{y}}}{\\partial{z}}=\\hat{y}*(1-\\hat{y})$\n",
        "\n",
        "\\\n",
        "\n",
        "3. Z w.r.t. input weights:\n",
        "\n",
        "$\\frac{\\partial{z}}{\\partial{w_0}}=\\frac{\\partial{(w_0+x_1*w_1+x_2*w_2)}}{\\partial{w_0}}$\n",
        "\n",
        "$\\therefore \\frac{\\partial{z}}{\\partial{w_0}}=1$\n",
        "\n",
        "$\\therefore \\frac{\\partial{z}}{\\partial{w_1}}=x_1$\n",
        "\n",
        "$\\therefore \\frac{\\partial{z}}{\\partial{w_2}}=x_2$\n",
        "\n",
        "* **Note**: The gradient of `z` with respect to `w` = `Input` as seen just now. This is so because z is the weighted sum.\n",
        "\n",
        "\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3QrAS7OLjY6"
      },
      "source": [
        "### Chain Rule using the local gradients:\n",
        "\n",
        "$\\therefore\\frac{\\partial{L}}{\\partial{\\hat{y}}}=-(\\frac{y}{\\hat{y}}-\\frac{(1-y)}{(1-\\hat{y})})$\n",
        "\n",
        "$\\therefore\\frac{\\partial{L}}{\\partial{z}}=\\frac{\\partial{L}}{\\partial{\\hat{y}}}*\\frac{\\partial{\\hat{y}}}{\\partial{z}}=-(\\frac{y}{\\hat{y}}-\\frac{(1-y)}{(1-\\hat{y})})*\\hat{y}*(1-\\hat{y})$\n",
        "\n",
        "\n",
        "$\\therefore\\frac{\\partial{L}}{\\partial{w_0}}=\\frac{\\partial{L}}{\\partial{\\hat{y}}}*\\frac{\\partial{\\hat{y}}}{\\partial{z}}*\\frac{\\partial{z}}{\\partial{w_0}}=-(\\frac{y}{\\hat{y}}-\\frac{(1-y)}{(1-\\hat{y})})*\\hat{y}(1-\\hat{y})*1$\n",
        "\n",
        "$\\therefore\\frac{\\partial{L}}{\\partial{w_1}}=\\frac{\\partial{L}}{\\partial{\\hat{y}}}*\\frac{\\partial{\\hat{y}}}{\\partial{z}}*\\frac{\\partial{z}}{\\partial{w_1}}=-(\\frac{y}{\\hat{y}}-\\frac{(1-y)}{(1-\\hat{y})})*\\hat{y}(1-\\hat{y})*x_1$\n",
        "\n",
        "$\\therefore \\frac{\\partial{L}}{\\partial{w_2}}=\\frac{\\partial{L}}{\\partial{\\hat{y}}}*\\frac{\\partial{\\hat{y}}}{\\partial{z}}*\\frac{\\partial{z}}{\\partial{w_2}}=-(\\frac{y}{\\hat{y}}-\\frac{(1-y)}{(1-\\hat{y})})*\\hat{y}(1-\\hat{y})*x_2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0QUVWEdL6B7"
      },
      "source": [
        "### Weight Update:\n",
        "The main objective of the backpropagation is to update the weights so that the model may \"learn\" something.\n",
        "\n",
        "Now, we update the weights as follows:\n",
        "\n",
        "$new \\space w = old \\space w + \\Delta w = old \\space w + \\eta (- \\frac{\\partial L}{\\partial w})$\n",
        "\n",
        "where, $\\eta$ is the learning rate.\n",
        "\n",
        "For example,\n",
        "\n",
        "$$new \\space w_0 = old \\space w_0 + \\Delta w_0 = old \\space w_0 + \\eta (- \\frac{\\partial L}{\\partial w_0})$$\n",
        "$$new \\space w_1 = old \\space w_1 + \\Delta w_1 = old \\space w_1 + \\eta (- \\frac{\\partial L}{\\partial w_1})$$\n",
        "\n",
        "$$new \\space w_2 = old \\space w_2 + \\Delta w_2 = old \\space w_2 + \\eta (- \\frac{\\partial L}{\\partial w_2})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L80Dj8WJMDHS"
      },
      "source": [
        "### Implementation of Backpropagation from Scratch:\n",
        "\n",
        "Link: https://github.com/SunlightWings/Machine-Learning-and-Data-Mining-/blob/main/ANN/Backpropagation_and_Computational_Graph.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80id9eZGMs8x"
      },
      "source": [
        "# Part B: MLP with Backpropagation (With PyTorch):\n",
        "\n",
        "Autograd in PyTorch is used for rapid computation of multiple derivatives (gradients).\n",
        "\n",
        "A machine learninig model can be defined as:\n",
        "\n",
        "$\\vec{y} = \\vec{M}(\\vec{x})$\n",
        "\n",
        "where,\n",
        "\n",
        "$\\vec{x}$ = *i*-dimensional vector $\\vec{x}$\n",
        "\n",
        "$\\vec{M}$ = Vector-valued function (function that outputs a vector $\\vec{y}$, and not a scalar)\n",
        "\n",
        "Meanwhile, The Loss function L($\\vec{y}$) = L($\\vec{M}$($\\vec{x}$)) is a single-valued scalar function of the model's output because it produces a single scalar value.\n",
        "\n",
        "Goal: Minimize loss function.\n",
        "      Ideally Make its gradient w.r.t input=0\n",
        "\n",
        "For a function $\\vec{y}=f(\\vec{x})$, with n-dimensional input and m-dimensional output, the complete gradient is a matrix of the derivative of every output with respect to every input, called the **Jacobian:**\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J\n",
        "=\n",
        "\\left(\\begin{array}{ccc}\n",
        "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "\\end{array}\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "A second function, $l=g\\left(\\vec{y}\\right)$ that takes m-dimensional input and produces a scalar output (like a loss function),  you can express its gradients with respect to $\\vec{y}$ as a column vector,\n",
        "$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$\n",
        "\n",
        "Now imagine 'J' is the PyTorch model, and 'v' is the loss function, now they can be multiplied to obtain the required column matrix, which contains the gradient of Loss with respect to inputs. Thats all we want!!\n",
        "\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J\\cdot v^{T}=\\left(\\begin{array}{ccc}\n",
        "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "\\end{array}\\right)\\left(\\begin{array}{c}\n",
        "\\frac{\\partial l}{\\partial y_{1}}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial l}{\\partial y_{m}}\n",
        "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
        "\\frac{\\partial l}{\\partial x_{1}}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial l}{\\partial x_{n}}\n",
        "\\end{array}\\right)\n",
        "\\end{aligned}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkBghUHzPFxo"
      },
      "source": [
        "## Building Network Architecture:\n",
        "Lets begin by building neural network architecture.\n",
        "The architecture looks something like this:\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=18Z0NofFLsTIxW4zBdILxcMEczhd113c4\" alt=\"ANN\" height=\"550\" width=\"650\">\n",
        "\n",
        "Figure 1: Neural Network with two hidden layers\n",
        "</center>\n",
        "\n",
        "In the above figure, the layers with the number of neurons are:\n",
        "* First layer = 10 neurons\n",
        "* Second layer = 16 neurons\n",
        "* Third layer = 8 neurons\n",
        "* Fourt layer = 1 neuron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I_5J8joyDNFm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5gVIokX-PLAl"
      },
      "outputs": [],
      "source": [
        "linear1 = nn.Linear(10,16)\n",
        "linear2 = nn.Linear(16,8)\n",
        "linear3 = nn.Linear(8,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTyMdRD7PTsF",
        "outputId": "d1b8091d-a2bb-4c04-d1af-cfd3226a2cc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Linear(in_features=10, out_features=16, bias=True),\n",
              " Linear(in_features=16, out_features=8, bias=True),\n",
              " Linear(in_features=8, out_features=1, bias=True))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear1, linear2, linear3  # print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PI8uVR9sPV-7"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(linear1, linear2, linear3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFH0UQZSPhCE",
        "outputId": "8a4a6969-9ad9-44df-db6a-a27c9f2539d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=10, out_features=16, bias=True)\n",
              "  (1): Linear(in_features=16, out_features=8, bias=True)\n",
              "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yAJqlCFP6z9"
      },
      "source": [
        "### Data preparation:\n",
        "\n",
        "Lets define some random data for now.\n",
        "\n",
        "Note that in actual application, this is one of the most important steps.\n",
        "\n",
        "It is usually done by making a 'Dataset' class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TllMCPAPh2K",
        "outputId": "dd9ac293-0029-467e-f2d8-9bbe8391281b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229, -0.1863,  2.2082, -0.6380,\n",
              "         0.4617,  0.2674])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "input1 = torch.randn(10)   # input to the model (fixed set of random numbers for reproducibility)\n",
        "input1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtlynoNkQHHG"
      },
      "source": [
        "### Forward pass:\n",
        "Output from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0vFUOu1P8vg",
        "outputId": "d404fb3d-6f4d-4e8b-c22b-511e7384d095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.4179], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output = model(input1)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocW2Z3PEREbq",
        "outputId": "d1f77850-6675-4676-dee1-0f65b4471194"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYld1N9GQMfB"
      },
      "source": [
        "**Task2: Manually do the forward pass through each layer and print them, along with their shape**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmiVED1rQEHu",
        "outputId": "b3e3a254-a6f1-492d-e755-5c81ee899e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-1.0438, -0.5736, -0.6513, -0.3797, -0.8882, -0.3074, -0.2861, -0.4009,\n",
            "         0.0018, -0.1239, -0.3237, -1.1705,  0.0145,  0.5963, -0.2574, -0.2240],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "## TO DO\n",
        "output1 = linear1(input1)\n",
        "print(output1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVvHYXxGRJrh",
        "outputId": "241711fd-a1dc-46c3-e44b-e59b8a3ca662"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "STxaCuE8QbjB"
      },
      "outputs": [],
      "source": [
        "## TO DO\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf8rMOOPRNQw"
      },
      "source": [
        "## With activation function:\n",
        "\n",
        "Activation functions add non-linearity to the network.\n",
        "\n",
        "Otherwise, whole network is equivalent to a single neuron.\n",
        "\n",
        "**Task3: Discuss why**\n",
        "\n",
        "Lets create a new model(network) with 2 neurons in the final layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7od_zzzfQeA1"
      },
      "outputs": [],
      "source": [
        "linear1 = nn.Linear(10,16)\n",
        "linear2 = nn.Linear(16,8)\n",
        "linear3 = nn.Linear(8,2)\n",
        "\n",
        "model2 = nn.Sequential(\n",
        "    linear1,\n",
        "    nn.LeakyReLU(0.02),\n",
        "    linear2,\n",
        "    nn.Sigmoid(),\n",
        "    linear3,\n",
        "    nn.Softmax(dim = -1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCCu6dDVRcMd",
        "outputId": "62fbd738-5d0e-4405-e84f-4eab0fe28856"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=10, out_features=16, bias=True)\n",
              "  (1): LeakyReLU(negative_slope=0.02)\n",
              "  (2): Linear(in_features=16, out_features=8, bias=True)\n",
              "  (3): Sigmoid()\n",
              "  (4): Linear(in_features=8, out_features=2, bias=True)\n",
              "  (5): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36vw7aFpRfwV"
      },
      "source": [
        "**Task4: Do forward pass on this new model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKMUFksmRdp1",
        "outputId": "6188be48-fe4f-4f0a-876d-044cb1e4b78c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.4489, 0.5511], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## TO DO\n",
        "output2 = model2(input1)\n",
        "output2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3WYHBS_SCwq"
      },
      "source": [
        "### One hot encoding:\n",
        "\n",
        "Until now, we have:\n",
        "* Input = `input1` = [ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229, -0.1863,  2.2082, -0.6380, 0.4617,  0.2674]\n",
        "* Output = `output_of_model` = [prob1, prob2]\n",
        "\n",
        "Now we can create ground truth as:\n",
        "* GT = `actual_output` = [actual_prob1, actual_prob2]\n",
        "\n",
        "But often in real world dataset, the ground truth could only mention the target class instead of probabilities such as:\n",
        "* GT = `actual_output` = Class 0\n",
        "\n",
        "**Task4: Answer/Ponder**\n",
        "\n",
        "In such case, how do we compare `output_of_model` and `actual_output` to calculate loss function?? Here they are simply incompatible in shape!!\n",
        "\n",
        "Lets say the output is class1. So the objective is to make it [0, 1] so that it can be compared to [prob1, prob2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpPtS6mpRntK",
        "outputId": "0b66748b-e336-4c19-c98a-e1694d658a74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-hot encoded ground truth: tensor([1., 0.])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "actual_output_class = 0\n",
        "\n",
        "actual_output_one_hot = F.one_hot(torch.tensor(actual_output_class), num_classes=2).float()\n",
        "print(\"One-hot encoded ground truth:\", actual_output_one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT0Yzxu5S0wy"
      },
      "source": [
        "**Task5: Check the one hot output after the forward pass on above model2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIfvenGkSwTv",
        "outputId": "84740aa3-ba63-4795-fa93-021c92555c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4489, 0.5511], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([1., 0.])\n"
          ]
        }
      ],
      "source": [
        "output_of_model = model2(input1)\n",
        "print(output_of_model)\n",
        "print(actual_output_one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaKhNbQcStxh"
      },
      "source": [
        "### Loss function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubnOyManSrFM",
        "outputId": "4c475d21-1441-4afa-a672-8c8d76453c5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criteria = nn.CrossEntropyLoss()                           # this is defined before the training loop\n",
        "criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KEygYDCSyRy",
        "outputId": "d331c17c-c2ff-4fd2-e092-91f884c62c8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.7456, grad_fn=<DivBackward1>)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = criteria(output_of_model, actual_output_one_hot)      # this will be done in the training loop.\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8AJif08S_ZB"
      },
      "outputs": [],
      "source": [
        "loss.backward(retain_graph = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MhA3_mdTEDr"
      },
      "source": [
        "### Weights and gradients:\n",
        "\n",
        "**Task6: Print them**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "szW7OjgCTBIe",
        "outputId": "b93903b3-7e71-435c-eb35-a8be3542be22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=10, out_features=16, bias=True)\n",
            "  (1): LeakyReLU(negative_slope=0.02)\n",
            "  (2): Linear(in_features=16, out_features=8, bias=True)\n",
            "  (3): Sigmoid()\n",
            "  (4): Linear(in_features=8, out_features=2, bias=True)\n",
            "  (5): Softmax(dim=-1)\n",
            ")\n",
            "Linear1 weights: Parameter containing:\n",
            "tensor([[-0.1457, -0.0371, -0.1284,  0.2098, -0.2496, -0.1458, -0.0893, -0.1901,\n",
            "          0.0298, -0.3123],\n",
            "        [ 0.2856, -0.2686,  0.2441,  0.0526, -0.1027,  0.1954,  0.0493,  0.2555,\n",
            "          0.0346, -0.0997],\n",
            "        [ 0.0850, -0.0858,  0.1331,  0.2823,  0.1828, -0.1382,  0.1825,  0.0566,\n",
            "          0.1606, -0.1927],\n",
            "        [-0.3130, -0.1222, -0.2426,  0.2595,  0.0911,  0.1310,  0.1000, -0.0055,\n",
            "          0.2475, -0.2247],\n",
            "        [ 0.0199, -0.2158,  0.0975, -0.1089,  0.0969, -0.0659,  0.2623, -0.1874,\n",
            "         -0.1886, -0.1886],\n",
            "        [ 0.2844,  0.1054,  0.3043, -0.2610, -0.3137, -0.2474, -0.2127,  0.1281,\n",
            "          0.1132,  0.2628],\n",
            "        [-0.1633, -0.2156,  0.1678, -0.1278,  0.1919, -0.0750,  0.1809, -0.2457,\n",
            "         -0.1596,  0.0964],\n",
            "        [ 0.0669, -0.0806,  0.1885,  0.2150, -0.2293, -0.1688,  0.2896, -0.1067,\n",
            "         -0.1121, -0.3060],\n",
            "        [-0.1811,  0.0790, -0.0417, -0.2295,  0.0074, -0.2160, -0.2683, -0.1741,\n",
            "         -0.2768, -0.2014],\n",
            "        [ 0.3161,  0.0597,  0.0974, -0.2949, -0.2077, -0.1053,  0.0494, -0.2783,\n",
            "         -0.1363, -0.1893],\n",
            "        [ 0.0009, -0.1177, -0.0219, -0.2143, -0.2171, -0.1845, -0.1082, -0.2496,\n",
            "          0.2651, -0.0628],\n",
            "        [ 0.2721,  0.0985, -0.2678,  0.2188, -0.0870, -0.1212, -0.2625, -0.3144,\n",
            "          0.0905, -0.0691],\n",
            "        [ 0.1231, -0.2595,  0.2348, -0.2321, -0.0546,  0.0660,  0.1633,  0.2553,\n",
            "          0.2881, -0.2507],\n",
            "        [ 0.0796, -0.1360, -0.0347, -0.2367,  0.2881, -0.2321,  0.1690,  0.1111,\n",
            "          0.1028, -0.1710],\n",
            "        [ 0.2874,  0.0695,  0.0407, -0.2787,  0.1327, -0.0474, -0.1449,  0.2716,\n",
            "          0.0705, -0.1750],\n",
            "        [-0.1601, -0.0151,  0.1766, -0.0808, -0.1804, -0.1083, -0.2362,  0.1128,\n",
            "          0.2448, -0.2977]], requires_grad=True)\n",
            "Linear1 biases: Parameter containing:\n",
            "tensor([ 0.0734,  0.1634,  0.0573, -0.1126,  0.1651,  0.1662,  0.1182, -0.0556,\n",
            "        -0.0837,  0.0338, -0.0559, -0.0942,  0.2022,  0.2718, -0.0313, -0.0708],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Accessing parameters of specific layers\n",
        "print(model2)\n",
        "print(\"Linear1 weights:\", model2[0].weight)\n",
        "print(\"Linear1 biases:\", model2[0].bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BN6555VITH7F",
        "outputId": "bbe21e4c-9332-48ec-bf27-fce677a48765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradients of Linear1 weights: tensor([[-1.1016e-03, -4.2145e-04, -7.6714e-04, -7.5363e-04,  3.6739e-03,\n",
            "          6.0965e-04, -7.2250e-03,  2.0875e-03, -1.5105e-03, -8.7474e-04],\n",
            "        [-7.7563e-04, -2.9674e-04, -5.4013e-04, -5.3062e-04,  2.5867e-03,\n",
            "          4.2924e-04, -5.0870e-03,  1.4697e-03, -1.0635e-03, -6.1589e-04],\n",
            "        [ 1.8456e-04,  7.0609e-05,  1.2852e-04,  1.2626e-04, -6.1551e-04,\n",
            "         -1.0214e-04,  1.2105e-03, -3.4973e-04,  2.5307e-04,  1.4655e-04],\n",
            "        [ 1.1969e-05,  4.5789e-06,  8.3346e-06,  8.1878e-06, -3.9915e-05,\n",
            "         -6.6235e-06,  7.8496e-05, -2.2679e-05,  1.6411e-05,  9.5037e-06],\n",
            "        [-3.0357e-03, -1.1614e-03, -2.1140e-03, -2.0767e-03,  1.0124e-02,\n",
            "          1.6800e-03, -1.9910e-02,  5.7524e-03, -4.1624e-03, -2.4105e-03],\n",
            "        [-3.0969e-03, -1.1848e-03, -2.1566e-03, -2.1186e-03,  1.0328e-02,\n",
            "          1.7139e-03, -2.0311e-02,  5.8683e-03, -4.2463e-03, -2.4591e-03],\n",
            "        [-6.3868e-04, -2.4434e-04, -4.4476e-04, -4.3693e-04,  2.1300e-03,\n",
            "          3.5345e-04, -4.1888e-03,  1.2102e-03, -8.7574e-04, -5.0715e-04],\n",
            "        [-3.0471e-04, -1.1658e-04, -2.1220e-04, -2.0846e-04,  1.0162e-03,\n",
            "          1.6863e-04, -1.9985e-03,  5.7741e-04, -4.1781e-04, -2.4196e-04],\n",
            "        [ 4.2807e-07,  1.6377e-07,  2.9810e-07,  2.9285e-07, -1.4276e-06,\n",
            "         -2.3690e-07,  2.8075e-06, -8.1116e-07,  5.8696e-07,  3.3991e-07],\n",
            "        [ 2.0199e-03,  7.7275e-04,  1.4066e-03,  1.3818e-03, -6.7362e-03,\n",
            "         -1.1178e-03,  1.3247e-02, -3.8275e-03,  2.7696e-03,  1.6039e-03],\n",
            "        [ 8.3624e-04,  3.1992e-04,  5.8234e-04,  5.7208e-04, -2.7888e-03,\n",
            "         -4.6278e-04,  5.4845e-03, -1.5846e-03,  1.1466e-03,  6.6402e-04],\n",
            "        [-2.1093e-05, -8.0697e-06, -1.4689e-05, -1.4430e-05,  7.0346e-05,\n",
            "          1.1673e-05, -1.3834e-04,  3.9970e-05, -2.8922e-05, -1.6749e-05],\n",
            "        [-6.5745e-03, -2.5152e-03, -4.5783e-03, -4.4977e-03,  2.1926e-02,\n",
            "          3.6384e-03, -4.3119e-02,  1.2458e-02, -9.0147e-03, -5.2205e-03],\n",
            "        [ 4.0126e-03,  1.5351e-03,  2.7942e-03,  2.7450e-03, -1.3382e-02,\n",
            "         -2.2206e-03,  2.6317e-02, -7.6034e-03,  5.5019e-03,  3.1862e-03],\n",
            "        [-1.5682e-05, -5.9997e-06, -1.0921e-05, -1.0728e-05,  5.2300e-05,\n",
            "          8.6788e-06, -1.0285e-04,  2.9717e-05, -2.1503e-05, -1.2453e-05],\n",
            "        [ 6.8444e-05,  2.6185e-05,  4.7662e-05,  4.6823e-05, -2.2826e-04,\n",
            "         -3.7877e-05,  4.4889e-04, -1.2969e-04,  9.3847e-05,  5.4348e-05]])\n",
            "Gradients of Linear1 biases: tensor([-3.2719e-03, -2.3037e-03,  5.4817e-04,  3.5548e-05, -9.0163e-03,\n",
            "        -9.1980e-03, -1.8969e-03, -9.0503e-04,  1.2714e-06,  5.9992e-03,\n",
            "         2.4837e-03, -6.2649e-05, -1.9527e-02,  1.1918e-02, -4.6578e-05,\n",
            "         2.0328e-04])\n"
          ]
        }
      ],
      "source": [
        "## TO DO:\n",
        "\n",
        "# Access the gradients of the first layer's parameters\n",
        "print(\"Gradients of Linear1 weights:\", model2[0].weight.grad)\n",
        "print(\"Gradients of Linear1 biases:\", model2[0].bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2WTdTu1TS8R"
      },
      "source": [
        "### Weight updation:\n",
        "\n",
        "We could update the weights of the first layers using the formula:\n",
        "\n",
        "`model2[0].weight = model2[0].weight - learning_rate * model2[0].weight.grad`.\n",
        "\n",
        "But there are various reasons we don't do so. They are:\n",
        "\n",
        "1. PyTorch doesn't allow assigning a new tensor to a parameter.\n",
        "\n",
        "  Thus, we can instead do this:\n",
        "\n",
        "  `model2[0].weight -= learning_rate * model2[0].weight.grad`\n",
        "\n",
        "2. Even though the above update works, Non-convex functions dont converge easily, so we need a proper optimizer to update the weights.\n",
        "\n",
        "  Thus we may use one of the most popular, simple and effective optimizer called `SGD`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J_HdXIsMTQG3"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jDIkXshGTeSH"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model2[0].weight -= learning_rate * model2[0].weight.grad    # updation using the above point number 1.\n",
        "    model2[0].bias -= learning_rate * model2[0].bias.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bF99n10FTfgo",
        "outputId": "999a5e9f-0308-48f0-fcd5-142a54811bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradients of Linear1 weights: tensor([[-1.1016e-03, -4.2145e-04, -7.6714e-04, -7.5363e-04,  3.6739e-03,\n",
            "          6.0965e-04, -7.2250e-03,  2.0875e-03, -1.5105e-03, -8.7474e-04],\n",
            "        [-7.7563e-04, -2.9674e-04, -5.4013e-04, -5.3062e-04,  2.5867e-03,\n",
            "          4.2924e-04, -5.0870e-03,  1.4697e-03, -1.0635e-03, -6.1589e-04],\n",
            "        [ 1.8456e-04,  7.0609e-05,  1.2852e-04,  1.2626e-04, -6.1551e-04,\n",
            "         -1.0214e-04,  1.2105e-03, -3.4973e-04,  2.5307e-04,  1.4655e-04],\n",
            "        [ 1.1969e-05,  4.5789e-06,  8.3346e-06,  8.1878e-06, -3.9915e-05,\n",
            "         -6.6235e-06,  7.8496e-05, -2.2679e-05,  1.6411e-05,  9.5037e-06],\n",
            "        [-3.0357e-03, -1.1614e-03, -2.1140e-03, -2.0767e-03,  1.0124e-02,\n",
            "          1.6800e-03, -1.9910e-02,  5.7524e-03, -4.1624e-03, -2.4105e-03],\n",
            "        [-3.0969e-03, -1.1848e-03, -2.1566e-03, -2.1186e-03,  1.0328e-02,\n",
            "          1.7139e-03, -2.0311e-02,  5.8683e-03, -4.2463e-03, -2.4591e-03],\n",
            "        [-6.3868e-04, -2.4434e-04, -4.4476e-04, -4.3693e-04,  2.1300e-03,\n",
            "          3.5345e-04, -4.1888e-03,  1.2102e-03, -8.7574e-04, -5.0715e-04],\n",
            "        [-3.0471e-04, -1.1658e-04, -2.1220e-04, -2.0846e-04,  1.0162e-03,\n",
            "          1.6863e-04, -1.9985e-03,  5.7741e-04, -4.1781e-04, -2.4196e-04],\n",
            "        [ 4.2807e-07,  1.6377e-07,  2.9810e-07,  2.9285e-07, -1.4276e-06,\n",
            "         -2.3690e-07,  2.8075e-06, -8.1116e-07,  5.8696e-07,  3.3991e-07],\n",
            "        [ 2.0199e-03,  7.7275e-04,  1.4066e-03,  1.3818e-03, -6.7362e-03,\n",
            "         -1.1178e-03,  1.3247e-02, -3.8275e-03,  2.7696e-03,  1.6039e-03],\n",
            "        [ 8.3624e-04,  3.1992e-04,  5.8234e-04,  5.7208e-04, -2.7888e-03,\n",
            "         -4.6278e-04,  5.4845e-03, -1.5846e-03,  1.1466e-03,  6.6402e-04],\n",
            "        [-2.1093e-05, -8.0697e-06, -1.4689e-05, -1.4430e-05,  7.0346e-05,\n",
            "          1.1673e-05, -1.3834e-04,  3.9970e-05, -2.8922e-05, -1.6749e-05],\n",
            "        [-6.5745e-03, -2.5152e-03, -4.5783e-03, -4.4977e-03,  2.1926e-02,\n",
            "          3.6384e-03, -4.3119e-02,  1.2458e-02, -9.0147e-03, -5.2205e-03],\n",
            "        [ 4.0126e-03,  1.5351e-03,  2.7942e-03,  2.7450e-03, -1.3382e-02,\n",
            "         -2.2206e-03,  2.6317e-02, -7.6034e-03,  5.5019e-03,  3.1862e-03],\n",
            "        [-1.5682e-05, -5.9997e-06, -1.0921e-05, -1.0728e-05,  5.2300e-05,\n",
            "          8.6788e-06, -1.0285e-04,  2.9717e-05, -2.1503e-05, -1.2453e-05],\n",
            "        [ 6.8444e-05,  2.6185e-05,  4.7662e-05,  4.6823e-05, -2.2826e-04,\n",
            "         -3.7877e-05,  4.4889e-04, -1.2969e-04,  9.3847e-05,  5.4348e-05]])\n",
            "Gradients of Linear1 biases: tensor([-3.2719e-03, -2.3037e-03,  5.4817e-04,  3.5548e-05, -9.0163e-03,\n",
            "        -9.1980e-03, -1.8969e-03, -9.0503e-04,  1.2714e-06,  5.9992e-03,\n",
            "         2.4837e-03, -6.2649e-05, -1.9527e-02,  1.1918e-02, -4.6578e-05,\n",
            "         2.0328e-04])\n"
          ]
        }
      ],
      "source": [
        "# Access the gradients of the first layer's parameters\n",
        "print(\"Gradients of Linear1 weights:\", model2[0].weight.grad)\n",
        "print(\"Gradients of Linear1 biases:\", model2[0].bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQCzzNNoTnNi"
      },
      "source": [
        "# Part C: Standard Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYAV6MJgUBfX"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNbtEx0TTgvW"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=5000, n_features=10, n_redundant=0, n_informative=3, n_clusters_per_class=2, n_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhnwSHuCTtId",
        "outputId": "4a75acd3-229e-494f-b9b2-f0fba6659a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5000, 10)\n",
            "(5000,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)\n",
        "print(y.shape)\n",
        "# print(y[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Rb5ul9xSTuNZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J3NMgjrTv0_",
        "outputId": "08d8f666-031c-4414-ef1a-ffd4bc45db8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(np.unique(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yoX6Z9cTxNn",
        "outputId": "43d1742e-10ae-4704-8c2e-5e19bbc3e780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4000, 10)\n",
            "(4000,)\n",
            "(1000, 10)\n",
            "(1000,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GeVf59JjT0TS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    # method to initialize dataset, and conversion to tensor\n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.X = torch.from_numpy(X_train.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y_train).type(torch.LongTensor)\n",
        "        self.len = self.X.shape[0]\n",
        "\n",
        "    # method to return a single sample from the dataset given an index.\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "    # method to return the number of samples in the dataset.\n",
        "    def __len__(self):\n",
        "        return self.len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4LKoeTFT2sP",
        "outputId": "bc4f5fc9-17a0-4339-f85e-4ad99f24264e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.33787199  0.05870305  0.14166288 -0.57210746 -1.07297266 -0.28239403\n",
            "  0.25714039 -0.64167298  1.80071396  0.93124613]\n",
            "(tensor([ 0.3379,  0.0587,  0.1417, -0.5721, -1.0730, -0.2824,  0.2571, -0.6417,\n",
            "         1.8007,  0.9312]), tensor(1))\n"
          ]
        }
      ],
      "source": [
        "print(X_train[25])                                     # simple access\n",
        "\n",
        "traindata = CustomDataset(X_train, y_train)\n",
        "print(traindata[25])                                   # access with above class' method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1Rwsyh-vT4AP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "trainloader = DataLoader(traindata, batch_size = 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eeGw8cY9T6CK",
        "outputId": "43787df9-97ad-4f3c-b3c7-05386bd5807d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n",
            "torch.Size([16, 10]) torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# Iterate through the TrainLoader\n",
        "\n",
        "for batch_X, batch_y in trainloader:\n",
        "    print(batch_X.shape, batch_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md6kCbTYT9su"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1-aSn791T7VJ"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    # Define the layers and all in the constructor.\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()  # Initialize parent class within the constructor\n",
        "        self.linear1 = nn.Linear(10, 16)\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.linear2 = nn.Linear(16, 8)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.linear3 = nn.Linear(8, 2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    # Method for forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.linear2(x)  # Pass input to the linear2 layer\n",
        "        x = self.sigmoid(x)  # Pass input to the sigmoid layer\n",
        "        x = self.linear3(x)  # Pass input to the linear3 layer\n",
        "        x = self.softmax(x)  # Pass input to the softmax layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7yso9cjUArl",
        "outputId": "caef3b73-097d-4c44-e87c-1c51e67acd2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (linear1): Linear(in_features=10, out_features=16, bias=True)\n",
            "  (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
            "  (linear2): Linear(in_features=16, out_features=8, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            "  (linear3): Linear(in_features=8, out_features=2, bias=True)\n",
            "  (softmax): Softmax(dim=-1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model3 = Network()\n",
        "print(model3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UjkGV9UUEaa",
        "outputId": "5d7c4e99-44a1-48ed-bc3d-a01b78794a1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CrossEntropyLoss()\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "print(criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXy3ZxrsUGH3",
        "outputId": "129a846f-d958-4706-cf09-322eb7fe1d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model3.parameters(), lr = 0.001)\n",
        "print(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vddCgCFUIKd"
      },
      "source": [
        "## Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF1ffL5RUHVM",
        "outputId": "f6f41744-bf55-4711-ac6a-3a34b4c586c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.002774604320526123\n",
            "Epoch 1/10, Loss: 0.00556499981880188\n",
            "Epoch 1/10, Loss: 0.008438211917877197\n",
            "Epoch 1/10, Loss: 0.011234872579574584\n",
            "Epoch 1/10, Loss: 0.01400769019126892\n",
            "Epoch 1/10, Loss: 0.016783303260803222\n",
            "Epoch 1/10, Loss: 0.019548776388168335\n",
            "Epoch 1/10, Loss: 0.022294262886047363\n",
            "Epoch 1/10, Loss: 0.0250646595954895\n",
            "Epoch 1/10, Loss: 0.027904199600219726\n",
            "Epoch 1/10, Loss: 0.03071305775642395\n",
            "Epoch 1/10, Loss: 0.033452316999435426\n",
            "Epoch 1/10, Loss: 0.036163713216781614\n",
            "Epoch 1/10, Loss: 0.03898351550102234\n",
            "Epoch 1/10, Loss: 0.04169109177589417\n",
            "Epoch 1/10, Loss: 0.044447808265686034\n",
            "Epoch 1/10, Loss: 0.047232107639312744\n",
            "Epoch 1/10, Loss: 0.0500559024810791\n",
            "Epoch 1/10, Loss: 0.052795119285583496\n",
            "Epoch 1/10, Loss: 0.05554371023178101\n",
            "Epoch 1/10, Loss: 0.05831700849533081\n",
            "Epoch 1/10, Loss: 0.06103218007087707\n",
            "Epoch 1/10, Loss: 0.06380135369300842\n",
            "Epoch 1/10, Loss: 0.06657582259178162\n",
            "Epoch 1/10, Loss: 0.06932265090942383\n",
            "Epoch 1/10, Loss: 0.07216454982757568\n",
            "Epoch 1/10, Loss: 0.07492550301551819\n",
            "Epoch 1/10, Loss: 0.07771974015235901\n",
            "Epoch 1/10, Loss: 0.08054404711723327\n",
            "Epoch 1/10, Loss: 0.083403715133667\n",
            "Epoch 1/10, Loss: 0.08622856998443604\n",
            "Epoch 1/10, Loss: 0.08906455540657043\n",
            "Epoch 1/10, Loss: 0.09178559064865112\n",
            "Epoch 1/10, Loss: 0.09451847767829895\n",
            "Epoch 1/10, Loss: 0.09726634860038758\n",
            "Epoch 1/10, Loss: 0.10001996612548827\n",
            "Epoch 1/10, Loss: 0.10285531544685364\n",
            "Epoch 1/10, Loss: 0.10558846998214722\n",
            "Epoch 1/10, Loss: 0.10835320115089417\n",
            "Epoch 1/10, Loss: 0.11114565706253052\n",
            "Epoch 1/10, Loss: 0.11391933703422547\n",
            "Epoch 1/10, Loss: 0.11670723009109497\n",
            "Epoch 1/10, Loss: 0.11939879584312439\n",
            "Epoch 1/10, Loss: 0.12214342522621155\n",
            "Epoch 1/10, Loss: 0.12488539338111877\n",
            "Epoch 1/10, Loss: 0.12762852144241332\n",
            "Epoch 1/10, Loss: 0.13037575721740724\n",
            "Epoch 1/10, Loss: 0.13314092421531679\n",
            "Epoch 1/10, Loss: 0.13591446971893312\n",
            "Epoch 1/10, Loss: 0.13867722606658936\n",
            "Epoch 1/10, Loss: 0.14142647004127504\n",
            "Epoch 1/10, Loss: 0.14413838624954223\n",
            "Epoch 1/10, Loss: 0.14691116881370545\n",
            "Epoch 1/10, Loss: 0.14967914009094238\n",
            "Epoch 1/10, Loss: 0.15240894389152526\n",
            "Epoch 1/10, Loss: 0.15516532373428343\n",
            "Epoch 1/10, Loss: 0.1579172911643982\n",
            "Epoch 1/10, Loss: 0.16068487334251405\n",
            "Epoch 1/10, Loss: 0.1634177849292755\n",
            "Epoch 1/10, Loss: 0.16617822074890137\n",
            "Epoch 1/10, Loss: 0.16893472838401793\n",
            "Epoch 1/10, Loss: 0.17168557476997376\n",
            "Epoch 1/10, Loss: 0.17445676708221436\n",
            "Epoch 1/10, Loss: 0.17718227577209472\n",
            "Epoch 1/10, Loss: 0.1799444065093994\n",
            "Epoch 1/10, Loss: 0.18267792558670043\n",
            "Epoch 1/10, Loss: 0.18544081377983093\n",
            "Epoch 1/10, Loss: 0.188129727602005\n",
            "Epoch 1/10, Loss: 0.19089048838615416\n",
            "Epoch 1/10, Loss: 0.19363717555999757\n",
            "Epoch 1/10, Loss: 0.19636239337921144\n",
            "Epoch 1/10, Loss: 0.19909530758857727\n",
            "Epoch 1/10, Loss: 0.20183655476570128\n",
            "Epoch 1/10, Loss: 0.20455873370170594\n",
            "Epoch 1/10, Loss: 0.20733739542961122\n",
            "Epoch 1/10, Loss: 0.2100722920894623\n",
            "Epoch 1/10, Loss: 0.2128158369064331\n",
            "Epoch 1/10, Loss: 0.21556025409698487\n",
            "Epoch 1/10, Loss: 0.2182702510356903\n",
            "Epoch 1/10, Loss: 0.22101330590248108\n",
            "Epoch 1/10, Loss: 0.2237582085132599\n",
            "Epoch 1/10, Loss: 0.2265043683052063\n",
            "Epoch 1/10, Loss: 0.22925917410850524\n",
            "Epoch 1/10, Loss: 0.231985773563385\n",
            "Epoch 1/10, Loss: 0.23471014046669006\n",
            "Epoch 1/10, Loss: 0.23747512316703798\n",
            "Epoch 1/10, Loss: 0.2402151458263397\n",
            "Epoch 1/10, Loss: 0.2429810767173767\n",
            "Epoch 1/10, Loss: 0.2457388687133789\n",
            "Epoch 1/10, Loss: 0.24844789171218873\n",
            "Epoch 1/10, Loss: 0.25120570278167725\n",
            "Epoch 1/10, Loss: 0.25395036244392394\n",
            "Epoch 1/10, Loss: 0.2567103705406189\n",
            "Epoch 1/10, Loss: 0.25945066785812376\n",
            "Epoch 1/10, Loss: 0.26214449214935304\n",
            "Epoch 1/10, Loss: 0.26485416197776795\n",
            "Epoch 1/10, Loss: 0.2675472066402435\n",
            "Epoch 1/10, Loss: 0.2702533323764801\n",
            "Epoch 1/10, Loss: 0.2729630262851715\n",
            "Epoch 1/10, Loss: 0.27569548940658567\n",
            "Epoch 1/10, Loss: 0.2783950214385986\n",
            "Epoch 1/10, Loss: 0.28107806348800657\n",
            "Epoch 1/10, Loss: 0.28380219864845274\n",
            "Epoch 1/10, Loss: 0.2865240297317505\n",
            "Epoch 1/10, Loss: 0.28924469208717346\n",
            "Epoch 1/10, Loss: 0.2919850013256073\n",
            "Epoch 1/10, Loss: 0.29470433592796325\n",
            "Epoch 1/10, Loss: 0.29745667457580566\n",
            "Epoch 1/10, Loss: 0.30014447593688964\n",
            "Epoch 1/10, Loss: 0.302852597951889\n",
            "Epoch 1/10, Loss: 0.30564108657836914\n",
            "Epoch 1/10, Loss: 0.30832575130462647\n",
            "Epoch 1/10, Loss: 0.3110438461303711\n",
            "Epoch 1/10, Loss: 0.3138184237480164\n",
            "Epoch 1/10, Loss: 0.3165126810073853\n",
            "Epoch 1/10, Loss: 0.31927663826942443\n",
            "Epoch 1/10, Loss: 0.32199372816085814\n",
            "Epoch 1/10, Loss: 0.3246905355453491\n",
            "Epoch 1/10, Loss: 0.3273848743438721\n",
            "Epoch 1/10, Loss: 0.33008170008659365\n",
            "Epoch 1/10, Loss: 0.3327956256866455\n",
            "Epoch 1/10, Loss: 0.33545590209960935\n",
            "Epoch 1/10, Loss: 0.3381895847320557\n",
            "Epoch 1/10, Loss: 0.3409168906211853\n",
            "Epoch 1/10, Loss: 0.34366389203071596\n",
            "Epoch 1/10, Loss: 0.3463346917629242\n",
            "Epoch 1/10, Loss: 0.34901095819473266\n",
            "Epoch 1/10, Loss: 0.35169581532478333\n",
            "Epoch 1/10, Loss: 0.3543779554367065\n",
            "Epoch 1/10, Loss: 0.3570632526874542\n",
            "Epoch 1/10, Loss: 0.3597729425430298\n",
            "Epoch 1/10, Loss: 0.3624213442802429\n",
            "Epoch 1/10, Loss: 0.365077894449234\n",
            "Epoch 1/10, Loss: 0.3677401256561279\n",
            "Epoch 1/10, Loss: 0.37043415403366087\n",
            "Epoch 1/10, Loss: 0.37311789417266844\n",
            "Epoch 1/10, Loss: 0.37585959005355835\n",
            "Epoch 1/10, Loss: 0.3785561103820801\n",
            "Epoch 1/10, Loss: 0.38123157000541685\n",
            "Epoch 1/10, Loss: 0.38392179846763613\n",
            "Epoch 1/10, Loss: 0.3865911288261414\n",
            "Epoch 1/10, Loss: 0.3893130023479462\n",
            "Epoch 1/10, Loss: 0.3920317769050598\n",
            "Epoch 1/10, Loss: 0.39472184705734253\n",
            "Epoch 1/10, Loss: 0.3973831677436829\n",
            "Epoch 1/10, Loss: 0.4000234868526459\n",
            "Epoch 1/10, Loss: 0.40269431018829344\n",
            "Epoch 1/10, Loss: 0.4053497040271759\n",
            "Epoch 1/10, Loss: 0.4080049374103546\n",
            "Epoch 1/10, Loss: 0.41067182111740114\n",
            "Epoch 1/10, Loss: 0.41330547308921817\n",
            "Epoch 1/10, Loss: 0.41598851919174196\n",
            "Epoch 1/10, Loss: 0.41863972663879395\n",
            "Epoch 1/10, Loss: 0.42127998423576357\n",
            "Epoch 1/10, Loss: 0.4238926577568054\n",
            "Epoch 1/10, Loss: 0.42657488441467284\n",
            "Epoch 1/10, Loss: 0.4292608165740967\n",
            "Epoch 1/10, Loss: 0.4318957748413086\n",
            "Epoch 1/10, Loss: 0.43456349420547485\n",
            "Epoch 1/10, Loss: 0.43721562123298646\n",
            "Epoch 1/10, Loss: 0.43983989119529726\n",
            "Epoch 1/10, Loss: 0.4425279633998871\n",
            "Epoch 1/10, Loss: 0.44518151044845583\n",
            "Epoch 1/10, Loss: 0.4478093614578247\n",
            "Epoch 1/10, Loss: 0.4504643239974976\n",
            "Epoch 1/10, Loss: 0.45311863017082216\n",
            "Epoch 1/10, Loss: 0.4557084212303162\n",
            "Epoch 1/10, Loss: 0.4582627079486847\n",
            "Epoch 1/10, Loss: 0.46086447381973267\n",
            "Epoch 1/10, Loss: 0.4635386254787445\n",
            "Epoch 1/10, Loss: 0.46608270645141603\n",
            "Epoch 1/10, Loss: 0.4686831741333008\n",
            "Epoch 1/10, Loss: 0.4712460958957672\n",
            "Epoch 1/10, Loss: 0.4738497867584229\n",
            "Epoch 1/10, Loss: 0.4765514841079712\n",
            "Epoch 1/10, Loss: 0.4792074563503265\n",
            "Epoch 1/10, Loss: 0.4818459346294403\n",
            "Epoch 1/10, Loss: 0.48441760087013247\n",
            "Epoch 1/10, Loss: 0.48697485113143923\n",
            "Epoch 1/10, Loss: 0.4896132810115814\n",
            "Epoch 1/10, Loss: 0.49222752928733826\n",
            "Epoch 1/10, Loss: 0.49478503251075745\n",
            "Epoch 1/10, Loss: 0.49741533827781675\n",
            "Epoch 1/10, Loss: 0.5001120264530182\n",
            "Epoch 1/10, Loss: 0.5026617212295532\n",
            "Epoch 1/10, Loss: 0.505285385131836\n",
            "Epoch 1/10, Loss: 0.5079107098579406\n",
            "Epoch 1/10, Loss: 0.5105527453422546\n",
            "Epoch 1/10, Loss: 0.5131992073059082\n",
            "Epoch 1/10, Loss: 0.5158375167846679\n",
            "Epoch 1/10, Loss: 0.518466765165329\n",
            "Epoch 1/10, Loss: 0.5210785794258118\n",
            "Epoch 1/10, Loss: 0.5236858434677124\n",
            "Epoch 1/10, Loss: 0.5262377498149872\n",
            "Epoch 1/10, Loss: 0.528867710351944\n",
            "Epoch 1/10, Loss: 0.531506368637085\n",
            "Epoch 1/10, Loss: 0.534160481929779\n",
            "Epoch 1/10, Loss: 0.5367292840480804\n",
            "Epoch 1/10, Loss: 0.5391984508037567\n",
            "Epoch 1/10, Loss: 0.541681866645813\n",
            "Epoch 1/10, Loss: 0.544182388305664\n",
            "Epoch 1/10, Loss: 0.5467597379684448\n",
            "Epoch 1/10, Loss: 0.5493322601318359\n",
            "Epoch 1/10, Loss: 0.551940536737442\n",
            "Epoch 1/10, Loss: 0.5544723353385925\n",
            "Epoch 1/10, Loss: 0.5569756801128387\n",
            "Epoch 1/10, Loss: 0.5594125280380249\n",
            "Epoch 1/10, Loss: 0.561946551322937\n",
            "Epoch 1/10, Loss: 0.5645610382556915\n",
            "Epoch 1/10, Loss: 0.567102549791336\n",
            "Epoch 1/10, Loss: 0.569765142917633\n",
            "Epoch 1/10, Loss: 0.5723427650928498\n",
            "Epoch 1/10, Loss: 0.5748472137451172\n",
            "Epoch 1/10, Loss: 0.5773366684913636\n",
            "Epoch 1/10, Loss: 0.5798223869800567\n",
            "Epoch 1/10, Loss: 0.5822738597393036\n",
            "Epoch 1/10, Loss: 0.5848047997951508\n",
            "Epoch 1/10, Loss: 0.5872693619728089\n",
            "Epoch 1/10, Loss: 0.5897277636528016\n",
            "Epoch 1/10, Loss: 0.5922115707397461\n",
            "Epoch 1/10, Loss: 0.5946540246009827\n",
            "Epoch 1/10, Loss: 0.5971466624736785\n",
            "Epoch 1/10, Loss: 0.5995724008083344\n",
            "Epoch 1/10, Loss: 0.6021224398612977\n",
            "Epoch 1/10, Loss: 0.6045869131088257\n",
            "Epoch 1/10, Loss: 0.6071987180709839\n",
            "Epoch 1/10, Loss: 0.6097849683761597\n",
            "Epoch 1/10, Loss: 0.6122942509651184\n",
            "Epoch 1/10, Loss: 0.6146759893894196\n",
            "Epoch 1/10, Loss: 0.617108361005783\n",
            "Epoch 1/10, Loss: 0.6196846318244934\n",
            "Epoch 1/10, Loss: 0.6222001650333404\n",
            "Epoch 1/10, Loss: 0.6246710081100464\n",
            "Epoch 1/10, Loss: 0.6270531513690949\n",
            "Epoch 1/10, Loss: 0.629524863243103\n",
            "Epoch 1/10, Loss: 0.6319461801052093\n",
            "Epoch 1/10, Loss: 0.6343292887210846\n",
            "Epoch 1/10, Loss: 0.6368725225925446\n",
            "Epoch 1/10, Loss: 0.6392373068332672\n",
            "Epoch 1/10, Loss: 0.6417475259304046\n",
            "Epoch 1/10, Loss: 0.644299563407898\n",
            "Epoch 1/10, Loss: 0.646685827255249\n",
            "Epoch 1/10, Loss: 0.6489774689674378\n",
            "Epoch 1/10, Loss: 0.651336091041565\n",
            "Epoch 1/10, Loss: 0.6537799060344696\n",
            "Epoch 1/10, Loss: 0.6560874965190887\n",
            "Epoch 1/10, Loss: 0.6583625202178955\n",
            "Epoch 1/10, Loss: 0.6606666071414947\n",
            "Epoch 1/10, Loss: 0.6629702281951905\n",
            "Epoch 1/10, Loss: 0.6653147177696228\n",
            "Epoch 2/10, Loss: 0.002299017906188965\n",
            "Epoch 2/10, Loss: 0.004738235712051391\n",
            "Epoch 2/10, Loss: 0.007129103660583496\n",
            "Epoch 2/10, Loss: 0.009526911735534669\n",
            "Epoch 2/10, Loss: 0.011890182733535766\n",
            "Epoch 2/10, Loss: 0.01427353835105896\n",
            "Epoch 2/10, Loss: 0.016471338987350464\n",
            "Epoch 2/10, Loss: 0.018987177610397338\n",
            "Epoch 2/10, Loss: 0.021323543071746828\n",
            "Epoch 2/10, Loss: 0.023663275241851807\n",
            "Epoch 2/10, Loss: 0.026059537649154665\n",
            "Epoch 2/10, Loss: 0.02838183617591858\n",
            "Epoch 2/10, Loss: 0.030606661796569824\n",
            "Epoch 2/10, Loss: 0.0330272912979126\n",
            "Epoch 2/10, Loss: 0.035328301429748535\n",
            "Epoch 2/10, Loss: 0.03778621339797974\n",
            "Epoch 2/10, Loss: 0.04003839707374573\n",
            "Epoch 2/10, Loss: 0.04232075762748718\n",
            "Epoch 2/10, Loss: 0.04451806497573853\n",
            "Epoch 2/10, Loss: 0.04685083794593811\n",
            "Epoch 2/10, Loss: 0.049040786981582644\n",
            "Epoch 2/10, Loss: 0.05152587914466858\n",
            "Epoch 2/10, Loss: 0.053972597122192385\n",
            "Epoch 2/10, Loss: 0.056186880588531496\n",
            "Epoch 2/10, Loss: 0.05864846706390381\n",
            "Epoch 2/10, Loss: 0.06091198992729187\n",
            "Epoch 2/10, Loss: 0.0632387137413025\n",
            "Epoch 2/10, Loss: 0.06536866760253907\n",
            "Epoch 2/10, Loss: 0.06766830134391785\n",
            "Epoch 2/10, Loss: 0.06988337111473084\n",
            "Epoch 2/10, Loss: 0.0720854287147522\n",
            "Epoch 2/10, Loss: 0.07436582851409912\n",
            "Epoch 2/10, Loss: 0.07662714123725892\n",
            "Epoch 2/10, Loss: 0.07876242971420289\n",
            "Epoch 2/10, Loss: 0.08101337552070617\n",
            "Epoch 2/10, Loss: 0.08317917132377625\n",
            "Epoch 2/10, Loss: 0.08535417532920837\n",
            "Epoch 2/10, Loss: 0.08771389794349671\n",
            "Epoch 2/10, Loss: 0.09009061241149903\n",
            "Epoch 2/10, Loss: 0.09213618803024291\n",
            "Epoch 2/10, Loss: 0.09434940433502197\n",
            "Epoch 2/10, Loss: 0.09666458249092102\n",
            "Epoch 2/10, Loss: 0.09897742176055908\n",
            "Epoch 2/10, Loss: 0.10127028822898865\n",
            "Epoch 2/10, Loss: 0.10341051149368286\n",
            "Epoch 2/10, Loss: 0.1054671597480774\n",
            "Epoch 2/10, Loss: 0.10756765413284301\n",
            "Epoch 2/10, Loss: 0.10987481021881104\n",
            "Epoch 2/10, Loss: 0.11197270131111145\n",
            "Epoch 2/10, Loss: 0.11430437541007996\n",
            "Epoch 2/10, Loss: 0.11640650773048401\n",
            "Epoch 2/10, Loss: 0.11840484499931335\n",
            "Epoch 2/10, Loss: 0.12077476072311401\n",
            "Epoch 2/10, Loss: 0.12303782606124877\n",
            "Epoch 2/10, Loss: 0.12544318103790283\n",
            "Epoch 2/10, Loss: 0.12774450635910034\n",
            "Epoch 2/10, Loss: 0.1298202931880951\n",
            "Epoch 2/10, Loss: 0.13190556836128234\n",
            "Epoch 2/10, Loss: 0.13413887834548952\n",
            "Epoch 2/10, Loss: 0.1364051365852356\n",
            "Epoch 2/10, Loss: 0.13851248908042907\n",
            "Epoch 2/10, Loss: 0.14045726060867308\n",
            "Epoch 2/10, Loss: 0.142716171503067\n",
            "Epoch 2/10, Loss: 0.14480811429023743\n",
            "Epoch 2/10, Loss: 0.1467824194431305\n",
            "Epoch 2/10, Loss: 0.14898042464256286\n",
            "Epoch 2/10, Loss: 0.15103626203536988\n",
            "Epoch 2/10, Loss: 0.15304217338562012\n",
            "Epoch 2/10, Loss: 0.15510730648040771\n",
            "Epoch 2/10, Loss: 0.157155033826828\n",
            "Epoch 2/10, Loss: 0.15945244908332826\n",
            "Epoch 2/10, Loss: 0.16158068537712098\n",
            "Epoch 2/10, Loss: 0.1637532958984375\n",
            "Epoch 2/10, Loss: 0.1657600803375244\n",
            "Epoch 2/10, Loss: 0.16794316959381103\n",
            "Epoch 2/10, Loss: 0.17006481099128723\n",
            "Epoch 2/10, Loss: 0.1722131655216217\n",
            "Epoch 2/10, Loss: 0.1744134397506714\n",
            "Epoch 2/10, Loss: 0.17648778295516968\n",
            "Epoch 2/10, Loss: 0.1786199345588684\n",
            "Epoch 2/10, Loss: 0.18096866941452025\n",
            "Epoch 2/10, Loss: 0.18302091598510742\n",
            "Epoch 2/10, Loss: 0.18521371364593506\n",
            "Epoch 2/10, Loss: 0.18740176892280577\n",
            "Epoch 2/10, Loss: 0.1893867461681366\n",
            "Epoch 2/10, Loss: 0.19158186411857606\n",
            "Epoch 2/10, Loss: 0.19380026769638062\n",
            "Epoch 2/10, Loss: 0.1959739305973053\n",
            "Epoch 2/10, Loss: 0.1983016653060913\n",
            "Epoch 2/10, Loss: 0.20038626885414124\n",
            "Epoch 2/10, Loss: 0.20251863431930542\n",
            "Epoch 2/10, Loss: 0.20475925660133362\n",
            "Epoch 2/10, Loss: 0.20693015575408935\n",
            "Epoch 2/10, Loss: 0.209148099899292\n",
            "Epoch 2/10, Loss: 0.21112915062904358\n",
            "Epoch 2/10, Loss: 0.21331401705741881\n",
            "Epoch 2/10, Loss: 0.21503634691238405\n",
            "Epoch 2/10, Loss: 0.2171866774559021\n",
            "Epoch 2/10, Loss: 0.219231586933136\n",
            "Epoch 2/10, Loss: 0.22110588765144348\n",
            "Epoch 2/10, Loss: 0.22322067475318907\n",
            "Epoch 2/10, Loss: 0.22524966216087342\n",
            "Epoch 2/10, Loss: 0.22722445368766786\n",
            "Epoch 2/10, Loss: 0.22930975484848024\n",
            "Epoch 2/10, Loss: 0.23138064980506898\n",
            "Epoch 2/10, Loss: 0.23346435976028443\n",
            "Epoch 2/10, Loss: 0.23552745246887208\n",
            "Epoch 2/10, Loss: 0.23762392497062684\n",
            "Epoch 2/10, Loss: 0.23979676485061646\n",
            "Epoch 2/10, Loss: 0.2419553108215332\n",
            "Epoch 2/10, Loss: 0.24439315366744996\n",
            "Epoch 2/10, Loss: 0.2464399218559265\n",
            "Epoch 2/10, Loss: 0.24850180459022522\n",
            "Epoch 2/10, Loss: 0.25066698622703554\n",
            "Epoch 2/10, Loss: 0.25261334037780764\n",
            "Epoch 2/10, Loss: 0.2548296973705292\n",
            "Epoch 2/10, Loss: 0.25681528401374815\n",
            "Epoch 2/10, Loss: 0.258806587934494\n",
            "Epoch 2/10, Loss: 0.2607449060678482\n",
            "Epoch 2/10, Loss: 0.26268599641323087\n",
            "Epoch 2/10, Loss: 0.2647502518892288\n",
            "Epoch 2/10, Loss: 0.26676817214488985\n",
            "Epoch 2/10, Loss: 0.2691981955766678\n",
            "Epoch 2/10, Loss: 0.27113095951080324\n",
            "Epoch 2/10, Loss: 0.2732682259082794\n",
            "Epoch 2/10, Loss: 0.2751420199871063\n",
            "Epoch 2/10, Loss: 0.27724506068229676\n",
            "Epoch 2/10, Loss: 0.2790788410902023\n",
            "Epoch 2/10, Loss: 0.28097985112667084\n",
            "Epoch 2/10, Loss: 0.28301752531528473\n",
            "Epoch 2/10, Loss: 0.28517581689357757\n",
            "Epoch 2/10, Loss: 0.28697237598896025\n",
            "Epoch 2/10, Loss: 0.2889044243097305\n",
            "Epoch 2/10, Loss: 0.2908416839838028\n",
            "Epoch 2/10, Loss: 0.2929075857400894\n",
            "Epoch 2/10, Loss: 0.2948515976667404\n",
            "Epoch 2/10, Loss: 0.29703657829761504\n",
            "Epoch 2/10, Loss: 0.299009406208992\n",
            "Epoch 2/10, Loss: 0.3011225241422653\n",
            "Epoch 2/10, Loss: 0.3030413695573807\n",
            "Epoch 2/10, Loss: 0.30500062549114226\n",
            "Epoch 2/10, Loss: 0.30730340206623075\n",
            "Epoch 2/10, Loss: 0.30962852227687837\n",
            "Epoch 2/10, Loss: 0.31171160328388214\n",
            "Epoch 2/10, Loss: 0.31379866778850557\n",
            "Epoch 2/10, Loss: 0.3160627237558365\n",
            "Epoch 2/10, Loss: 0.3178310744762421\n",
            "Epoch 2/10, Loss: 0.31983206343650816\n",
            "Epoch 2/10, Loss: 0.3217536225318909\n",
            "Epoch 2/10, Loss: 0.3234921666383743\n",
            "Epoch 2/10, Loss: 0.3256164027452469\n",
            "Epoch 2/10, Loss: 0.3276178158521652\n",
            "Epoch 2/10, Loss: 0.3296294199228287\n",
            "Epoch 2/10, Loss: 0.33141908252239227\n",
            "Epoch 2/10, Loss: 0.3333911896944046\n",
            "Epoch 2/10, Loss: 0.33546659362316134\n",
            "Epoch 2/10, Loss: 0.3373919667005539\n",
            "Epoch 2/10, Loss: 0.33919386243820193\n",
            "Epoch 2/10, Loss: 0.3411031229496002\n",
            "Epoch 2/10, Loss: 0.34287244868278505\n",
            "Epoch 2/10, Loss: 0.34491089057922364\n",
            "Epoch 2/10, Loss: 0.34678007757663726\n",
            "Epoch 2/10, Loss: 0.34866403114795685\n",
            "Epoch 2/10, Loss: 0.3505393851995468\n",
            "Epoch 2/10, Loss: 0.35239578557014467\n",
            "Epoch 2/10, Loss: 0.3542141146659851\n",
            "Epoch 2/10, Loss: 0.35612513101100923\n",
            "Epoch 2/10, Loss: 0.35790901362895966\n",
            "Epoch 2/10, Loss: 0.359778977394104\n",
            "Epoch 2/10, Loss: 0.3617366772890091\n",
            "Epoch 2/10, Loss: 0.3636246393918991\n",
            "Epoch 2/10, Loss: 0.3653809164762497\n",
            "Epoch 2/10, Loss: 0.36713050258159635\n",
            "Epoch 2/10, Loss: 0.36887622785568236\n",
            "Epoch 2/10, Loss: 0.37110021352767947\n",
            "Epoch 2/10, Loss: 0.3732076449394226\n",
            "Epoch 2/10, Loss: 0.375271910905838\n",
            "Epoch 2/10, Loss: 0.3771578402519226\n",
            "Epoch 2/10, Loss: 0.3789593594074249\n",
            "Epoch 2/10, Loss: 0.3810823225975037\n",
            "Epoch 2/10, Loss: 0.3832721621990204\n",
            "Epoch 2/10, Loss: 0.38497909712791445\n",
            "Epoch 2/10, Loss: 0.3869076496362686\n",
            "Epoch 2/10, Loss: 0.38935563004016877\n",
            "Epoch 2/10, Loss: 0.3911430050134659\n",
            "Epoch 2/10, Loss: 0.392959059715271\n",
            "Epoch 2/10, Loss: 0.39517787837982177\n",
            "Epoch 2/10, Loss: 0.3973955171108246\n",
            "Epoch 2/10, Loss: 0.3994307994842529\n",
            "Epoch 2/10, Loss: 0.4018141615390778\n",
            "Epoch 2/10, Loss: 0.40406376314163206\n",
            "Epoch 2/10, Loss: 0.4059956201314926\n",
            "Epoch 2/10, Loss: 0.40795548486709593\n",
            "Epoch 2/10, Loss: 0.4097679953575134\n",
            "Epoch 2/10, Loss: 0.4117120144367218\n",
            "Epoch 2/10, Loss: 0.4137891170978546\n",
            "Epoch 2/10, Loss: 0.4159801661968231\n",
            "Epoch 2/10, Loss: 0.41770733654499054\n",
            "Epoch 2/10, Loss: 0.4193261469602585\n",
            "Epoch 2/10, Loss: 0.4211193128824234\n",
            "Epoch 2/10, Loss: 0.4228147863149643\n",
            "Epoch 2/10, Loss: 0.42484368598461153\n",
            "Epoch 2/10, Loss: 0.4269031199216843\n",
            "Epoch 2/10, Loss: 0.42901862466335294\n",
            "Epoch 2/10, Loss: 0.4307945908308029\n",
            "Epoch 2/10, Loss: 0.4325964467525482\n",
            "Epoch 2/10, Loss: 0.43437254869937897\n",
            "Epoch 2/10, Loss: 0.4364553319215774\n",
            "Epoch 2/10, Loss: 0.4384653471708298\n",
            "Epoch 2/10, Loss: 0.44046649396419524\n",
            "Epoch 2/10, Loss: 0.4426528984308243\n",
            "Epoch 2/10, Loss: 0.44453821778297425\n",
            "Epoch 2/10, Loss: 0.44630283081531524\n",
            "Epoch 2/10, Loss: 0.44814966595172884\n",
            "Epoch 2/10, Loss: 0.4498108240365982\n",
            "Epoch 2/10, Loss: 0.45154007720947265\n",
            "Epoch 2/10, Loss: 0.4534392533302307\n",
            "Epoch 2/10, Loss: 0.45506600999832153\n",
            "Epoch 2/10, Loss: 0.4570984742641449\n",
            "Epoch 2/10, Loss: 0.4589194598197937\n",
            "Epoch 2/10, Loss: 0.4605265266895294\n",
            "Epoch 2/10, Loss: 0.4624603329896927\n",
            "Epoch 2/10, Loss: 0.4640807023048401\n",
            "Epoch 2/10, Loss: 0.4661799397468567\n",
            "Epoch 2/10, Loss: 0.4679211525917053\n",
            "Epoch 2/10, Loss: 0.4699695327281952\n",
            "Epoch 2/10, Loss: 0.47218859481811526\n",
            "Epoch 2/10, Loss: 0.4740812522172928\n",
            "Epoch 2/10, Loss: 0.47582800149917603\n",
            "Epoch 2/10, Loss: 0.47753197884559634\n",
            "Epoch 2/10, Loss: 0.47956547236442565\n",
            "Epoch 2/10, Loss: 0.4815749180316925\n",
            "Epoch 2/10, Loss: 0.48355736553668976\n",
            "Epoch 2/10, Loss: 0.4852258890867233\n",
            "Epoch 2/10, Loss: 0.4871791706085205\n",
            "Epoch 2/10, Loss: 0.48896028828620913\n",
            "Epoch 2/10, Loss: 0.49066659426689146\n",
            "Epoch 2/10, Loss: 0.49280082201957703\n",
            "Epoch 2/10, Loss: 0.49454076671600344\n",
            "Epoch 2/10, Loss: 0.4964182260036468\n",
            "Epoch 2/10, Loss: 0.4986168475151062\n",
            "Epoch 2/10, Loss: 0.5004698630571366\n",
            "Epoch 2/10, Loss: 0.5021033267974854\n",
            "Epoch 2/10, Loss: 0.5039109525680542\n",
            "Epoch 2/10, Loss: 0.5059563021659851\n",
            "Epoch 2/10, Loss: 0.5075609173774719\n",
            "Epoch 2/10, Loss: 0.509280488729477\n",
            "Epoch 2/10, Loss: 0.510866961479187\n",
            "Epoch 2/10, Loss: 0.5125372289419174\n",
            "Epoch 2/10, Loss: 0.5142214519977569\n",
            "Epoch 3/10, Loss: 0.0017037776708602906\n",
            "Epoch 3/10, Loss: 0.003601951003074646\n",
            "Epoch 3/10, Loss: 0.0054730850458145144\n",
            "Epoch 3/10, Loss: 0.007282959580421447\n",
            "Epoch 3/10, Loss: 0.009037710189819336\n",
            "Epoch 3/10, Loss: 0.010935518264770508\n",
            "Epoch 3/10, Loss: 0.01246926462650299\n",
            "Epoch 3/10, Loss: 0.014466137051582336\n",
            "Epoch 3/10, Loss: 0.016266041040420533\n",
            "Epoch 3/10, Loss: 0.01802627456188202\n",
            "Epoch 3/10, Loss: 0.019939308285713195\n",
            "Epoch 3/10, Loss: 0.021620015501976012\n",
            "Epoch 3/10, Loss: 0.02326960074901581\n",
            "Epoch 3/10, Loss: 0.02517335879802704\n",
            "Epoch 3/10, Loss: 0.026916449189186096\n",
            "Epoch 3/10, Loss: 0.029151289582252504\n",
            "Epoch 3/10, Loss: 0.030818393111228942\n",
            "Epoch 3/10, Loss: 0.0326276627779007\n",
            "Epoch 3/10, Loss: 0.03421040332317352\n",
            "Epoch 3/10, Loss: 0.036119141817092894\n",
            "Epoch 3/10, Loss: 0.0377628835439682\n",
            "Epoch 3/10, Loss: 0.03993449175357819\n",
            "Epoch 3/10, Loss: 0.04197406804561615\n",
            "Epoch 3/10, Loss: 0.043637614488601685\n",
            "Epoch 3/10, Loss: 0.045741364240646364\n",
            "Epoch 3/10, Loss: 0.04737850451469421\n",
            "Epoch 3/10, Loss: 0.04928799152374268\n",
            "Epoch 3/10, Loss: 0.050829478859901425\n",
            "Epoch 3/10, Loss: 0.05261424291133881\n",
            "Epoch 3/10, Loss: 0.05424554753303528\n",
            "Epoch 3/10, Loss: 0.05584461784362793\n",
            "Epoch 3/10, Loss: 0.05780820155143738\n",
            "Epoch 3/10, Loss: 0.059553632378578185\n",
            "Epoch 3/10, Loss: 0.06124997293949127\n",
            "Epoch 3/10, Loss: 0.06316579759120941\n",
            "Epoch 3/10, Loss: 0.06484925329685211\n",
            "Epoch 3/10, Loss: 0.0665833728313446\n",
            "Epoch 3/10, Loss: 0.06862239408493041\n",
            "Epoch 3/10, Loss: 0.07047592449188232\n",
            "Epoch 3/10, Loss: 0.07198052096366882\n",
            "Epoch 3/10, Loss: 0.07380458498001098\n",
            "Epoch 3/10, Loss: 0.07584604072570801\n",
            "Epoch 3/10, Loss: 0.07781017780303955\n",
            "Epoch 3/10, Loss: 0.07980377924442292\n",
            "Epoch 3/10, Loss: 0.08148892724514008\n",
            "Epoch 3/10, Loss: 0.083098219871521\n",
            "Epoch 3/10, Loss: 0.08465624380111694\n",
            "Epoch 3/10, Loss: 0.08663296687602996\n",
            "Epoch 3/10, Loss: 0.0883037737607956\n",
            "Epoch 3/10, Loss: 0.09028665292263031\n",
            "Epoch 3/10, Loss: 0.0919560604095459\n",
            "Epoch 3/10, Loss: 0.09351122438907623\n",
            "Epoch 3/10, Loss: 0.09563547909259797\n",
            "Epoch 3/10, Loss: 0.09732440960407257\n",
            "Epoch 3/10, Loss: 0.09961906778812409\n",
            "Epoch 3/10, Loss: 0.10167294418811798\n",
            "Epoch 3/10, Loss: 0.10332057404518127\n",
            "Epoch 3/10, Loss: 0.10495963835716247\n",
            "Epoch 3/10, Loss: 0.10703844881057739\n",
            "Epoch 3/10, Loss: 0.10903566563129426\n",
            "Epoch 3/10, Loss: 0.11090329146385193\n",
            "Epoch 3/10, Loss: 0.11250818061828613\n",
            "Epoch 3/10, Loss: 0.11443272709846497\n",
            "Epoch 3/10, Loss: 0.11610738277435302\n",
            "Epoch 3/10, Loss: 0.11765693485736847\n",
            "Epoch 3/10, Loss: 0.11949845385551452\n",
            "Epoch 3/10, Loss: 0.12110307359695435\n",
            "Epoch 3/10, Loss: 0.12274197244644165\n",
            "Epoch 3/10, Loss: 0.12447586345672608\n",
            "Epoch 3/10, Loss: 0.1260953118801117\n",
            "Epoch 3/10, Loss: 0.1282737033367157\n",
            "Epoch 3/10, Loss: 0.12998161137104033\n",
            "Epoch 3/10, Loss: 0.13172837030887605\n",
            "Epoch 3/10, Loss: 0.13333577811717987\n",
            "Epoch 3/10, Loss: 0.13510064375400543\n",
            "Epoch 3/10, Loss: 0.13695709788799285\n",
            "Epoch 3/10, Loss: 0.13880016529560088\n",
            "Epoch 3/10, Loss: 0.14070915138721465\n",
            "Epoch 3/10, Loss: 0.14260259318351745\n",
            "Epoch 3/10, Loss: 0.14439057242870332\n",
            "Epoch 3/10, Loss: 0.14641554343700408\n",
            "Epoch 3/10, Loss: 0.14814237427711488\n",
            "Epoch 3/10, Loss: 0.15005099034309388\n",
            "Epoch 3/10, Loss: 0.15206681513786316\n",
            "Epoch 3/10, Loss: 0.15364328467845917\n",
            "Epoch 3/10, Loss: 0.15552901208400727\n",
            "Epoch 3/10, Loss: 0.1575017114877701\n",
            "Epoch 3/10, Loss: 0.15921304774284362\n",
            "Epoch 3/10, Loss: 0.16128810000419616\n",
            "Epoch 3/10, Loss: 0.16304249250888825\n",
            "Epoch 3/10, Loss: 0.16484365797042846\n",
            "Epoch 3/10, Loss: 0.1669065408706665\n",
            "Epoch 3/10, Loss: 0.1687974588871002\n",
            "Epoch 3/10, Loss: 0.1706681262254715\n",
            "Epoch 3/10, Loss: 0.17234796524047852\n",
            "Epoch 3/10, Loss: 0.17435304737091065\n",
            "Epoch 3/10, Loss: 0.1757566180229187\n",
            "Epoch 3/10, Loss: 0.17764185655117035\n",
            "Epoch 3/10, Loss: 0.17934140491485595\n",
            "Epoch 3/10, Loss: 0.1808215421438217\n",
            "Epoch 3/10, Loss: 0.18257267987728118\n",
            "Epoch 3/10, Loss: 0.18432422268390655\n",
            "Epoch 3/10, Loss: 0.1859833734035492\n",
            "Epoch 3/10, Loss: 0.18778468358516692\n",
            "Epoch 3/10, Loss: 0.18954998672008513\n",
            "Epoch 3/10, Loss: 0.19133020091056824\n",
            "Epoch 3/10, Loss: 0.19307626605033876\n",
            "Epoch 3/10, Loss: 0.1949065897464752\n",
            "Epoch 3/10, Loss: 0.1967718470096588\n",
            "Epoch 3/10, Loss: 0.19866473245620728\n",
            "Epoch 3/10, Loss: 0.20095576882362365\n",
            "Epoch 3/10, Loss: 0.20267591309547425\n",
            "Epoch 3/10, Loss: 0.20443242037296294\n",
            "Epoch 3/10, Loss: 0.20629582250118256\n",
            "Epoch 3/10, Loss: 0.20790246748924254\n",
            "Epoch 3/10, Loss: 0.20987483417987823\n",
            "Epoch 3/10, Loss: 0.21165777492523194\n",
            "Epoch 3/10, Loss: 0.21341601538658142\n",
            "Epoch 3/10, Loss: 0.21515809762477875\n",
            "Epoch 3/10, Loss: 0.2168379465341568\n",
            "Epoch 3/10, Loss: 0.21857347333431243\n",
            "Epoch 3/10, Loss: 0.22032061100006103\n",
            "Epoch 3/10, Loss: 0.2226856029033661\n",
            "Epoch 3/10, Loss: 0.22422126245498658\n",
            "Epoch 3/10, Loss: 0.22612181580066681\n",
            "Epoch 3/10, Loss: 0.22775395786762237\n",
            "Epoch 3/10, Loss: 0.22973151063919067\n",
            "Epoch 3/10, Loss: 0.23126967310905455\n",
            "Epoch 3/10, Loss: 0.23292824339866638\n",
            "Epoch 3/10, Loss: 0.2348480155467987\n",
            "Epoch 3/10, Loss: 0.23684928488731385\n",
            "Epoch 3/10, Loss: 0.2384334944486618\n",
            "Epoch 3/10, Loss: 0.2401732621192932\n",
            "Epoch 3/10, Loss: 0.24197171437740325\n",
            "Epoch 3/10, Loss: 0.243767480134964\n",
            "Epoch 3/10, Loss: 0.24541200518608094\n",
            "Epoch 3/10, Loss: 0.247384459733963\n",
            "Epoch 3/10, Loss: 0.24913629031181336\n",
            "Epoch 3/10, Loss: 0.2510495595932007\n",
            "Epoch 3/10, Loss: 0.252735512137413\n",
            "Epoch 3/10, Loss: 0.2546173666715622\n",
            "Epoch 3/10, Loss: 0.2568359297513962\n",
            "Epoch 3/10, Loss: 0.2590640038251877\n",
            "Epoch 3/10, Loss: 0.26100121581554414\n",
            "Epoch 3/10, Loss: 0.2629824991226196\n",
            "Epoch 3/10, Loss: 0.26518948531150816\n",
            "Epoch 3/10, Loss: 0.26665687811374666\n",
            "Epoch 3/10, Loss: 0.2686008039712906\n",
            "Epoch 3/10, Loss: 0.2703022118806839\n",
            "Epoch 3/10, Loss: 0.2717775532007217\n",
            "Epoch 3/10, Loss: 0.2738735171556473\n",
            "Epoch 3/10, Loss: 0.2756286097764969\n",
            "Epoch 3/10, Loss: 0.27744767427444456\n",
            "Epoch 3/10, Loss: 0.2791042686700821\n",
            "Epoch 3/10, Loss: 0.280908358335495\n",
            "Epoch 3/10, Loss: 0.28274902737140656\n",
            "Epoch 3/10, Loss: 0.28441226136684417\n",
            "Epoch 3/10, Loss: 0.28591321778297424\n",
            "Epoch 3/10, Loss: 0.28757492327690126\n",
            "Epoch 3/10, Loss: 0.28911902475357054\n",
            "Epoch 3/10, Loss: 0.29105567002296445\n",
            "Epoch 3/10, Loss: 0.29268481707572935\n",
            "Epoch 3/10, Loss: 0.29442004358768464\n",
            "Epoch 3/10, Loss: 0.2960624450445175\n",
            "Epoch 3/10, Loss: 0.2977048444747925\n",
            "Epoch 3/10, Loss: 0.29936227011680605\n",
            "Epoch 3/10, Loss: 0.3011285825967789\n",
            "Epoch 3/10, Loss: 0.30278989756107333\n",
            "Epoch 3/10, Loss: 0.30439601790904997\n",
            "Epoch 3/10, Loss: 0.3062193366289139\n",
            "Epoch 3/10, Loss: 0.30796761643886567\n",
            "Epoch 3/10, Loss: 0.30946082413196563\n",
            "Epoch 3/10, Loss: 0.3110580199956894\n",
            "Epoch 3/10, Loss: 0.31251623678207396\n",
            "Epoch 3/10, Loss: 0.3146271438598633\n",
            "Epoch 3/10, Loss: 0.31659566283226015\n",
            "Epoch 3/10, Loss: 0.31856657910346986\n",
            "Epoch 3/10, Loss: 0.32027727174758913\n",
            "Epoch 3/10, Loss: 0.32188379383087157\n",
            "Epoch 3/10, Loss: 0.3239006505012512\n",
            "Epoch 3/10, Loss: 0.3260385940074921\n",
            "Epoch 3/10, Loss: 0.3274990805387497\n",
            "Epoch 3/10, Loss: 0.3292812150716782\n",
            "Epoch 3/10, Loss: 0.33178907215595244\n",
            "Epoch 3/10, Loss: 0.3333670654296875\n",
            "Epoch 3/10, Loss: 0.33499959695339204\n",
            "Epoch 3/10, Loss: 0.33715275657176974\n",
            "Epoch 3/10, Loss: 0.3393293710947037\n",
            "Epoch 3/10, Loss: 0.34115909779071807\n",
            "Epoch 3/10, Loss: 0.34348717439174653\n",
            "Epoch 3/10, Loss: 0.3457770482301712\n",
            "Epoch 3/10, Loss: 0.3475060123205185\n",
            "Epoch 3/10, Loss: 0.3493376981019974\n",
            "Epoch 3/10, Loss: 0.3509860721826553\n",
            "Epoch 3/10, Loss: 0.3527174918651581\n",
            "Epoch 3/10, Loss: 0.3546549937725067\n",
            "Epoch 3/10, Loss: 0.35676129150390623\n",
            "Epoch 3/10, Loss: 0.3582387698888779\n",
            "Epoch 3/10, Loss: 0.35968609297275544\n",
            "Epoch 3/10, Loss: 0.36136021447181704\n",
            "Epoch 3/10, Loss: 0.3628514088392258\n",
            "Epoch 3/10, Loss: 0.36478930759429934\n",
            "Epoch 3/10, Loss: 0.36673896765708924\n",
            "Epoch 3/10, Loss: 0.36873498606681826\n",
            "Epoch 3/10, Loss: 0.3702602026462555\n",
            "Epoch 3/10, Loss: 0.3719230448007584\n",
            "Epoch 3/10, Loss: 0.3735423365831375\n",
            "Epoch 3/10, Loss: 0.3755516027212143\n",
            "Epoch 3/10, Loss: 0.37740460562705996\n",
            "Epoch 3/10, Loss: 0.3793062999248505\n",
            "Epoch 3/10, Loss: 0.38139417934417724\n",
            "Epoch 3/10, Loss: 0.38303691637516024\n",
            "Epoch 3/10, Loss: 0.38466732382774355\n",
            "Epoch 3/10, Loss: 0.3864398379325867\n",
            "Epoch 3/10, Loss: 0.38794223582744597\n",
            "Epoch 3/10, Loss: 0.3895247421264648\n",
            "Epoch 3/10, Loss: 0.39129410827159883\n",
            "Epoch 3/10, Loss: 0.3927625478506088\n",
            "Epoch 3/10, Loss: 0.3946908664703369\n",
            "Epoch 3/10, Loss: 0.3963759918212891\n",
            "Epoch 3/10, Loss: 0.39777792525291444\n",
            "Epoch 3/10, Loss: 0.3996020429134369\n",
            "Epoch 3/10, Loss: 0.40102885794639587\n",
            "Epoch 3/10, Loss: 0.40307705926895143\n",
            "Epoch 3/10, Loss: 0.40468115723133086\n",
            "Epoch 3/10, Loss: 0.406580069065094\n",
            "Epoch 3/10, Loss: 0.40875516390800476\n",
            "Epoch 3/10, Loss: 0.4104955632686615\n",
            "Epoch 3/10, Loss: 0.4121191667318344\n",
            "Epoch 3/10, Loss: 0.4136343199014664\n",
            "Epoch 3/10, Loss: 0.41558405113220215\n",
            "Epoch 3/10, Loss: 0.41752227771282197\n",
            "Epoch 3/10, Loss: 0.4193646351099014\n",
            "Epoch 3/10, Loss: 0.4208658682107925\n",
            "Epoch 3/10, Loss: 0.4227186013460159\n",
            "Epoch 3/10, Loss: 0.42435913360118865\n",
            "Epoch 3/10, Loss: 0.42589599537849426\n",
            "Epoch 3/10, Loss: 0.4279590284824371\n",
            "Epoch 3/10, Loss: 0.4295265234708786\n",
            "Epoch 3/10, Loss: 0.4313091181516647\n",
            "Epoch 3/10, Loss: 0.4333935445547104\n",
            "Epoch 3/10, Loss: 0.43519019055366515\n",
            "Epoch 3/10, Loss: 0.4366769758462906\n",
            "Epoch 3/10, Loss: 0.43835704481601717\n",
            "Epoch 3/10, Loss: 0.4403520203828812\n",
            "Epoch 3/10, Loss: 0.44180995965003966\n",
            "Epoch 3/10, Loss: 0.4434425597190857\n",
            "Epoch 3/10, Loss: 0.44486225962638853\n",
            "Epoch 3/10, Loss: 0.4464304574728012\n",
            "Epoch 3/10, Loss: 0.4480486708879471\n",
            "Epoch 4/10, Loss: 0.0015929980278015137\n",
            "Epoch 4/10, Loss: 0.00339171826839447\n",
            "Epoch 4/10, Loss: 0.00519940459728241\n",
            "Epoch 4/10, Loss: 0.0068806569576263426\n",
            "Epoch 4/10, Loss: 0.008513745665550233\n",
            "Epoch 4/10, Loss: 0.010337934136390685\n",
            "Epoch 4/10, Loss: 0.011739399433135986\n",
            "Epoch 4/10, Loss: 0.013648560881614685\n",
            "Epoch 4/10, Loss: 0.015343827962875365\n",
            "Epoch 4/10, Loss: 0.016965771436691285\n",
            "Epoch 4/10, Loss: 0.0187902352809906\n",
            "Epoch 4/10, Loss: 0.020368690133094787\n",
            "Epoch 4/10, Loss: 0.021937948107719422\n",
            "Epoch 4/10, Loss: 0.02376049268245697\n",
            "Epoch 4/10, Loss: 0.025369444727897643\n",
            "Epoch 4/10, Loss: 0.027627106070518492\n",
            "Epoch 4/10, Loss: 0.029188719391822816\n",
            "Epoch 4/10, Loss: 0.03090042805671692\n",
            "Epoch 4/10, Loss: 0.03231946766376495\n",
            "Epoch 4/10, Loss: 0.03413213658332825\n",
            "Epoch 4/10, Loss: 0.03567823112010956\n",
            "Epoch 4/10, Loss: 0.037794682383537295\n",
            "Epoch 4/10, Loss: 0.03976760101318359\n",
            "Epoch 4/10, Loss: 0.041337067365646366\n",
            "Epoch 4/10, Loss: 0.04336010241508484\n",
            "Epoch 4/10, Loss: 0.044816897273063656\n",
            "Epoch 4/10, Loss: 0.04664203190803528\n",
            "Epoch 4/10, Loss: 0.0480602685213089\n",
            "Epoch 4/10, Loss: 0.049715035200119016\n",
            "Epoch 4/10, Loss: 0.051169924974441526\n",
            "Epoch 4/10, Loss: 0.05262654864788056\n",
            "Epoch 4/10, Loss: 0.05453691649436951\n",
            "Epoch 4/10, Loss: 0.05617042279243469\n",
            "Epoch 4/10, Loss: 0.057777658581733705\n",
            "Epoch 4/10, Loss: 0.059601115226745605\n",
            "Epoch 4/10, Loss: 0.06110440921783447\n",
            "Epoch 4/10, Loss: 0.06277085053920746\n",
            "Epoch 4/10, Loss: 0.06477028572559357\n",
            "Epoch 4/10, Loss: 0.06650117790699005\n",
            "Epoch 4/10, Loss: 0.06793334066867829\n",
            "Epoch 4/10, Loss: 0.069697234749794\n",
            "Epoch 4/10, Loss: 0.0717119950056076\n",
            "Epoch 4/10, Loss: 0.07365076339244843\n",
            "Epoch 4/10, Loss: 0.07561171782016754\n",
            "Epoch 4/10, Loss: 0.07720893943309784\n",
            "Epoch 4/10, Loss: 0.0787700035572052\n",
            "Epoch 4/10, Loss: 0.08018897879123688\n",
            "Epoch 4/10, Loss: 0.08209323859214783\n",
            "Epoch 4/10, Loss: 0.0836414989233017\n",
            "Epoch 4/10, Loss: 0.08549228763580322\n",
            "Epoch 4/10, Loss: 0.08707820677757264\n",
            "Epoch 4/10, Loss: 0.08850788807868958\n",
            "Epoch 4/10, Loss: 0.0905642864704132\n",
            "Epoch 4/10, Loss: 0.09209498131275178\n",
            "Epoch 4/10, Loss: 0.09441962301731109\n",
            "Epoch 4/10, Loss: 0.09645267307758332\n",
            "Epoch 4/10, Loss: 0.0979808931350708\n",
            "Epoch 4/10, Loss: 0.09956337976455688\n",
            "Epoch 4/10, Loss: 0.10164269280433655\n",
            "Epoch 4/10, Loss: 0.10361226546764374\n",
            "Epoch 4/10, Loss: 0.10542492151260376\n",
            "Epoch 4/10, Loss: 0.10698168730735778\n",
            "Epoch 4/10, Loss: 0.10884395039081574\n",
            "Epoch 4/10, Loss: 0.11045277214050293\n",
            "Epoch 4/10, Loss: 0.11194595634937286\n",
            "Epoch 4/10, Loss: 0.1137228434085846\n",
            "Epoch 4/10, Loss: 0.11519966375827789\n",
            "Epoch 4/10, Loss: 0.1167209906578064\n",
            "Epoch 4/10, Loss: 0.11831874990463256\n",
            "Epoch 4/10, Loss: 0.11983535766601562\n",
            "Epoch 4/10, Loss: 0.12197597861289979\n",
            "Epoch 4/10, Loss: 0.12356471097469329\n",
            "Epoch 4/10, Loss: 0.1251727967262268\n",
            "Epoch 4/10, Loss: 0.12668747615814208\n",
            "Epoch 4/10, Loss: 0.12836026835441589\n",
            "Epoch 4/10, Loss: 0.130183412194252\n",
            "Epoch 4/10, Loss: 0.13193697249889375\n",
            "Epoch 4/10, Loss: 0.13379125666618347\n",
            "Epoch 4/10, Loss: 0.13564290416240693\n",
            "Epoch 4/10, Loss: 0.13738030064105988\n",
            "Epoch 4/10, Loss: 0.13924035036563873\n",
            "Epoch 4/10, Loss: 0.14088154244422912\n",
            "Epoch 4/10, Loss: 0.14270914828777312\n",
            "Epoch 4/10, Loss: 0.1447002238035202\n",
            "Epoch 4/10, Loss: 0.14615367221832276\n",
            "Epoch 4/10, Loss: 0.14795398819446565\n",
            "Epoch 4/10, Loss: 0.14984986782073975\n",
            "Epoch 4/10, Loss: 0.15143132770061493\n",
            "Epoch 4/10, Loss: 0.15341717553138734\n",
            "Epoch 4/10, Loss: 0.1550674455165863\n",
            "Epoch 4/10, Loss: 0.15679273521900178\n",
            "Epoch 4/10, Loss: 0.15882335937023162\n",
            "Epoch 4/10, Loss: 0.16065725576877593\n",
            "Epoch 4/10, Loss: 0.16240986347198486\n",
            "Epoch 4/10, Loss: 0.16401686608791352\n",
            "Epoch 4/10, Loss: 0.16597619724273682\n",
            "Epoch 4/10, Loss: 0.16731237637996674\n",
            "Epoch 4/10, Loss: 0.16909361588954924\n",
            "Epoch 4/10, Loss: 0.17068648386001586\n",
            "Epoch 4/10, Loss: 0.17207493615150452\n",
            "Epoch 4/10, Loss: 0.17372399497032165\n",
            "Epoch 4/10, Loss: 0.1753906636238098\n",
            "Epoch 4/10, Loss: 0.17698317074775696\n",
            "Epoch 4/10, Loss: 0.1787005993127823\n",
            "Epoch 4/10, Loss: 0.1803941433429718\n",
            "Epoch 4/10, Loss: 0.1820909812450409\n",
            "Epoch 4/10, Loss: 0.18373861753940582\n",
            "Epoch 4/10, Loss: 0.18549111688137054\n",
            "Epoch 4/10, Loss: 0.1872638314962387\n",
            "Epoch 4/10, Loss: 0.18909792137145995\n",
            "Epoch 4/10, Loss: 0.19138694834709166\n",
            "Epoch 4/10, Loss: 0.19299863076210022\n",
            "Epoch 4/10, Loss: 0.19469543039798737\n",
            "Epoch 4/10, Loss: 0.19648359334468843\n",
            "Epoch 4/10, Loss: 0.19796279871463776\n",
            "Epoch 4/10, Loss: 0.19985968065261842\n",
            "Epoch 4/10, Loss: 0.20159135365486144\n",
            "Epoch 4/10, Loss: 0.20331174743175506\n",
            "Epoch 4/10, Loss: 0.205015554189682\n",
            "Epoch 4/10, Loss: 0.20664205276966094\n",
            "Epoch 4/10, Loss: 0.2082734888792038\n",
            "Epoch 4/10, Loss: 0.20992587745189667\n",
            "Epoch 4/10, Loss: 0.2122623702287674\n",
            "Epoch 4/10, Loss: 0.2137034581899643\n",
            "Epoch 4/10, Loss: 0.2155271567106247\n",
            "Epoch 4/10, Loss: 0.21710092663764954\n",
            "Epoch 4/10, Loss: 0.21903428435325623\n",
            "Epoch 4/10, Loss: 0.2204829317331314\n",
            "Epoch 4/10, Loss: 0.22208209669589996\n",
            "Epoch 4/10, Loss: 0.22401781845092775\n",
            "Epoch 4/10, Loss: 0.22597096848487855\n",
            "Epoch 4/10, Loss: 0.22750602662563324\n",
            "Epoch 4/10, Loss: 0.22920412945747376\n",
            "Epoch 4/10, Loss: 0.2309787656068802\n",
            "Epoch 4/10, Loss: 0.2326817305088043\n",
            "Epoch 4/10, Loss: 0.23423983836174012\n",
            "Epoch 4/10, Loss: 0.23613902819156646\n",
            "Epoch 4/10, Loss: 0.2378381584882736\n",
            "Epoch 4/10, Loss: 0.23970530366897583\n",
            "Epoch 4/10, Loss: 0.24135477817058562\n",
            "Epoch 4/10, Loss: 0.24324605631828308\n",
            "Epoch 4/10, Loss: 0.2454612522125244\n",
            "Epoch 4/10, Loss: 0.2476821744441986\n",
            "Epoch 4/10, Loss: 0.24959669971466064\n",
            "Epoch 4/10, Loss: 0.2515573298931122\n",
            "Epoch 4/10, Loss: 0.2537313940525055\n",
            "Epoch 4/10, Loss: 0.2551192352771759\n",
            "Epoch 4/10, Loss: 0.2570767701864243\n",
            "Epoch 4/10, Loss: 0.2587043957710266\n",
            "Epoch 4/10, Loss: 0.2601049407720566\n",
            "Epoch 4/10, Loss: 0.26220493376255033\n",
            "Epoch 4/10, Loss: 0.2638901790380478\n",
            "Epoch 4/10, Loss: 0.26565695023536684\n",
            "Epoch 4/10, Loss: 0.2673336228132248\n",
            "Epoch 4/10, Loss: 0.2690522080659866\n",
            "Epoch 4/10, Loss: 0.27080961549282073\n",
            "Epoch 4/10, Loss: 0.2724052493572235\n",
            "Epoch 4/10, Loss: 0.2738103792667389\n",
            "Epoch 4/10, Loss: 0.2753909063339233\n",
            "Epoch 4/10, Loss: 0.27686131381988527\n",
            "Epoch 4/10, Loss: 0.2787631690502167\n",
            "Epoch 4/10, Loss: 0.28033476614952085\n",
            "Epoch 4/10, Loss: 0.28205742955207824\n",
            "Epoch 4/10, Loss: 0.28361906099319456\n",
            "Epoch 4/10, Loss: 0.28520113027095795\n",
            "Epoch 4/10, Loss: 0.28682394528388977\n",
            "Epoch 4/10, Loss: 0.28855608224868773\n",
            "Epoch 4/10, Loss: 0.29019439458847046\n",
            "Epoch 4/10, Loss: 0.29172582495212557\n",
            "Epoch 4/10, Loss: 0.2935187826156616\n",
            "Epoch 4/10, Loss: 0.29521069645881653\n",
            "Epoch 4/10, Loss: 0.2966218158006668\n",
            "Epoch 4/10, Loss: 0.298179678440094\n",
            "Epoch 4/10, Loss: 0.29955004835128785\n",
            "Epoch 4/10, Loss: 0.3016138565540314\n",
            "Epoch 4/10, Loss: 0.30354393148422243\n",
            "Epoch 4/10, Loss: 0.3055038069486618\n",
            "Epoch 4/10, Loss: 0.3071269021034241\n",
            "Epoch 4/10, Loss: 0.3086705236434937\n",
            "Epoch 4/10, Loss: 0.3106626261472702\n",
            "Epoch 4/10, Loss: 0.3128034456968308\n",
            "Epoch 4/10, Loss: 0.31417852246761324\n",
            "Epoch 4/10, Loss: 0.31593234980106355\n",
            "Epoch 4/10, Loss: 0.318497771859169\n",
            "Epoch 4/10, Loss: 0.3200121911764145\n",
            "Epoch 4/10, Loss: 0.32159094607830047\n",
            "Epoch 4/10, Loss: 0.3237277971506119\n",
            "Epoch 4/10, Loss: 0.325900332570076\n",
            "Epoch 4/10, Loss: 0.3276193137168884\n",
            "Epoch 4/10, Loss: 0.32991385316848754\n",
            "Epoch 4/10, Loss: 0.3322335636615753\n",
            "Epoch 4/10, Loss: 0.33387301981449125\n",
            "Epoch 4/10, Loss: 0.3356788841485977\n",
            "Epoch 4/10, Loss: 0.3372873227596283\n",
            "Epoch 4/10, Loss: 0.3389424175024033\n",
            "Epoch 4/10, Loss: 0.3408329492807388\n",
            "Epoch 4/10, Loss: 0.34291481363773346\n",
            "Epoch 4/10, Loss: 0.34430499613285065\n",
            "Epoch 4/10, Loss: 0.34570996356010436\n",
            "Epoch 4/10, Loss: 0.34735282135009765\n",
            "Epoch 4/10, Loss: 0.3487689150571823\n",
            "Epoch 4/10, Loss: 0.3506905382871628\n",
            "Epoch 4/10, Loss: 0.35259721195697785\n",
            "Epoch 4/10, Loss: 0.3545323907136917\n",
            "Epoch 4/10, Loss: 0.3559701279401779\n",
            "Epoch 4/10, Loss: 0.35760389697551725\n",
            "Epoch 4/10, Loss: 0.3591706923246384\n",
            "Epoch 4/10, Loss: 0.3611300232410431\n",
            "Epoch 4/10, Loss: 0.3629396488666534\n",
            "Epoch 4/10, Loss: 0.3648149290084839\n",
            "Epoch 4/10, Loss: 0.36686925983428953\n",
            "Epoch 4/10, Loss: 0.3684011093378067\n",
            "Epoch 4/10, Loss: 0.3700023888349533\n",
            "Epoch 4/10, Loss: 0.3717562938928604\n",
            "Epoch 4/10, Loss: 0.37321042561531065\n",
            "Epoch 4/10, Loss: 0.374750443816185\n",
            "Epoch 4/10, Loss: 0.37647147238254547\n",
            "Epoch 4/10, Loss: 0.3778854066133499\n",
            "Epoch 4/10, Loss: 0.3797594513893127\n",
            "Epoch 4/10, Loss: 0.3814056030511856\n",
            "Epoch 4/10, Loss: 0.3827424737215042\n",
            "Epoch 4/10, Loss: 0.3845311702489853\n",
            "Epoch 4/10, Loss: 0.38588853800296785\n",
            "Epoch 4/10, Loss: 0.38791864264011383\n",
            "Epoch 4/10, Loss: 0.3894733692407608\n",
            "Epoch 4/10, Loss: 0.3913147312402725\n",
            "Epoch 4/10, Loss: 0.39347427713871\n",
            "Epoch 4/10, Loss: 0.39515467143058775\n",
            "Epoch 4/10, Loss: 0.3967436076402664\n",
            "Epoch 4/10, Loss: 0.39818621683120725\n",
            "Epoch 4/10, Loss: 0.4001078703403473\n",
            "Epoch 4/10, Loss: 0.4020387172698975\n",
            "Epoch 4/10, Loss: 0.4038221558332443\n",
            "Epoch 4/10, Loss: 0.40525043189525606\n",
            "Epoch 4/10, Loss: 0.40706889212131503\n",
            "Epoch 4/10, Loss: 0.4086572357416153\n",
            "Epoch 4/10, Loss: 0.4101059948205948\n",
            "Epoch 4/10, Loss: 0.41216120636463166\n",
            "Epoch 4/10, Loss: 0.4136533944606781\n",
            "Epoch 4/10, Loss: 0.4154269909858704\n",
            "Epoch 4/10, Loss: 0.41746869969367983\n",
            "Epoch 4/10, Loss: 0.41925270307064055\n",
            "Epoch 4/10, Loss: 0.42067092406749723\n",
            "Epoch 4/10, Loss: 0.42230517864227296\n",
            "Epoch 4/10, Loss: 0.4242926518917084\n",
            "Epoch 4/10, Loss: 0.4256875239610672\n",
            "Epoch 4/10, Loss: 0.4272965525388718\n",
            "Epoch 4/10, Loss: 0.4286557375192642\n",
            "Epoch 4/10, Loss: 0.4301974071264267\n",
            "Epoch 4/10, Loss: 0.4318440992832184\n",
            "Epoch 5/10, Loss: 0.0015567300319671632\n",
            "Epoch 5/10, Loss: 0.0033286265134811403\n",
            "Epoch 5/10, Loss: 0.005127367973327637\n",
            "Epoch 5/10, Loss: 0.006761422872543335\n",
            "Epoch 5/10, Loss: 0.008352169156074524\n",
            "Epoch 5/10, Loss: 0.010154788851737977\n",
            "Epoch 5/10, Loss: 0.011500760912895203\n",
            "Epoch 5/10, Loss: 0.013379696726799012\n",
            "Epoch 5/10, Loss: 0.015026568174362183\n",
            "Epoch 5/10, Loss: 0.016597065329551696\n",
            "Epoch 5/10, Loss: 0.018390796780586242\n",
            "Epoch 5/10, Loss: 0.019937689423561097\n",
            "Epoch 5/10, Loss: 0.021484542846679687\n",
            "Epoch 5/10, Loss: 0.02328256869316101\n",
            "Epoch 5/10, Loss: 0.02481999492645264\n",
            "Epoch 5/10, Loss: 0.02708720326423645\n",
            "Epoch 5/10, Loss: 0.028605462074279785\n",
            "Epoch 5/10, Loss: 0.030257468342781067\n",
            "Epoch 5/10, Loss: 0.031618581175804135\n",
            "Epoch 5/10, Loss: 0.0333716048002243\n",
            "Epoch 5/10, Loss: 0.034884732246398924\n",
            "Epoch 5/10, Loss: 0.03697720742225647\n",
            "Epoch 5/10, Loss: 0.03892910778522492\n",
            "Epoch 5/10, Loss: 0.04046407413482666\n",
            "Epoch 5/10, Loss: 0.042471539974212644\n",
            "Epoch 5/10, Loss: 0.04385508871078491\n",
            "Epoch 5/10, Loss: 0.045648364901542664\n",
            "Epoch 5/10, Loss: 0.04702321267127991\n",
            "Epoch 5/10, Loss: 0.04862560820579529\n",
            "Epoch 5/10, Loss: 0.05000326323509216\n",
            "Epoch 5/10, Loss: 0.05140045046806335\n",
            "Epoch 5/10, Loss: 0.053288614749908446\n",
            "Epoch 5/10, Loss: 0.05488677442073822\n",
            "Epoch 5/10, Loss: 0.0564399493932724\n",
            "Epoch 5/10, Loss: 0.05822886168956756\n",
            "Epoch 5/10, Loss: 0.05965219485759735\n",
            "Epoch 5/10, Loss: 0.06129266548156738\n",
            "Epoch 5/10, Loss: 0.06325495183467865\n",
            "Epoch 5/10, Loss: 0.06493821656703949\n",
            "Epoch 5/10, Loss: 0.06636434161663056\n",
            "Epoch 5/10, Loss: 0.06811334824562072\n",
            "Epoch 5/10, Loss: 0.07011998295783997\n",
            "Epoch 5/10, Loss: 0.0720554575920105\n",
            "Epoch 5/10, Loss: 0.07400983202457428\n",
            "Epoch 5/10, Loss: 0.07557085013389588\n",
            "Epoch 5/10, Loss: 0.07712280666828156\n",
            "Epoch 5/10, Loss: 0.07847855973243713\n",
            "Epoch 5/10, Loss: 0.0803609642982483\n",
            "Epoch 5/10, Loss: 0.0818395458459854\n",
            "Epoch 5/10, Loss: 0.08361412179470062\n",
            "Epoch 5/10, Loss: 0.08516924476623536\n",
            "Epoch 5/10, Loss: 0.08653852438926697\n",
            "Epoch 5/10, Loss: 0.08855518817901611\n",
            "Epoch 5/10, Loss: 0.09003939270973206\n",
            "Epoch 5/10, Loss: 0.09239535737037659\n",
            "Epoch 5/10, Loss: 0.09442739534378052\n",
            "Epoch 5/10, Loss: 0.09588868677616119\n",
            "Epoch 5/10, Loss: 0.0974527804851532\n",
            "Epoch 5/10, Loss: 0.09952646708488465\n",
            "Epoch 5/10, Loss: 0.10148365497589111\n",
            "Epoch 5/10, Loss: 0.10326527452468873\n",
            "Epoch 5/10, Loss: 0.10480367231369019\n",
            "Epoch 5/10, Loss: 0.10664327359199524\n",
            "Epoch 5/10, Loss: 0.10822972667217255\n",
            "Epoch 5/10, Loss: 0.10971540641784668\n",
            "Epoch 5/10, Loss: 0.1114635647535324\n",
            "Epoch 5/10, Loss: 0.11287885189056396\n",
            "Epoch 5/10, Loss: 0.11433783030509949\n",
            "Epoch 5/10, Loss: 0.1158573784828186\n",
            "Epoch 5/10, Loss: 0.11732181143760681\n",
            "Epoch 5/10, Loss: 0.1194326469898224\n",
            "Epoch 5/10, Loss: 0.12095251584053039\n",
            "Epoch 5/10, Loss: 0.12250609099864959\n",
            "Epoch 5/10, Loss: 0.12398280954360962\n",
            "Epoch 5/10, Loss: 0.12562353873252868\n",
            "Epoch 5/10, Loss: 0.12744015729427338\n",
            "Epoch 5/10, Loss: 0.12914478421211242\n",
            "Epoch 5/10, Loss: 0.13097327315807342\n",
            "Epoch 5/10, Loss: 0.13278768932819365\n",
            "Epoch 5/10, Loss: 0.13452622294425964\n",
            "Epoch 5/10, Loss: 0.1362857437133789\n",
            "Epoch 5/10, Loss: 0.13788173043727875\n",
            "Epoch 5/10, Loss: 0.13968484544754028\n",
            "Epoch 5/10, Loss: 0.1416732647418976\n",
            "Epoch 5/10, Loss: 0.14307197892665863\n",
            "Epoch 5/10, Loss: 0.14481999707221985\n",
            "Epoch 5/10, Loss: 0.14666578280925752\n",
            "Epoch 5/10, Loss: 0.14819283163547517\n",
            "Epoch 5/10, Loss: 0.1501171761751175\n",
            "Epoch 5/10, Loss: 0.1517193785905838\n",
            "Epoch 5/10, Loss: 0.15340056538581848\n",
            "Epoch 5/10, Loss: 0.15541972208023072\n",
            "Epoch 5/10, Loss: 0.15721992003917695\n",
            "Epoch 5/10, Loss: 0.15890268063545226\n",
            "Epoch 5/10, Loss: 0.1604587389230728\n",
            "Epoch 5/10, Loss: 0.16239580309391022\n",
            "Epoch 5/10, Loss: 0.16370427536964416\n",
            "Epoch 5/10, Loss: 0.16543506872653962\n",
            "Epoch 5/10, Loss: 0.16696167767047881\n",
            "Epoch 5/10, Loss: 0.1683130248785019\n",
            "Epoch 5/10, Loss: 0.16992091763019562\n",
            "Epoch 5/10, Loss: 0.17155460166931152\n",
            "Epoch 5/10, Loss: 0.17312951385974884\n",
            "Epoch 5/10, Loss: 0.174812526345253\n",
            "Epoch 5/10, Loss: 0.17647387742996215\n",
            "Epoch 5/10, Loss: 0.17813098776340486\n",
            "Epoch 5/10, Loss: 0.17972183203697203\n",
            "Epoch 5/10, Loss: 0.18142850196361543\n",
            "Epoch 5/10, Loss: 0.18315202808380127\n",
            "Epoch 5/10, Loss: 0.1849657369852066\n",
            "Epoch 5/10, Loss: 0.1872658554315567\n",
            "Epoch 5/10, Loss: 0.18884000706672668\n",
            "Epoch 5/10, Loss: 0.1905225570201874\n",
            "Epoch 5/10, Loss: 0.1922713441848755\n",
            "Epoch 5/10, Loss: 0.19368408966064454\n",
            "Epoch 5/10, Loss: 0.19552715921401978\n",
            "Epoch 5/10, Loss: 0.19722740924358367\n",
            "Epoch 5/10, Loss: 0.19894583523273468\n",
            "Epoch 5/10, Loss: 0.20062920248508453\n",
            "Epoch 5/10, Loss: 0.20223207175731658\n",
            "Epoch 5/10, Loss: 0.20381868362426758\n",
            "Epoch 5/10, Loss: 0.2054236445426941\n",
            "Epoch 5/10, Loss: 0.20774023747444154\n",
            "Epoch 5/10, Loss: 0.20914444613456726\n",
            "Epoch 5/10, Loss: 0.2109217357635498\n",
            "Epoch 5/10, Loss: 0.21247008895874023\n",
            "Epoch 5/10, Loss: 0.21437231838703155\n",
            "Epoch 5/10, Loss: 0.21577654588222503\n",
            "Epoch 5/10, Loss: 0.21734250175952913\n",
            "Epoch 5/10, Loss: 0.21929629981517793\n",
            "Epoch 5/10, Loss: 0.22122346591949463\n",
            "Epoch 5/10, Loss: 0.2227376129627228\n",
            "Epoch 5/10, Loss: 0.22442154622077942\n",
            "Epoch 5/10, Loss: 0.22618385589122772\n",
            "Epoch 5/10, Loss: 0.22783974397182466\n",
            "Epoch 5/10, Loss: 0.22935298907756804\n",
            "Epoch 5/10, Loss: 0.2312076178789139\n",
            "Epoch 5/10, Loss: 0.23289043509960175\n",
            "Epoch 5/10, Loss: 0.2347352751493454\n",
            "Epoch 5/10, Loss: 0.2363765103816986\n",
            "Epoch 5/10, Loss: 0.2382685797214508\n",
            "Epoch 5/10, Loss: 0.24049011659622194\n",
            "Epoch 5/10, Loss: 0.2427144765853882\n",
            "Epoch 5/10, Loss: 0.24461303591728212\n",
            "Epoch 5/10, Loss: 0.2465671854019165\n",
            "Epoch 5/10, Loss: 0.24871202087402344\n",
            "Epoch 5/10, Loss: 0.2500581986904144\n",
            "Epoch 5/10, Loss: 0.25202790772914885\n",
            "Epoch 5/10, Loss: 0.2536151478290558\n",
            "Epoch 5/10, Loss: 0.2549791065454483\n",
            "Epoch 5/10, Loss: 0.25709064972400664\n",
            "Epoch 5/10, Loss: 0.2587448408603668\n",
            "Epoch 5/10, Loss: 0.260490442276001\n",
            "Epoch 5/10, Loss: 0.26219105362892153\n",
            "Epoch 5/10, Loss: 0.2638538067340851\n",
            "Epoch 5/10, Loss: 0.26556239783763885\n",
            "Epoch 5/10, Loss: 0.26712994086742403\n",
            "Epoch 5/10, Loss: 0.2684972014427185\n",
            "Epoch 5/10, Loss: 0.2700373225212097\n",
            "Epoch 5/10, Loss: 0.2714620559215546\n",
            "Epoch 5/10, Loss: 0.2733415755033493\n",
            "Epoch 5/10, Loss: 0.27488931143283846\n",
            "Epoch 5/10, Loss: 0.2766125649213791\n",
            "Epoch 5/10, Loss: 0.2781244715452194\n",
            "Epoch 5/10, Loss: 0.2796817961931229\n",
            "Epoch 5/10, Loss: 0.2812949632406235\n",
            "Epoch 5/10, Loss: 0.28301172840595246\n",
            "Epoch 5/10, Loss: 0.2846403156518936\n",
            "Epoch 5/10, Loss: 0.28615852689743043\n",
            "Epoch 5/10, Loss: 0.28793803668022155\n",
            "Epoch 5/10, Loss: 0.2896061161756516\n",
            "Epoch 5/10, Loss: 0.2909729609489441\n",
            "Epoch 5/10, Loss: 0.2925140167474747\n",
            "Epoch 5/10, Loss: 0.2938439860343933\n",
            "Epoch 5/10, Loss: 0.2958737106323242\n",
            "Epoch 5/10, Loss: 0.2977865264415741\n",
            "Epoch 5/10, Loss: 0.2997488839626312\n",
            "Epoch 5/10, Loss: 0.3012872120141983\n",
            "Epoch 5/10, Loss: 0.30279201436042785\n",
            "Epoch 5/10, Loss: 0.30476849591732025\n",
            "Epoch 5/10, Loss: 0.3069149457216263\n",
            "Epoch 5/10, Loss: 0.30824982380867005\n",
            "Epoch 5/10, Loss: 0.31000302505493166\n",
            "Epoch 5/10, Loss: 0.31261392498016355\n",
            "Epoch 5/10, Loss: 0.3140989294052124\n",
            "Epoch 5/10, Loss: 0.31565196239948273\n",
            "Epoch 5/10, Loss: 0.3177590228319168\n",
            "Epoch 5/10, Loss: 0.31992708694934846\n",
            "Epoch 5/10, Loss: 0.3215646289587021\n",
            "Epoch 5/10, Loss: 0.3238325411081314\n",
            "Epoch 5/10, Loss: 0.3261614488363266\n",
            "Epoch 5/10, Loss: 0.32774721789360045\n",
            "Epoch 5/10, Loss: 0.32955827927589415\n",
            "Epoch 5/10, Loss: 0.3311509621143341\n",
            "Epoch 5/10, Loss: 0.33277228772640227\n",
            "Epoch 5/10, Loss: 0.33464253199100497\n",
            "Epoch 5/10, Loss: 0.3367100991010666\n",
            "Epoch 5/10, Loss: 0.33805642199516295\n",
            "Epoch 5/10, Loss: 0.3394471226930618\n",
            "Epoch 5/10, Loss: 0.3410775349140167\n",
            "Epoch 5/10, Loss: 0.3424497961997986\n",
            "Epoch 5/10, Loss: 0.34436743426322936\n",
            "Epoch 5/10, Loss: 0.34623997402191165\n",
            "Epoch 5/10, Loss: 0.34813343250751494\n",
            "Epoch 5/10, Loss: 0.3495289412736893\n",
            "Epoch 5/10, Loss: 0.35115452110767365\n",
            "Epoch 5/10, Loss: 0.3526981227397919\n",
            "Epoch 5/10, Loss: 0.35461490631103515\n",
            "Epoch 5/10, Loss: 0.35640492141246793\n",
            "Epoch 5/10, Loss: 0.35826541018486024\n",
            "Epoch 5/10, Loss: 0.3603032541275024\n",
            "Epoch 5/10, Loss: 0.36176444876194\n",
            "Epoch 5/10, Loss: 0.3633716779947281\n",
            "Epoch 5/10, Loss: 0.3651174871921539\n",
            "Epoch 5/10, Loss: 0.3665455540418625\n",
            "Epoch 5/10, Loss: 0.3680679444074631\n",
            "Epoch 5/10, Loss: 0.3697576551437378\n",
            "Epoch 5/10, Loss: 0.3711454385519028\n",
            "Epoch 5/10, Loss: 0.3729880372285843\n",
            "Epoch 5/10, Loss: 0.3746217212677002\n",
            "Epoch 5/10, Loss: 0.37592926108837127\n",
            "Epoch 5/10, Loss: 0.3777011023759842\n",
            "Epoch 5/10, Loss: 0.3790235241651535\n",
            "Epoch 5/10, Loss: 0.3810406988859177\n",
            "Epoch 5/10, Loss: 0.3825682682991028\n",
            "Epoch 5/10, Loss: 0.3843780071735382\n",
            "Epoch 5/10, Loss: 0.38653846669197084\n",
            "Epoch 5/10, Loss: 0.38818127834796906\n",
            "Epoch 5/10, Loss: 0.38975366497039793\n",
            "Epoch 5/10, Loss: 0.3911529215574264\n",
            "Epoch 5/10, Loss: 0.3930587660074234\n",
            "Epoch 5/10, Loss: 0.3949885610342026\n",
            "Epoch 5/10, Loss: 0.3967377907037735\n",
            "Epoch 5/10, Loss: 0.39812232863903046\n",
            "Epoch 5/10, Loss: 0.3999215338230133\n",
            "Epoch 5/10, Loss: 0.40148363447189334\n",
            "Epoch 5/10, Loss: 0.4028745129108429\n",
            "Epoch 5/10, Loss: 0.4049257915019989\n",
            "Epoch 5/10, Loss: 0.4063721332550049\n",
            "Epoch 5/10, Loss: 0.4081428723335266\n",
            "Epoch 5/10, Loss: 0.4101670517921448\n",
            "Epoch 5/10, Loss: 0.41194973361492154\n",
            "Epoch 5/10, Loss: 0.41331891644001006\n",
            "Epoch 5/10, Loss: 0.4149318132400513\n",
            "Epoch 5/10, Loss: 0.4169199516773224\n",
            "Epoch 5/10, Loss: 0.418276074051857\n",
            "Epoch 5/10, Loss: 0.41987924873828886\n",
            "Epoch 5/10, Loss: 0.42120895075798037\n",
            "Epoch 5/10, Loss: 0.4227426246404648\n",
            "Epoch 5/10, Loss: 0.4244259457588196\n",
            "Epoch 6/10, Loss: 0.0015394991636276244\n",
            "Epoch 6/10, Loss: 0.0033008534908294676\n",
            "Epoch 6/10, Loss: 0.0050991463661193845\n",
            "Epoch 6/10, Loss: 0.006711856722831726\n",
            "Epoch 6/10, Loss: 0.008280853390693664\n",
            "Epoch 6/10, Loss: 0.010069038391113281\n",
            "Epoch 6/10, Loss: 0.011385581254959106\n",
            "Epoch 6/10, Loss: 0.013245139837265014\n",
            "Epoch 6/10, Loss: 0.014860427379608155\n",
            "Epoch 6/10, Loss: 0.016406725645065307\n",
            "Epoch 6/10, Loss: 0.01818292021751404\n",
            "Epoch 6/10, Loss: 0.019715038657188415\n",
            "Epoch 6/10, Loss: 0.021251801490783692\n",
            "Epoch 6/10, Loss: 0.023037579774856567\n",
            "Epoch 6/10, Loss: 0.02452017104625702\n",
            "Epoch 6/10, Loss: 0.02678717339038849\n",
            "Epoch 6/10, Loss: 0.028280096292495727\n",
            "Epoch 6/10, Loss: 0.029883838891983032\n",
            "Epoch 6/10, Loss: 0.031218135714530946\n",
            "Epoch 6/10, Loss: 0.0329275484085083\n",
            "Epoch 6/10, Loss: 0.03442579913139343\n",
            "Epoch 6/10, Loss: 0.03650482439994812\n",
            "Epoch 6/10, Loss: 0.038448257088661196\n",
            "Epoch 6/10, Loss: 0.03996500635147095\n",
            "Epoch 6/10, Loss: 0.0419728262424469\n",
            "Epoch 6/10, Loss: 0.04331866836547851\n",
            "Epoch 6/10, Loss: 0.04509243142604828\n",
            "Epoch 6/10, Loss: 0.04644727385044098\n",
            "Epoch 6/10, Loss: 0.04802118504047394\n",
            "Epoch 6/10, Loss: 0.049358290791511536\n",
            "Epoch 6/10, Loss: 0.05072286117076874\n",
            "Epoch 6/10, Loss: 0.05259805655479431\n",
            "Epoch 6/10, Loss: 0.05417789030075073\n",
            "Epoch 6/10, Loss: 0.05568596136569977\n",
            "Epoch 6/10, Loss: 0.05745819509029389\n",
            "Epoch 6/10, Loss: 0.05884467899799347\n",
            "Epoch 6/10, Loss: 0.06046974575519562\n",
            "Epoch 6/10, Loss: 0.062391635894775394\n",
            "Epoch 6/10, Loss: 0.06404569911956787\n",
            "Epoch 6/10, Loss: 0.06548181176185608\n",
            "Epoch 6/10, Loss: 0.06722315144538879\n",
            "Epoch 6/10, Loss: 0.06922425580024719\n",
            "Epoch 6/10, Loss: 0.07115610289573669\n",
            "Epoch 6/10, Loss: 0.07311441469192505\n",
            "Epoch 6/10, Loss: 0.0746571581363678\n",
            "Epoch 6/10, Loss: 0.07620614123344421\n",
            "Epoch 6/10, Loss: 0.07752506148815155\n",
            "Epoch 6/10, Loss: 0.07939827609062194\n",
            "Epoch 6/10, Loss: 0.0808236781358719\n",
            "Epoch 6/10, Loss: 0.08255358242988586\n",
            "Epoch 6/10, Loss: 0.08409318375587463\n",
            "Epoch 6/10, Loss: 0.08542579805850983\n",
            "Epoch 6/10, Loss: 0.08741994762420655\n",
            "Epoch 6/10, Loss: 0.0888886524438858\n",
            "Epoch 6/10, Loss: 0.0912754203081131\n",
            "Epoch 6/10, Loss: 0.09330670249462128\n",
            "Epoch 6/10, Loss: 0.09472466039657593\n",
            "Epoch 6/10, Loss: 0.09627696442604065\n",
            "Epoch 6/10, Loss: 0.09834032940864564\n",
            "Epoch 6/10, Loss: 0.10028568160533904\n",
            "Epoch 6/10, Loss: 0.10205187094211578\n",
            "Epoch 6/10, Loss: 0.10358045470714569\n",
            "Epoch 6/10, Loss: 0.10540775072574615\n",
            "Epoch 6/10, Loss: 0.10698038983345032\n",
            "Epoch 6/10, Loss: 0.10846812331676484\n",
            "Epoch 6/10, Loss: 0.11020710682868957\n",
            "Epoch 6/10, Loss: 0.11157866799831391\n",
            "Epoch 6/10, Loss: 0.11299540972709655\n",
            "Epoch 6/10, Loss: 0.11446216750144958\n",
            "Epoch 6/10, Loss: 0.11590034961700439\n",
            "Epoch 6/10, Loss: 0.1179858500957489\n",
            "Epoch 6/10, Loss: 0.11945632719993592\n",
            "Epoch 6/10, Loss: 0.12097968649864196\n",
            "Epoch 6/10, Loss: 0.12244099700450897\n",
            "Epoch 6/10, Loss: 0.12406697845458985\n",
            "Epoch 6/10, Loss: 0.12587659645080568\n",
            "Epoch 6/10, Loss: 0.12755113792419434\n",
            "Epoch 6/10, Loss: 0.12936170673370362\n",
            "Epoch 6/10, Loss: 0.13114134526252746\n",
            "Epoch 6/10, Loss: 0.1328910356760025\n",
            "Epoch 6/10, Loss: 0.13458488547801972\n",
            "Epoch 6/10, Loss: 0.1361533304452896\n",
            "Epoch 6/10, Loss: 0.13795735037326812\n",
            "Epoch 6/10, Loss: 0.13994855213165283\n",
            "Epoch 6/10, Loss: 0.141314435839653\n",
            "Epoch 6/10, Loss: 0.1430273745059967\n",
            "Epoch 6/10, Loss: 0.14482976734638214\n",
            "Epoch 6/10, Loss: 0.14632569646835328\n",
            "Epoch 6/10, Loss: 0.14820035457611083\n",
            "Epoch 6/10, Loss: 0.14978244984149933\n",
            "Epoch 6/10, Loss: 0.15143099105358124\n",
            "Epoch 6/10, Loss: 0.1534449247121811\n",
            "Epoch 6/10, Loss: 0.15521842765808105\n",
            "Epoch 6/10, Loss: 0.1568580013513565\n",
            "Epoch 6/10, Loss: 0.15836581313610076\n",
            "Epoch 6/10, Loss: 0.16028433489799498\n",
            "Epoch 6/10, Loss: 0.16157771360874176\n",
            "Epoch 6/10, Loss: 0.16328586792945862\n",
            "Epoch 6/10, Loss: 0.16475773274898528\n",
            "Epoch 6/10, Loss: 0.1660879807472229\n",
            "Epoch 6/10, Loss: 0.16767301726341247\n",
            "Epoch 6/10, Loss: 0.1692872712612152\n",
            "Epoch 6/10, Loss: 0.17085956645011902\n",
            "Epoch 6/10, Loss: 0.17252458143234253\n",
            "Epoch 6/10, Loss: 0.17416434276103973\n",
            "Epoch 6/10, Loss: 0.17580085122585296\n",
            "Epoch 6/10, Loss: 0.17735109412670136\n",
            "Epoch 6/10, Loss: 0.17903346741199494\n",
            "Epoch 6/10, Loss: 0.1807265249490738\n",
            "Epoch 6/10, Loss: 0.18253564155101776\n",
            "Epoch 6/10, Loss: 0.1848410404920578\n",
            "Epoch 6/10, Loss: 0.18639603757858275\n",
            "Epoch 6/10, Loss: 0.18807365560531616\n",
            "Epoch 6/10, Loss: 0.18978443121910096\n",
            "Epoch 6/10, Loss: 0.19116030156612396\n",
            "Epoch 6/10, Loss: 0.19296033358573914\n",
            "Epoch 6/10, Loss: 0.19463796746730805\n",
            "Epoch 6/10, Loss: 0.19636234652996062\n",
            "Epoch 6/10, Loss: 0.19802528035640715\n",
            "Epoch 6/10, Loss: 0.19961174595355988\n",
            "Epoch 6/10, Loss: 0.201174720287323\n",
            "Epoch 6/10, Loss: 0.20275535452365875\n",
            "Epoch 6/10, Loss: 0.20506402003765106\n",
            "Epoch 6/10, Loss: 0.2064478976726532\n",
            "Epoch 6/10, Loss: 0.20819761383533478\n",
            "Epoch 6/10, Loss: 0.209732541680336\n",
            "Epoch 6/10, Loss: 0.21161112999916076\n",
            "Epoch 6/10, Loss: 0.2129919399023056\n",
            "Epoch 6/10, Loss: 0.21452884232997893\n",
            "Epoch 6/10, Loss: 0.21649202120304106\n",
            "Epoch 6/10, Loss: 0.21840284931659698\n",
            "Epoch 6/10, Loss: 0.21990546345710754\n",
            "Epoch 6/10, Loss: 0.22158173823356628\n",
            "Epoch 6/10, Loss: 0.22333752393722534\n",
            "Epoch 6/10, Loss: 0.22496281480789185\n",
            "Epoch 6/10, Loss: 0.22644442057609557\n",
            "Epoch 6/10, Loss: 0.22827015030384062\n",
            "Epoch 6/10, Loss: 0.22994803369045258\n",
            "Epoch 6/10, Loss: 0.23178068065643312\n",
            "Epoch 6/10, Loss: 0.23342212402820586\n",
            "Epoch 6/10, Loss: 0.23530494701862334\n",
            "Epoch 6/10, Loss: 0.23753240525722505\n",
            "Epoch 6/10, Loss: 0.23976036870479583\n",
            "Epoch 6/10, Loss: 0.24164504182338714\n",
            "Epoch 6/10, Loss: 0.24359342074394227\n",
            "Epoch 6/10, Loss: 0.24571427202224733\n",
            "Epoch 6/10, Loss: 0.24703405952453614\n",
            "Epoch 6/10, Loss: 0.24901271784305573\n",
            "Epoch 6/10, Loss: 0.2505758125782013\n",
            "Epoch 6/10, Loss: 0.2519152277708054\n",
            "Epoch 6/10, Loss: 0.25403536069393157\n",
            "Epoch 6/10, Loss: 0.2556712229251862\n",
            "Epoch 6/10, Loss: 0.2574007650613785\n",
            "Epoch 6/10, Loss: 0.25911446607112887\n",
            "Epoch 6/10, Loss: 0.26074127662181856\n",
            "Epoch 6/10, Loss: 0.26241679775714877\n",
            "Epoch 6/10, Loss: 0.263969269990921\n",
            "Epoch 6/10, Loss: 0.26532002294063567\n",
            "Epoch 6/10, Loss: 0.2668368264436722\n",
            "Epoch 6/10, Loss: 0.26822952020168306\n",
            "Epoch 6/10, Loss: 0.270093044757843\n",
            "Epoch 6/10, Loss: 0.2716282784938812\n",
            "Epoch 6/10, Loss: 0.2733497997522354\n",
            "Epoch 6/10, Loss: 0.2748177160024643\n",
            "Epoch 6/10, Loss: 0.2763607484102249\n",
            "Epoch 6/10, Loss: 0.27796091306209564\n",
            "Epoch 6/10, Loss: 0.2796718535423279\n",
            "Epoch 6/10, Loss: 0.2812937262058258\n",
            "Epoch 6/10, Loss: 0.28281288611888883\n",
            "Epoch 6/10, Loss: 0.2845842528343201\n",
            "Epoch 6/10, Loss: 0.2862436611652374\n",
            "Epoch 6/10, Loss: 0.2875812237262726\n",
            "Epoch 6/10, Loss: 0.2891134433746338\n",
            "Epoch 6/10, Loss: 0.2904212990999222\n",
            "Epoch 6/10, Loss: 0.2924216650724411\n",
            "Epoch 6/10, Loss: 0.29432148969173433\n",
            "Epoch 6/10, Loss: 0.2962863177061081\n",
            "Epoch 6/10, Loss: 0.29774843668937684\n",
            "Epoch 6/10, Loss: 0.2992208231687546\n",
            "Epoch 6/10, Loss: 0.3011788455247879\n",
            "Epoch 6/10, Loss: 0.3033306671380997\n",
            "Epoch 6/10, Loss: 0.3046428246498108\n",
            "Epoch 6/10, Loss: 0.3063925793170929\n",
            "Epoch 6/10, Loss: 0.30903736090660094\n",
            "Epoch 6/10, Loss: 0.3105079349279404\n",
            "Epoch 6/10, Loss: 0.3120472824573517\n",
            "Epoch 6/10, Loss: 0.3141200184822083\n",
            "Epoch 6/10, Loss: 0.31628129744529726\n",
            "Epoch 6/10, Loss: 0.3178462413549423\n",
            "Epoch 6/10, Loss: 0.3200882304906845\n",
            "Epoch 6/10, Loss: 0.32241668236255644\n",
            "Epoch 6/10, Loss: 0.3239643305540085\n",
            "Epoch 6/10, Loss: 0.32579451394081116\n",
            "Epoch 6/10, Loss: 0.32738206577301027\n",
            "Epoch 6/10, Loss: 0.32898747646808624\n",
            "Epoch 6/10, Loss: 0.33084929060935975\n",
            "Epoch 6/10, Loss: 0.3329078695774078\n",
            "Epoch 6/10, Loss: 0.3342284644842148\n",
            "Epoch 6/10, Loss: 0.33561606788635256\n",
            "Epoch 6/10, Loss: 0.33723528277873993\n",
            "Epoch 6/10, Loss: 0.3385779106616974\n",
            "Epoch 6/10, Loss: 0.340493354678154\n",
            "Epoch 6/10, Loss: 0.34233654963970184\n",
            "Epoch 6/10, Loss: 0.344197646856308\n",
            "Epoch 6/10, Loss: 0.34557772624492644\n",
            "Epoch 6/10, Loss: 0.3471970477104187\n",
            "Epoch 6/10, Loss: 0.34872844433784483\n",
            "Epoch 6/10, Loss: 0.35061272859573367\n",
            "Epoch 6/10, Loss: 0.35239301228523257\n",
            "Epoch 6/10, Loss: 0.3542396730184555\n",
            "Epoch 6/10, Loss: 0.3562675117254257\n",
            "Epoch 6/10, Loss: 0.3576910753250122\n",
            "Epoch 6/10, Loss: 0.3593099504709244\n",
            "Epoch 6/10, Loss: 0.36105102455615995\n",
            "Epoch 6/10, Loss: 0.36245440983772276\n",
            "Epoch 6/10, Loss: 0.36396488654613496\n",
            "Epoch 6/10, Loss: 0.36563045597076416\n",
            "Epoch 6/10, Loss: 0.3670048043727875\n",
            "Epoch 6/10, Loss: 0.36882718348503113\n",
            "Epoch 6/10, Loss: 0.370452791929245\n",
            "Epoch 6/10, Loss: 0.37174450027942657\n",
            "Epoch 6/10, Loss: 0.3735011554956436\n",
            "Epoch 6/10, Loss: 0.3748035782575607\n",
            "Epoch 6/10, Loss: 0.37680020308494566\n",
            "Epoch 6/10, Loss: 0.3783101004362106\n",
            "Epoch 6/10, Loss: 0.38010463273525236\n",
            "Epoch 6/10, Loss: 0.38227481544017794\n",
            "Epoch 6/10, Loss: 0.3838949850797653\n",
            "Epoch 6/10, Loss: 0.38545774400234223\n",
            "Epoch 6/10, Loss: 0.3868295986652374\n",
            "Epoch 6/10, Loss: 0.38871541583538055\n",
            "Epoch 6/10, Loss: 0.39064233434200285\n",
            "Epoch 6/10, Loss: 0.3923719848394394\n",
            "Epoch 6/10, Loss: 0.39372758090496063\n",
            "Epoch 6/10, Loss: 0.39551536321640013\n",
            "Epoch 6/10, Loss: 0.3970624405145645\n",
            "Epoch 6/10, Loss: 0.3984130744934082\n",
            "Epoch 6/10, Loss: 0.4004656596183777\n",
            "Epoch 6/10, Loss: 0.40188297247886656\n",
            "Epoch 6/10, Loss: 0.4036462436914444\n",
            "Epoch 6/10, Loss: 0.40566062223911287\n",
            "Epoch 6/10, Loss: 0.40744958341121673\n",
            "Epoch 6/10, Loss: 0.4087829627990723\n",
            "Epoch 6/10, Loss: 0.4103842420578003\n",
            "Epoch 6/10, Loss: 0.4123740050792694\n",
            "Epoch 6/10, Loss: 0.41370252704620364\n",
            "Epoch 6/10, Loss: 0.4153044221401215\n",
            "Epoch 6/10, Loss: 0.41661682856082916\n",
            "Epoch 6/10, Loss: 0.41815272796154024\n",
            "Epoch 6/10, Loss: 0.4198603365421295\n",
            "Epoch 7/10, Loss: 0.0015302823781967163\n",
            "Epoch 7/10, Loss: 0.0032882447242736815\n",
            "Epoch 7/10, Loss: 0.005084950566291809\n",
            "Epoch 7/10, Loss: 0.006684867620468139\n",
            "Epoch 7/10, Loss: 0.008241611719131469\n",
            "Epoch 7/10, Loss: 0.01001806938648224\n",
            "Epoch 7/10, Loss: 0.011318031191825867\n",
            "Epoch 7/10, Loss: 0.013163247466087342\n",
            "Epoch 7/10, Loss: 0.014757237553596496\n",
            "Epoch 7/10, Loss: 0.01628984546661377\n",
            "Epoch 7/10, Loss: 0.018057291030883788\n",
            "Epoch 7/10, Loss: 0.01958099341392517\n",
            "Epoch 7/10, Loss: 0.02111156761646271\n",
            "Epoch 7/10, Loss: 0.02288968229293823\n",
            "Epoch 7/10, Loss: 0.02432736825942993\n",
            "Epoch 7/10, Loss: 0.0265917432308197\n",
            "Epoch 7/10, Loss: 0.028066606879234315\n",
            "Epoch 7/10, Loss: 0.02963486099243164\n",
            "Epoch 7/10, Loss: 0.030954665064811707\n",
            "Epoch 7/10, Loss: 0.03262947106361389\n",
            "Epoch 7/10, Loss: 0.03412142372131348\n",
            "Epoch 7/10, Loss: 0.036193576574325564\n",
            "Epoch 7/10, Loss: 0.03813473522663116\n",
            "Epoch 7/10, Loss: 0.039639023423194884\n",
            "Epoch 7/10, Loss: 0.04165338575839996\n",
            "Epoch 7/10, Loss: 0.042979132294654845\n",
            "Epoch 7/10, Loss: 0.04473749113082886\n",
            "Epoch 7/10, Loss: 0.04608197808265686\n",
            "Epoch 7/10, Loss: 0.04763947987556458\n",
            "Epoch 7/10, Loss: 0.048954833269119265\n",
            "Epoch 7/10, Loss: 0.05029663157463074\n",
            "Epoch 7/10, Loss: 0.05216832184791565\n",
            "Epoch 7/10, Loss: 0.05373974597454071\n",
            "Epoch 7/10, Loss: 0.05521474206447601\n",
            "Epoch 7/10, Loss: 0.05698223078250885\n",
            "Epoch 7/10, Loss: 0.0583596419095993\n",
            "Epoch 7/10, Loss: 0.0599726927280426\n",
            "Epoch 7/10, Loss: 0.06186438763141632\n",
            "Epoch 7/10, Loss: 0.06349797642230988\n",
            "Epoch 7/10, Loss: 0.06494369196891785\n",
            "Epoch 7/10, Loss: 0.06667606687545777\n",
            "Epoch 7/10, Loss: 0.06867436480522156\n",
            "Epoch 7/10, Loss: 0.07059802341461181\n",
            "Epoch 7/10, Loss: 0.07255500543117523\n",
            "Epoch 7/10, Loss: 0.07408732998371124\n",
            "Epoch 7/10, Loss: 0.07563536536693573\n",
            "Epoch 7/10, Loss: 0.07693328273296356\n",
            "Epoch 7/10, Loss: 0.07880331075191498\n",
            "Epoch 7/10, Loss: 0.08018853640556335\n",
            "Epoch 7/10, Loss: 0.0818922119140625\n",
            "Epoch 7/10, Loss: 0.08342191457748413\n",
            "Epoch 7/10, Loss: 0.08473138952255249\n",
            "Epoch 7/10, Loss: 0.08671424615383148\n",
            "Epoch 7/10, Loss: 0.08817874276638031\n",
            "Epoch 7/10, Loss: 0.09059397876262665\n",
            "Epoch 7/10, Loss: 0.09262318098545075\n",
            "Epoch 7/10, Loss: 0.09400472927093506\n",
            "Epoch 7/10, Loss: 0.09554745209217072\n",
            "Epoch 7/10, Loss: 0.09759709441661835\n",
            "Epoch 7/10, Loss: 0.09953188586235047\n",
            "Epoch 7/10, Loss: 0.10128358852863312\n",
            "Epoch 7/10, Loss: 0.10280625975131988\n",
            "Epoch 7/10, Loss: 0.10462589728832244\n",
            "Epoch 7/10, Loss: 0.1061852525472641\n",
            "Epoch 7/10, Loss: 0.10767804515361785\n",
            "Epoch 7/10, Loss: 0.10941282916069031\n",
            "Epoch 7/10, Loss: 0.11075105690956116\n",
            "Epoch 7/10, Loss: 0.11213297224044799\n",
            "Epoch 7/10, Loss: 0.11356142163276672\n",
            "Epoch 7/10, Loss: 0.11498701250553131\n",
            "Epoch 7/10, Loss: 0.11705594527721405\n",
            "Epoch 7/10, Loss: 0.11848601412773133\n",
            "Epoch 7/10, Loss: 0.11998835670948028\n",
            "Epoch 7/10, Loss: 0.12144320976734162\n",
            "Epoch 7/10, Loss: 0.12306251990795135\n",
            "Epoch 7/10, Loss: 0.12486456382274627\n",
            "Epoch 7/10, Loss: 0.12651885890960693\n",
            "Epoch 7/10, Loss: 0.12831604552268983\n",
            "Epoch 7/10, Loss: 0.13006667566299437\n",
            "Epoch 7/10, Loss: 0.1318281168937683\n",
            "Epoch 7/10, Loss: 0.13347368240356444\n",
            "Epoch 7/10, Loss: 0.13502502572536468\n",
            "Epoch 7/10, Loss: 0.13684677660465241\n",
            "Epoch 7/10, Loss: 0.13884165704250337\n",
            "Epoch 7/10, Loss: 0.14018987107276917\n",
            "Epoch 7/10, Loss: 0.1418792906999588\n",
            "Epoch 7/10, Loss: 0.14364668083190918\n",
            "Epoch 7/10, Loss: 0.1451227091550827\n",
            "Epoch 7/10, Loss: 0.146966032743454\n",
            "Epoch 7/10, Loss: 0.14854392349720003\n",
            "Epoch 7/10, Loss: 0.15016076636314393\n",
            "Epoch 7/10, Loss: 0.1521719090938568\n",
            "Epoch 7/10, Loss: 0.15392398595809936\n",
            "Epoch 7/10, Loss: 0.1555352348089218\n",
            "Epoch 7/10, Loss: 0.15699854171276093\n",
            "Epoch 7/10, Loss: 0.15889628446102141\n",
            "Epoch 7/10, Loss: 0.16018037247657776\n",
            "Epoch 7/10, Loss: 0.1618864197731018\n",
            "Epoch 7/10, Loss: 0.16331720161437988\n",
            "Epoch 7/10, Loss: 0.16463379085063934\n",
            "Epoch 7/10, Loss: 0.16620447170734406\n",
            "Epoch 7/10, Loss: 0.16780722188949584\n",
            "Epoch 7/10, Loss: 0.1693852381706238\n",
            "Epoch 7/10, Loss: 0.17103999185562133\n",
            "Epoch 7/10, Loss: 0.17266441619396208\n",
            "Epoch 7/10, Loss: 0.17427914392948152\n",
            "Epoch 7/10, Loss: 0.17580033230781555\n",
            "Epoch 7/10, Loss: 0.1774669734239578\n",
            "Epoch 7/10, Loss: 0.17914044070243834\n",
            "Epoch 7/10, Loss: 0.18094955468177795\n",
            "Epoch 7/10, Loss: 0.18325821805000306\n",
            "Epoch 7/10, Loss: 0.184799978017807\n",
            "Epoch 7/10, Loss: 0.18647644805908203\n",
            "Epoch 7/10, Loss: 0.18815697622299193\n",
            "Epoch 7/10, Loss: 0.18951664280891417\n",
            "Epoch 7/10, Loss: 0.19128578805923463\n",
            "Epoch 7/10, Loss: 0.19294376730918883\n",
            "Epoch 7/10, Loss: 0.19467741346359252\n",
            "Epoch 7/10, Loss: 0.19632023751735686\n",
            "Epoch 7/10, Loss: 0.19789468586444856\n",
            "Epoch 7/10, Loss: 0.19944272673130034\n",
            "Epoch 7/10, Loss: 0.20100987458229064\n",
            "Epoch 7/10, Loss: 0.20331827664375304\n",
            "Epoch 7/10, Loss: 0.2046868405342102\n",
            "Epoch 7/10, Loss: 0.2064184721708298\n",
            "Epoch 7/10, Loss: 0.20794543731212617\n",
            "Epoch 7/10, Loss: 0.20980738425254822\n",
            "Epoch 7/10, Loss: 0.21117348551750184\n",
            "Epoch 7/10, Loss: 0.21268929135799408\n",
            "Epoch 7/10, Loss: 0.21465467929840087\n",
            "Epoch 7/10, Loss: 0.21655696034431457\n",
            "Epoch 7/10, Loss: 0.21805102610588073\n",
            "Epoch 7/10, Loss: 0.2197186130285263\n",
            "Epoch 7/10, Loss: 0.2214692187309265\n",
            "Epoch 7/10, Loss: 0.22307119393348693\n",
            "Epoch 7/10, Loss: 0.2245285884141922\n",
            "Epoch 7/10, Loss: 0.2263347145318985\n",
            "Epoch 7/10, Loss: 0.2280073949098587\n",
            "Epoch 7/10, Loss: 0.2298281660079956\n",
            "Epoch 7/10, Loss: 0.23147635543346404\n",
            "Epoch 7/10, Loss: 0.23334744036197663\n",
            "Epoch 7/10, Loss: 0.23557849037647247\n",
            "Epoch 7/10, Loss: 0.2378073765039444\n",
            "Epoch 7/10, Loss: 0.23967758131027223\n",
            "Epoch 7/10, Loss: 0.24162663280963897\n",
            "Epoch 7/10, Loss: 0.2437273532152176\n",
            "Epoch 7/10, Loss: 0.24502925157546998\n",
            "Epoch 7/10, Loss: 0.24701205170154572\n",
            "Epoch 7/10, Loss: 0.24856135153770448\n",
            "Epoch 7/10, Loss: 0.2498848879337311\n",
            "Epoch 7/10, Loss: 0.25201220440864563\n",
            "Epoch 7/10, Loss: 0.253636354804039\n",
            "Epoch 7/10, Loss: 0.25535279560089114\n",
            "Epoch 7/10, Loss: 0.25707277274131773\n",
            "Epoch 7/10, Loss: 0.25867465090751646\n",
            "Epoch 7/10, Loss: 0.26032400751113893\n",
            "Epoch 7/10, Loss: 0.2618668186664581\n",
            "Epoch 7/10, Loss: 0.26321058320999147\n",
            "Epoch 7/10, Loss: 0.26471466207504274\n",
            "Epoch 7/10, Loss: 0.2660845766067505\n",
            "Epoch 7/10, Loss: 0.2679378920793533\n",
            "Epoch 7/10, Loss: 0.26946555840969083\n",
            "Epoch 7/10, Loss: 0.271181933760643\n",
            "Epoch 7/10, Loss: 0.2726115529537201\n",
            "Epoch 7/10, Loss: 0.2741451135873795\n",
            "Epoch 7/10, Loss: 0.27573213493824006\n",
            "Epoch 7/10, Loss: 0.2774423540830612\n",
            "Epoch 7/10, Loss: 0.2790537247657776\n",
            "Epoch 7/10, Loss: 0.28058510982990265\n",
            "Epoch 7/10, Loss: 0.2823504183292389\n",
            "Epoch 7/10, Loss: 0.28400815844535826\n",
            "Epoch 7/10, Loss: 0.28532495331764224\n",
            "Epoch 7/10, Loss: 0.28685194766521455\n",
            "Epoch 7/10, Loss: 0.288146672129631\n",
            "Epoch 7/10, Loss: 0.2901205070018768\n",
            "Epoch 7/10, Loss: 0.29200865876674653\n",
            "Epoch 7/10, Loss: 0.29397614967823027\n",
            "Epoch 7/10, Loss: 0.29537433230876925\n",
            "Epoch 7/10, Loss: 0.29681119549274443\n",
            "Epoch 7/10, Loss: 0.2987547180652618\n",
            "Epoch 7/10, Loss: 0.3009074501991272\n",
            "Epoch 7/10, Loss: 0.30220516777038575\n",
            "Epoch 7/10, Loss: 0.3039596792459488\n",
            "Epoch 7/10, Loss: 0.3066276563405991\n",
            "Epoch 7/10, Loss: 0.3080870490074158\n",
            "Epoch 7/10, Loss: 0.3096179219484329\n",
            "Epoch 7/10, Loss: 0.3116473058462143\n",
            "Epoch 7/10, Loss: 0.31379706394672396\n",
            "Epoch 7/10, Loss: 0.3153099477291107\n",
            "Epoch 7/10, Loss: 0.3175315873622894\n",
            "Epoch 7/10, Loss: 0.3198500897884369\n",
            "Epoch 7/10, Loss: 0.3213695495128632\n",
            "Epoch 7/10, Loss: 0.32322246754169465\n",
            "Epoch 7/10, Loss: 0.32480564546585083\n",
            "Epoch 7/10, Loss: 0.326400230884552\n",
            "Epoch 7/10, Loss: 0.3282619519233704\n",
            "Epoch 7/10, Loss: 0.330311856508255\n",
            "Epoch 7/10, Loss: 0.33161597275733945\n",
            "Epoch 7/10, Loss: 0.3330068657398224\n",
            "Epoch 7/10, Loss: 0.3346143401861191\n",
            "Epoch 7/10, Loss: 0.3359343478679657\n",
            "Epoch 7/10, Loss: 0.337847585439682\n",
            "Epoch 7/10, Loss: 0.33966839718818664\n",
            "Epoch 7/10, Loss: 0.3415041562318802\n",
            "Epoch 7/10, Loss: 0.3428710666894913\n",
            "Epoch 7/10, Loss: 0.3444849933385849\n",
            "Epoch 7/10, Loss: 0.34600905573368074\n",
            "Epoch 7/10, Loss: 0.3478647190332413\n",
            "Epoch 7/10, Loss: 0.34963869750499726\n",
            "Epoch 7/10, Loss: 0.3514773794412613\n",
            "Epoch 7/10, Loss: 0.3534991582632065\n",
            "Epoch 7/10, Loss: 0.3548947696685791\n",
            "Epoch 7/10, Loss: 0.35652165913581846\n",
            "Epoch 7/10, Loss: 0.3582593115568161\n",
            "Epoch 7/10, Loss: 0.35963963067531585\n",
            "Epoch 7/10, Loss: 0.36114994740486145\n",
            "Epoch 7/10, Loss: 0.3627987160682678\n",
            "Epoch 7/10, Loss: 0.36416655468940734\n",
            "Epoch 7/10, Loss: 0.3659755344390869\n",
            "Epoch 7/10, Loss: 0.3675907258987427\n",
            "Epoch 7/10, Loss: 0.36887273454666136\n",
            "Epoch 7/10, Loss: 0.37061744117736817\n",
            "Epoch 7/10, Loss: 0.371910786151886\n",
            "Epoch 7/10, Loss: 0.3738902621269226\n",
            "Epoch 7/10, Loss: 0.3753852360248566\n",
            "Epoch 7/10, Loss: 0.37717029321193696\n",
            "Epoch 7/10, Loss: 0.3793498414754868\n",
            "Epoch 7/10, Loss: 0.38095631742477415\n",
            "Epoch 7/10, Loss: 0.3825164864063263\n",
            "Epoch 7/10, Loss: 0.38386805760860443\n",
            "Epoch 7/10, Loss: 0.3857337675094604\n",
            "Epoch 7/10, Loss: 0.38765705180168153\n",
            "Epoch 7/10, Loss: 0.3893744000196457\n",
            "Epoch 7/10, Loss: 0.3907112022638321\n",
            "Epoch 7/10, Loss: 0.39249183106422425\n",
            "Epoch 7/10, Loss: 0.3940294600725174\n",
            "Epoch 7/10, Loss: 0.3953553012609482\n",
            "Epoch 7/10, Loss: 0.39739773070812223\n",
            "Epoch 7/10, Loss: 0.3987954170703888\n",
            "Epoch 7/10, Loss: 0.400558362364769\n",
            "Epoch 7/10, Loss: 0.40256690919399263\n",
            "Epoch 7/10, Loss: 0.4043642494678497\n",
            "Epoch 7/10, Loss: 0.40567249739170075\n",
            "Epoch 7/10, Loss: 0.40726545202732084\n",
            "Epoch 7/10, Loss: 0.4092571521997452\n",
            "Epoch 7/10, Loss: 0.4105685453414917\n",
            "Epoch 7/10, Loss: 0.4121705892086029\n",
            "Epoch 7/10, Loss: 0.413472305059433\n",
            "Epoch 7/10, Loss: 0.415013016462326\n",
            "Epoch 7/10, Loss: 0.4167340090274811\n",
            "Epoch 8/10, Loss: 0.0015244089365005493\n",
            "Epoch 8/10, Loss: 0.0032800747156143187\n",
            "Epoch 8/10, Loss: 0.005072018623352051\n",
            "Epoch 8/10, Loss: 0.006662673592567444\n",
            "Epoch 8/10, Loss: 0.00821132791042328\n",
            "Epoch 8/10, Loss: 0.00997645902633667\n",
            "Epoch 8/10, Loss: 0.011267402172088624\n",
            "Epoch 8/10, Loss: 0.013105261206626893\n",
            "Epoch 8/10, Loss: 0.014686294317245484\n",
            "Epoch 8/10, Loss: 0.01620895743370056\n",
            "Epoch 8/10, Loss: 0.01796964979171753\n",
            "Epoch 8/10, Loss: 0.019488052010536192\n",
            "Epoch 8/10, Loss: 0.02101473784446716\n",
            "Epoch 8/10, Loss: 0.022788100481033326\n",
            "Epoch 8/10, Loss: 0.024194979548454284\n",
            "Epoch 8/10, Loss: 0.026452921986579895\n",
            "Epoch 8/10, Loss: 0.02791267192363739\n",
            "Epoch 8/10, Loss: 0.029454708099365234\n",
            "Epoch 8/10, Loss: 0.030765336513519287\n",
            "Epoch 8/10, Loss: 0.03241048014163971\n",
            "Epoch 8/10, Loss: 0.03389996075630188\n",
            "Epoch 8/10, Loss: 0.03596657633781433\n",
            "Epoch 8/10, Loss: 0.037910555005073544\n",
            "Epoch 8/10, Loss: 0.03940422940254212\n",
            "Epoch 8/10, Loss: 0.041424479722976686\n",
            "Epoch 8/10, Loss: 0.04273623037338257\n",
            "Epoch 8/10, Loss: 0.04447959935665131\n",
            "Epoch 8/10, Loss: 0.045818998456001285\n",
            "Epoch 8/10, Loss: 0.04736654269695282\n",
            "Epoch 8/10, Loss: 0.04866840469837189\n",
            "Epoch 8/10, Loss: 0.04999486219882965\n",
            "Epoch 8/10, Loss: 0.05185892939567566\n",
            "Epoch 8/10, Loss: 0.05342522239685059\n",
            "Epoch 8/10, Loss: 0.054876571893692015\n",
            "Epoch 8/10, Loss: 0.05662860071659088\n",
            "Epoch 8/10, Loss: 0.05800717008113861\n",
            "Epoch 8/10, Loss: 0.059606170535087585\n",
            "Epoch 8/10, Loss: 0.061473243951797485\n",
            "Epoch 8/10, Loss: 0.06309149312973023\n",
            "Epoch 8/10, Loss: 0.06454339730739593\n",
            "Epoch 8/10, Loss: 0.06626798343658448\n",
            "Epoch 8/10, Loss: 0.06826272797584533\n",
            "Epoch 8/10, Loss: 0.0701733431816101\n",
            "Epoch 8/10, Loss: 0.07212864172458648\n",
            "Epoch 8/10, Loss: 0.07365438091754914\n",
            "Epoch 8/10, Loss: 0.07520263147354125\n",
            "Epoch 8/10, Loss: 0.07648800384998322\n",
            "Epoch 8/10, Loss: 0.07835925328731537\n",
            "Epoch 8/10, Loss: 0.07971481144428254\n",
            "Epoch 8/10, Loss: 0.08140909159183503\n",
            "Epoch 8/10, Loss: 0.08293247985839844\n",
            "Epoch 8/10, Loss: 0.08422669899463654\n",
            "Epoch 8/10, Loss: 0.08619217050075531\n",
            "Epoch 8/10, Loss: 0.08766017889976502\n",
            "Epoch 8/10, Loss: 0.09009593296051026\n",
            "Epoch 8/10, Loss: 0.09212122344970704\n",
            "Epoch 8/10, Loss: 0.09347795820236206\n",
            "Epoch 8/10, Loss: 0.09501185977458954\n",
            "Epoch 8/10, Loss: 0.09704710376262665\n",
            "Epoch 8/10, Loss: 0.09897302091121674\n",
            "Epoch 8/10, Loss: 0.1007122665643692\n",
            "Epoch 8/10, Loss: 0.10223097681999206\n",
            "Epoch 8/10, Loss: 0.1040454386472702\n",
            "Epoch 8/10, Loss: 0.10559203386306763\n",
            "Epoch 8/10, Loss: 0.10708808505535125\n",
            "Epoch 8/10, Loss: 0.10881411981582642\n",
            "Epoch 8/10, Loss: 0.11012697100639343\n",
            "Epoch 8/10, Loss: 0.11148035609722137\n",
            "Epoch 8/10, Loss: 0.11287725663185119\n",
            "Epoch 8/10, Loss: 0.11428755879402161\n",
            "Epoch 8/10, Loss: 0.11634860730171204\n",
            "Epoch 8/10, Loss: 0.11775036072731018\n",
            "Epoch 8/10, Loss: 0.11923256659507751\n",
            "Epoch 8/10, Loss: 0.1206925848722458\n",
            "Epoch 8/10, Loss: 0.12231018960475921\n",
            "Epoch 8/10, Loss: 0.1241219527721405\n",
            "Epoch 8/10, Loss: 0.12576715242862702\n",
            "Epoch 8/10, Loss: 0.12755440890789033\n",
            "Epoch 8/10, Loss: 0.12928796005249024\n",
            "Epoch 8/10, Loss: 0.13105806708335876\n",
            "Epoch 8/10, Loss: 0.13267429506778716\n",
            "Epoch 8/10, Loss: 0.13421426904201508\n",
            "Epoch 8/10, Loss: 0.13605519533157348\n",
            "Epoch 8/10, Loss: 0.13805650782585144\n",
            "Epoch 8/10, Loss: 0.13939659118652345\n",
            "Epoch 8/10, Loss: 0.1410638757944107\n",
            "Epoch 8/10, Loss: 0.14279918432235716\n",
            "Epoch 8/10, Loss: 0.14426315832138062\n",
            "Epoch 8/10, Loss: 0.14608839559555054\n",
            "Epoch 8/10, Loss: 0.14766740369796752\n",
            "Epoch 8/10, Loss: 0.1492576777935028\n",
            "Epoch 8/10, Loss: 0.15126718878746032\n",
            "Epoch 8/10, Loss: 0.15300317895412446\n",
            "Epoch 8/10, Loss: 0.1545944319963455\n",
            "Epoch 8/10, Loss: 0.15601781368255616\n",
            "Epoch 8/10, Loss: 0.15789550387859344\n",
            "Epoch 8/10, Loss: 0.15917321729660033\n",
            "Epoch 8/10, Loss: 0.16088391768932342\n",
            "Epoch 8/10, Loss: 0.16228635656833648\n",
            "Epoch 8/10, Loss: 0.16359542632102966\n",
            "Epoch 8/10, Loss: 0.16515528845787047\n",
            "Epoch 8/10, Loss: 0.16675111484527588\n",
            "Epoch 8/10, Loss: 0.16834044635295867\n",
            "Epoch 8/10, Loss: 0.16999034106731414\n",
            "Epoch 8/10, Loss: 0.17160196685791015\n",
            "Epoch 8/10, Loss: 0.17320288181304933\n",
            "Epoch 8/10, Loss: 0.1746999113559723\n",
            "Epoch 8/10, Loss: 0.17635224783420564\n",
            "Epoch 8/10, Loss: 0.1780077792406082\n",
            "Epoch 8/10, Loss: 0.17981667745113372\n",
            "Epoch 8/10, Loss: 0.1821258064508438\n",
            "Epoch 8/10, Loss: 0.18365787315368653\n",
            "Epoch 8/10, Loss: 0.185343510389328\n",
            "Epoch 8/10, Loss: 0.18699391961097717\n",
            "Epoch 8/10, Loss: 0.18835182380676269\n",
            "Epoch 8/10, Loss: 0.19009798896312713\n",
            "Epoch 8/10, Loss: 0.1917368061542511\n",
            "Epoch 8/10, Loss: 0.1934786618947983\n",
            "Epoch 8/10, Loss: 0.19510216104984285\n",
            "Epoch 8/10, Loss: 0.1966680371761322\n",
            "Epoch 8/10, Loss: 0.19820640337467194\n",
            "Epoch 8/10, Loss: 0.19976779317855836\n",
            "Epoch 8/10, Loss: 0.20208455443382264\n",
            "Epoch 8/10, Loss: 0.2034378627538681\n",
            "Epoch 8/10, Loss: 0.205160751581192\n",
            "Epoch 8/10, Loss: 0.2066822210550308\n",
            "Epoch 8/10, Loss: 0.20853164327144622\n",
            "Epoch 8/10, Loss: 0.20988941717147827\n",
            "Epoch 8/10, Loss: 0.21138909471035003\n",
            "Epoch 8/10, Loss: 0.21336076879501342\n",
            "Epoch 8/10, Loss: 0.2152629851102829\n",
            "Epoch 8/10, Loss: 0.21675023734569548\n",
            "Epoch 8/10, Loss: 0.21841024696826936\n",
            "Epoch 8/10, Loss: 0.22015882432460784\n",
            "Epoch 8/10, Loss: 0.22174362182617188\n",
            "Epoch 8/10, Loss: 0.2231747990846634\n",
            "Epoch 8/10, Loss: 0.2249681159257889\n",
            "Epoch 8/10, Loss: 0.22663099014759064\n",
            "Epoch 8/10, Loss: 0.22844049274921419\n",
            "Epoch 8/10, Loss: 0.23009828090667725\n",
            "Epoch 8/10, Loss: 0.23195706152915954\n",
            "Epoch 8/10, Loss: 0.23418683290481568\n",
            "Epoch 8/10, Loss: 0.2364172797203064\n",
            "Epoch 8/10, Loss: 0.2382646243572235\n",
            "Epoch 8/10, Loss: 0.24021707201004028\n",
            "Epoch 8/10, Loss: 0.24230171322822572\n",
            "Epoch 8/10, Loss: 0.24359154653549195\n",
            "Epoch 8/10, Loss: 0.2455786191225052\n",
            "Epoch 8/10, Loss: 0.24711875998973845\n",
            "Epoch 8/10, Loss: 0.2484279478788376\n",
            "Epoch 8/10, Loss: 0.2505583654642105\n",
            "Epoch 8/10, Loss: 0.2521736023426056\n",
            "Epoch 8/10, Loss: 0.25387566220760344\n",
            "Epoch 8/10, Loss: 0.25559796667099\n",
            "Epoch 8/10, Loss: 0.2571822823286056\n",
            "Epoch 8/10, Loss: 0.2588098680973053\n",
            "Epoch 8/10, Loss: 0.26034558975696565\n",
            "Epoch 8/10, Loss: 0.26168985533714295\n",
            "Epoch 8/10, Loss: 0.26318444323539736\n",
            "Epoch 8/10, Loss: 0.2645372363328934\n",
            "Epoch 8/10, Loss: 0.26638029956817627\n",
            "Epoch 8/10, Loss: 0.2679029324054718\n",
            "Epoch 8/10, Loss: 0.2696086599826813\n",
            "Epoch 8/10, Loss: 0.27100863933563235\n",
            "Epoch 8/10, Loss: 0.2725353829860687\n",
            "Epoch 8/10, Loss: 0.27410696530342105\n",
            "Epoch 8/10, Loss: 0.2758180772066116\n",
            "Epoch 8/10, Loss: 0.2774223289489746\n",
            "Epoch 8/10, Loss: 0.2789603234529495\n",
            "Epoch 8/10, Loss: 0.28072224342823027\n",
            "Epoch 8/10, Loss: 0.28238408493995665\n",
            "Epoch 8/10, Loss: 0.28368546354770663\n",
            "Epoch 8/10, Loss: 0.2852089099884033\n",
            "Epoch 8/10, Loss: 0.2864947719573975\n",
            "Epoch 8/10, Loss: 0.2884458601474762\n",
            "Epoch 8/10, Loss: 0.29032853949069976\n",
            "Epoch 8/10, Loss: 0.29229668247699736\n",
            "Epoch 8/10, Loss: 0.29365089213848117\n",
            "Epoch 8/10, Loss: 0.29505482399463656\n",
            "Epoch 8/10, Loss: 0.29698771905899046\n",
            "Epoch 8/10, Loss: 0.2991368138790131\n",
            "Epoch 8/10, Loss: 0.3004241502285004\n",
            "Epoch 8/10, Loss: 0.30219484531879426\n",
            "Epoch 8/10, Loss: 0.30487529170513156\n",
            "Epoch 8/10, Loss: 0.30632717633247375\n",
            "Epoch 8/10, Loss: 0.30785268092155454\n",
            "Epoch 8/10, Loss: 0.3098393532037735\n",
            "Epoch 8/10, Loss: 0.31197128593921664\n",
            "Epoch 8/10, Loss: 0.31344464886188506\n",
            "Epoch 8/10, Loss: 0.3156485878229141\n",
            "Epoch 8/10, Loss: 0.31795702755451205\n",
            "Epoch 8/10, Loss: 0.3194553024768829\n",
            "Epoch 8/10, Loss: 0.321329177737236\n",
            "Epoch 8/10, Loss: 0.3229096337556839\n",
            "Epoch 8/10, Loss: 0.3244957450628281\n",
            "Epoch 8/10, Loss: 0.32636006009578705\n",
            "Epoch 8/10, Loss: 0.32840303575992585\n",
            "Epoch 8/10, Loss: 0.32969626557827\n",
            "Epoch 8/10, Loss: 0.331093964099884\n",
            "Epoch 8/10, Loss: 0.3326887313127518\n",
            "Epoch 8/10, Loss: 0.3339928444623947\n",
            "Epoch 8/10, Loss: 0.3359048448801041\n",
            "Epoch 8/10, Loss: 0.33771145248413087\n",
            "Epoch 8/10, Loss: 0.3395279369354248\n",
            "Epoch 8/10, Loss: 0.34087897741794587\n",
            "Epoch 8/10, Loss: 0.34248914861679075\n",
            "Epoch 8/10, Loss: 0.3440084655284882\n",
            "Epoch 8/10, Loss: 0.3458394904136658\n",
            "Epoch 8/10, Loss: 0.3476091194152832\n",
            "Epoch 8/10, Loss: 0.34944029569625856\n",
            "Epoch 8/10, Loss: 0.3514581289291382\n",
            "Epoch 8/10, Loss: 0.3528340303897858\n",
            "Epoch 8/10, Loss: 0.354465322971344\n",
            "Epoch 8/10, Loss: 0.35620091497898104\n",
            "Epoch 8/10, Loss: 0.3575571792125702\n",
            "Epoch 8/10, Loss: 0.3590704997777939\n",
            "Epoch 8/10, Loss: 0.36070397520065306\n",
            "Epoch 8/10, Loss: 0.3620691999197006\n",
            "Epoch 8/10, Loss: 0.3638684530258179\n",
            "Epoch 8/10, Loss: 0.36546982038021086\n",
            "Epoch 8/10, Loss: 0.36674549758434294\n",
            "Epoch 8/10, Loss: 0.3684783033132553\n",
            "Epoch 8/10, Loss: 0.3697676784992218\n",
            "Epoch 8/10, Loss: 0.371731086730957\n",
            "Epoch 8/10, Loss: 0.3732126853466034\n",
            "Epoch 8/10, Loss: 0.3749913516044617\n",
            "Epoch 8/10, Loss: 0.3771831440925598\n",
            "Epoch 8/10, Loss: 0.37878104591369627\n",
            "Epoch 8/10, Loss: 0.38033887565135954\n",
            "Epoch 8/10, Loss: 0.38167390203475954\n",
            "Epoch 8/10, Loss: 0.38351820623874666\n",
            "Epoch 8/10, Loss: 0.3854409230947495\n",
            "Epoch 8/10, Loss: 0.3871487545967102\n",
            "Epoch 8/10, Loss: 0.3884725478887558\n",
            "Epoch 8/10, Loss: 0.39024808168411257\n",
            "Epoch 8/10, Loss: 0.39177952909469604\n",
            "Epoch 8/10, Loss: 0.39308865189552306\n",
            "Epoch 8/10, Loss: 0.3951175842285156\n",
            "Epoch 8/10, Loss: 0.39649224662780763\n",
            "Epoch 8/10, Loss: 0.3982550663948059\n",
            "Epoch 8/10, Loss: 0.4002601654529572\n",
            "Epoch 8/10, Loss: 0.4020669676065445\n",
            "Epoch 8/10, Loss: 0.40336052179336546\n",
            "Epoch 8/10, Loss: 0.4049487384557724\n",
            "Epoch 8/10, Loss: 0.4069419964551926\n",
            "Epoch 8/10, Loss: 0.40824227094650267\n",
            "Epoch 8/10, Loss: 0.4098466194868088\n",
            "Epoch 8/10, Loss: 0.41114151406288146\n",
            "Epoch 8/10, Loss: 0.41268691658973694\n",
            "Epoch 8/10, Loss: 0.41441521763801575\n",
            "Epoch 9/10, Loss: 0.001520581603050232\n",
            "Epoch 9/10, Loss: 0.003274815917015076\n",
            "Epoch 9/10, Loss: 0.005062781572341919\n",
            "Epoch 9/10, Loss: 0.006649710893630981\n",
            "Epoch 9/10, Loss: 0.008192853093147278\n",
            "Epoch 9/10, Loss: 0.009948070764541626\n",
            "Epoch 9/10, Loss: 0.011233088731765747\n",
            "Epoch 9/10, Loss: 0.013067380428314209\n",
            "Epoch 9/10, Loss: 0.014642855167388917\n",
            "Epoch 9/10, Loss: 0.016157930850982667\n",
            "Epoch 9/10, Loss: 0.017914164900779725\n",
            "Epoch 9/10, Loss: 0.019429093837738037\n",
            "Epoch 9/10, Loss: 0.020953242659568785\n",
            "Epoch 9/10, Loss: 0.02272296488285065\n",
            "Epoch 9/10, Loss: 0.02410576105117798\n",
            "Epoch 9/10, Loss: 0.026356193780899047\n",
            "Epoch 9/10, Loss: 0.027801202058792114\n",
            "Epoch 9/10, Loss: 0.02932033681869507\n",
            "Epoch 9/10, Loss: 0.030624393939971923\n",
            "Epoch 9/10, Loss: 0.03224063777923584\n",
            "Epoch 9/10, Loss: 0.03372943437099457\n",
            "Epoch 9/10, Loss: 0.03579025256633758\n",
            "Epoch 9/10, Loss: 0.037737519025802614\n",
            "Epoch 9/10, Loss: 0.03922183322906494\n",
            "Epoch 9/10, Loss: 0.04124842023849487\n",
            "Epoch 9/10, Loss: 0.042549137711524965\n",
            "Epoch 9/10, Loss: 0.04427793490886688\n",
            "Epoch 9/10, Loss: 0.04561408841609955\n",
            "Epoch 9/10, Loss: 0.047155519366264345\n",
            "Epoch 9/10, Loss: 0.048448153734207154\n",
            "Epoch 9/10, Loss: 0.04976365554332733\n",
            "Epoch 9/10, Loss: 0.05162268257141113\n",
            "Epoch 9/10, Loss: 0.053187226772308346\n",
            "Epoch 9/10, Loss: 0.05462201714515686\n",
            "Epoch 9/10, Loss: 0.05635944616794586\n",
            "Epoch 9/10, Loss: 0.057745747089385985\n",
            "Epoch 9/10, Loss: 0.05933115684986114\n",
            "Epoch 9/10, Loss: 0.06118364071846008\n",
            "Epoch 9/10, Loss: 0.06279038810729981\n",
            "Epoch 9/10, Loss: 0.06424728858470917\n",
            "Epoch 9/10, Loss: 0.06596475470066071\n",
            "Epoch 9/10, Loss: 0.06795646846294404\n",
            "Epoch 9/10, Loss: 0.06984995126724243\n",
            "Epoch 9/10, Loss: 0.07180400252342224\n",
            "Epoch 9/10, Loss: 0.07332532775402069\n",
            "Epoch 9/10, Loss: 0.07487481307983399\n",
            "Epoch 9/10, Loss: 0.07615232372283935\n",
            "Epoch 9/10, Loss: 0.07802907156944275\n",
            "Epoch 9/10, Loss: 0.07936463689804077\n",
            "Epoch 9/10, Loss: 0.08105858039855957\n",
            "Epoch 9/10, Loss: 0.08257774019241333\n",
            "Epoch 9/10, Loss: 0.08386151003837586\n",
            "Epoch 9/10, Loss: 0.08580739963054657\n",
            "Epoch 9/10, Loss: 0.0872818843126297\n",
            "Epoch 9/10, Loss: 0.08973203384876251\n",
            "Epoch 9/10, Loss: 0.09175287497043609\n",
            "Epoch 9/10, Loss: 0.09309216952323913\n",
            "Epoch 9/10, Loss: 0.0946197235584259\n",
            "Epoch 9/10, Loss: 0.0966412434577942\n",
            "Epoch 9/10, Loss: 0.09855960619449615\n",
            "Epoch 9/10, Loss: 0.10028479099273682\n",
            "Epoch 9/10, Loss: 0.10180078375339507\n",
            "Epoch 9/10, Loss: 0.10361375200748443\n",
            "Epoch 9/10, Loss: 0.10515268278121949\n",
            "Epoch 9/10, Loss: 0.10665128755569458\n",
            "Epoch 9/10, Loss: 0.10836363005638122\n",
            "Epoch 9/10, Loss: 0.10965903735160827\n",
            "Epoch 9/10, Loss: 0.11099092543125152\n",
            "Epoch 9/10, Loss: 0.11236368203163147\n",
            "Epoch 9/10, Loss: 0.11376246821880341\n",
            "Epoch 9/10, Loss: 0.11581818306446076\n",
            "Epoch 9/10, Loss: 0.11719987964630127\n",
            "Epoch 9/10, Loss: 0.1186678683757782\n",
            "Epoch 9/10, Loss: 0.12013584983348846\n",
            "Epoch 9/10, Loss: 0.12175393331050872\n",
            "Epoch 9/10, Loss: 0.12357663071155547\n",
            "Epoch 9/10, Loss: 0.12521707260608672\n",
            "Epoch 9/10, Loss: 0.12699780762195587\n",
            "Epoch 9/10, Loss: 0.12871846747398377\n",
            "Epoch 9/10, Loss: 0.13049481534957885\n",
            "Epoch 9/10, Loss: 0.1320904792547226\n",
            "Epoch 9/10, Loss: 0.13362254631519319\n",
            "Epoch 9/10, Loss: 0.13548401880264282\n",
            "Epoch 9/10, Loss: 0.13748897290229797\n",
            "Epoch 9/10, Loss: 0.13882566714286804\n",
            "Epoch 9/10, Loss: 0.14047546565532684\n",
            "Epoch 9/10, Loss: 0.1421827074289322\n",
            "Epoch 9/10, Loss: 0.1436419360637665\n",
            "Epoch 9/10, Loss: 0.14545117795467377\n",
            "Epoch 9/10, Loss: 0.14703274893760682\n",
            "Epoch 9/10, Loss: 0.1485983703136444\n",
            "Epoch 9/10, Loss: 0.150607093334198\n",
            "Epoch 9/10, Loss: 0.1523284310102463\n",
            "Epoch 9/10, Loss: 0.15390533077716828\n",
            "Epoch 9/10, Loss: 0.15529523003101348\n",
            "Epoch 9/10, Loss: 0.15714903712272643\n",
            "Epoch 9/10, Loss: 0.1584225071668625\n",
            "Epoch 9/10, Loss: 0.16013813602924348\n",
            "Epoch 9/10, Loss: 0.16152051317691804\n",
            "Epoch 9/10, Loss: 0.16282512629032134\n",
            "Epoch 9/10, Loss: 0.1643781088590622\n",
            "Epoch 9/10, Loss: 0.16596988832950593\n",
            "Epoch 9/10, Loss: 0.16757562518119812\n",
            "Epoch 9/10, Loss: 0.16922391653060914\n",
            "Epoch 9/10, Loss: 0.17082559287548066\n",
            "Epoch 9/10, Loss: 0.17241943335533141\n",
            "Epoch 9/10, Loss: 0.1738986339569092\n",
            "Epoch 9/10, Loss: 0.17553902471065522\n",
            "Epoch 9/10, Loss: 0.17718076419830323\n",
            "Epoch 9/10, Loss: 0.17898958599567413\n",
            "Epoch 9/10, Loss: 0.18130055153369903\n",
            "Epoch 9/10, Loss: 0.18282833313941957\n",
            "Epoch 9/10, Loss: 0.1845221837759018\n",
            "Epoch 9/10, Loss: 0.18615129458904267\n",
            "Epoch 9/10, Loss: 0.18751834774017334\n",
            "Epoch 9/10, Loss: 0.18924209403991699\n",
            "Epoch 9/10, Loss: 0.19086981976032258\n",
            "Epoch 9/10, Loss: 0.19261928927898406\n",
            "Epoch 9/10, Loss: 0.19422693240642547\n",
            "Epoch 9/10, Loss: 0.1957856135368347\n",
            "Epoch 9/10, Loss: 0.1973175175189972\n",
            "Epoch 9/10, Loss: 0.1988761923313141\n",
            "Epoch 9/10, Loss: 0.2012064311504364\n",
            "Epoch 9/10, Loss: 0.20255210304260254\n",
            "Epoch 9/10, Loss: 0.20427685618400573\n",
            "Epoch 9/10, Loss: 0.2057946127653122\n",
            "Epoch 9/10, Loss: 0.20764019978046416\n",
            "Epoch 9/10, Loss: 0.20899527752399444\n",
            "Epoch 9/10, Loss: 0.21048212850093842\n",
            "Epoch 9/10, Loss: 0.21245912790298463\n",
            "Epoch 9/10, Loss: 0.2143627324104309\n",
            "Epoch 9/10, Loss: 0.21584298503398897\n",
            "Epoch 9/10, Loss: 0.2174970978498459\n",
            "Epoch 9/10, Loss: 0.21924544525146483\n",
            "Epoch 9/10, Loss: 0.22081654059886932\n",
            "Epoch 9/10, Loss: 0.22222445905208588\n",
            "Epoch 9/10, Loss: 0.22400895142555236\n",
            "Epoch 9/10, Loss: 0.2256639178991318\n",
            "Epoch 9/10, Loss: 0.22746296739578248\n",
            "Epoch 9/10, Loss: 0.22912989246845245\n",
            "Epoch 9/10, Loss: 0.2309774248600006\n",
            "Epoch 9/10, Loss: 0.23320240116119384\n",
            "Epoch 9/10, Loss: 0.23543263959884644\n",
            "Epoch 9/10, Loss: 0.2372638087272644\n",
            "Epoch 9/10, Loss: 0.23921723663806915\n",
            "Epoch 9/10, Loss: 0.24129036128520964\n",
            "Epoch 9/10, Loss: 0.2425721148252487\n",
            "Epoch 9/10, Loss: 0.24456735515594483\n",
            "Epoch 9/10, Loss: 0.2461016401052475\n",
            "Epoch 9/10, Loss: 0.24739921510219573\n",
            "Epoch 9/10, Loss: 0.24952774441242218\n",
            "Epoch 9/10, Loss: 0.2511368426084518\n",
            "Epoch 9/10, Loss: 0.25282294416427614\n",
            "Epoch 9/10, Loss: 0.2545465084314346\n",
            "Epoch 9/10, Loss: 0.2561128054857254\n",
            "Epoch 9/10, Loss: 0.2577220677137375\n",
            "Epoch 9/10, Loss: 0.25925259363651276\n",
            "Epoch 9/10, Loss: 0.260601709485054\n",
            "Epoch 9/10, Loss: 0.26208788764476776\n",
            "Epoch 9/10, Loss: 0.2634266867637634\n",
            "Epoch 9/10, Loss: 0.26526038360595705\n",
            "Epoch 9/10, Loss: 0.26677934277057647\n",
            "Epoch 9/10, Loss: 0.26847046506404876\n",
            "Epoch 9/10, Loss: 0.26984466898441317\n",
            "Epoch 9/10, Loss: 0.2713665628433228\n",
            "Epoch 9/10, Loss: 0.27291992616653443\n",
            "Epoch 9/10, Loss: 0.274630747795105\n",
            "Epoch 9/10, Loss: 0.27622601330280305\n",
            "Epoch 9/10, Loss: 0.2777612817287445\n",
            "Epoch 9/10, Loss: 0.27952067279815673\n",
            "Epoch 9/10, Loss: 0.2811868923902512\n",
            "Epoch 9/10, Loss: 0.2824781609773636\n",
            "Epoch 9/10, Loss: 0.28399896597862245\n",
            "Epoch 9/10, Loss: 0.28527896165847777\n",
            "Epoch 9/10, Loss: 0.2872138957977295\n",
            "Epoch 9/10, Loss: 0.2890924253463745\n",
            "Epoch 9/10, Loss: 0.2910628654956818\n",
            "Epoch 9/10, Loss: 0.2923937015533447\n",
            "Epoch 9/10, Loss: 0.2937720936536789\n",
            "Epoch 9/10, Loss: 0.29569524371623995\n",
            "Epoch 9/10, Loss: 0.29783816850185396\n",
            "Epoch 9/10, Loss: 0.2991184777021408\n",
            "Epoch 9/10, Loss: 0.30090212070941924\n",
            "Epoch 9/10, Loss: 0.3035873264074326\n",
            "Epoch 9/10, Loss: 0.305028892159462\n",
            "Epoch 9/10, Loss: 0.3065507891178131\n",
            "Epoch 9/10, Loss: 0.3085037715435028\n",
            "Epoch 9/10, Loss: 0.310618574142456\n",
            "Epoch 9/10, Loss: 0.3120648974180222\n",
            "Epoch 9/10, Loss: 0.31425932013988495\n",
            "Epoch 9/10, Loss: 0.3165573731660843\n",
            "Epoch 9/10, Loss: 0.3180374097824097\n",
            "Epoch 9/10, Loss: 0.3199277538061142\n",
            "Epoch 9/10, Loss: 0.321505819439888\n",
            "Epoch 9/10, Loss: 0.3230838748216629\n",
            "Epoch 9/10, Loss: 0.32494610965251924\n",
            "Epoch 9/10, Loss: 0.32698319733142855\n",
            "Epoch 9/10, Loss: 0.3282685018777847\n",
            "Epoch 9/10, Loss: 0.32967152643203734\n",
            "Epoch 9/10, Loss: 0.3312546422481537\n",
            "Epoch 9/10, Loss: 0.33254764556884764\n",
            "Epoch 9/10, Loss: 0.3344605189561844\n",
            "Epoch 9/10, Loss: 0.33625161230564116\n",
            "Epoch 9/10, Loss: 0.3380566433668137\n",
            "Epoch 9/10, Loss: 0.3393951692581177\n",
            "Epoch 9/10, Loss: 0.34100322842597963\n",
            "Epoch 9/10, Loss: 0.342519327044487\n",
            "Epoch 9/10, Loss: 0.34433210229873656\n",
            "Epoch 9/10, Loss: 0.3460986144542694\n",
            "Epoch 9/10, Loss: 0.34792266523838045\n",
            "Epoch 9/10, Loss: 0.34993779933452607\n",
            "Epoch 9/10, Loss: 0.35130062139034274\n",
            "Epoch 9/10, Loss: 0.3529327609539032\n",
            "Epoch 9/10, Loss: 0.35466579592227937\n",
            "Epoch 9/10, Loss: 0.3560015635490417\n",
            "Epoch 9/10, Loss: 0.35751686489582063\n",
            "Epoch 9/10, Loss: 0.35913697111606596\n",
            "Epoch 9/10, Loss: 0.36050466692447664\n",
            "Epoch 9/10, Loss: 0.36229815196990967\n",
            "Epoch 9/10, Loss: 0.3638821723461151\n",
            "Epoch 9/10, Loss: 0.36515346574783325\n",
            "Epoch 9/10, Loss: 0.3668777334690094\n",
            "Epoch 9/10, Loss: 0.3681653081178665\n",
            "Epoch 9/10, Loss: 0.37011474823951723\n",
            "Epoch 9/10, Loss: 0.3715791264772415\n",
            "Epoch 9/10, Loss: 0.37335336196422575\n",
            "Epoch 9/10, Loss: 0.3755539108514786\n",
            "Epoch 9/10, Loss: 0.37714428555965424\n",
            "Epoch 9/10, Loss: 0.3786995666027069\n",
            "Epoch 9/10, Loss: 0.380022360086441\n",
            "Epoch 9/10, Loss: 0.3818444803953171\n",
            "Epoch 9/10, Loss: 0.3837630776166916\n",
            "Epoch 9/10, Loss: 0.38546177875995635\n",
            "Epoch 9/10, Loss: 0.3867769906520844\n",
            "Epoch 9/10, Loss: 0.38854928624629975\n",
            "Epoch 9/10, Loss: 0.39007576084136963\n",
            "Epoch 9/10, Loss: 0.3913740828037262\n",
            "Epoch 9/10, Loss: 0.39338187384605405\n",
            "Epoch 9/10, Loss: 0.3947401548624039\n",
            "Epoch 9/10, Loss: 0.39649970149993896\n",
            "Epoch 9/10, Loss: 0.3985025119781494\n",
            "Epoch 9/10, Loss: 0.40031862366199494\n",
            "Epoch 9/10, Loss: 0.4016027591228485\n",
            "Epoch 9/10, Loss: 0.4031855571269989\n",
            "Epoch 9/10, Loss: 0.40518048918247224\n",
            "Epoch 9/10, Loss: 0.40647350883483885\n",
            "Epoch 9/10, Loss: 0.408081192612648\n",
            "Epoch 9/10, Loss: 0.4093712193965912\n",
            "Epoch 9/10, Loss: 0.410920645236969\n",
            "Epoch 9/10, Loss: 0.4126535758972168\n",
            "Epoch 10/10, Loss: 0.0015178463459014893\n",
            "Epoch 10/10, Loss: 0.0032713046073913573\n",
            "Epoch 10/10, Loss: 0.005056553363800049\n",
            "Epoch 10/10, Loss: 0.006640574336051941\n",
            "Epoch 10/10, Loss: 0.008179173231124878\n",
            "Epoch 10/10, Loss: 0.009922430872917175\n",
            "Epoch 10/10, Loss: 0.011203877210617065\n",
            "Epoch 10/10, Loss: 0.013036302089691162\n",
            "Epoch 10/10, Loss: 0.014611823558807373\n",
            "Epoch 10/10, Loss: 0.016119786262512207\n",
            "Epoch 10/10, Loss: 0.017870970606803895\n",
            "Epoch 10/10, Loss: 0.019383262753486633\n",
            "Epoch 10/10, Loss: 0.020905622124671935\n",
            "Epoch 10/10, Loss: 0.022672323346138\n",
            "Epoch 10/10, Loss: 0.024039450883865356\n",
            "Epoch 10/10, Loss: 0.02628511953353882\n",
            "Epoch 10/10, Loss: 0.02771497929096222\n",
            "Epoch 10/10, Loss: 0.029217161774635316\n",
            "Epoch 10/10, Loss: 0.030516472458839417\n",
            "Epoch 10/10, Loss: 0.032104819893836976\n",
            "Epoch 10/10, Loss: 0.03359478092193603\n",
            "Epoch 10/10, Loss: 0.03564934086799622\n",
            "Epoch 10/10, Loss: 0.03760041582584381\n",
            "Epoch 10/10, Loss: 0.03907605791091919\n",
            "Epoch 10/10, Loss: 0.04110954356193543\n",
            "Epoch 10/10, Loss: 0.04240257525444031\n",
            "Epoch 10/10, Loss: 0.04411617183685303\n",
            "Epoch 10/10, Loss: 0.0454502055644989\n",
            "Epoch 10/10, Loss: 0.04698730182647705\n",
            "Epoch 10/10, Loss: 0.048273688435554504\n",
            "Epoch 10/10, Loss: 0.04958293426036835\n",
            "Epoch 10/10, Loss: 0.05143321669101715\n",
            "Epoch 10/10, Loss: 0.05299748682975769\n",
            "Epoch 10/10, Loss: 0.05441999411582947\n",
            "Epoch 10/10, Loss: 0.056140844464302064\n",
            "Epoch 10/10, Loss: 0.05753288292884827\n",
            "Epoch 10/10, Loss: 0.0591091034412384\n",
            "Epoch 10/10, Loss: 0.06095875608921051\n",
            "Epoch 10/10, Loss: 0.0625593398809433\n",
            "Epoch 10/10, Loss: 0.06401970613002778\n",
            "Epoch 10/10, Loss: 0.06573093771934509\n",
            "Epoch 10/10, Loss: 0.06772051429748535\n",
            "Epoch 10/10, Loss: 0.06959720385074615\n",
            "Epoch 10/10, Loss: 0.07155091166496277\n",
            "Epoch 10/10, Loss: 0.0730691431760788\n",
            "Epoch 10/10, Loss: 0.07462000107765197\n",
            "Epoch 10/10, Loss: 0.07589227032661439\n",
            "Epoch 10/10, Loss: 0.07777575242519379\n",
            "Epoch 10/10, Loss: 0.07909726011753082\n",
            "Epoch 10/10, Loss: 0.08079388606548309\n",
            "Epoch 10/10, Loss: 0.08231014013290405\n",
            "Epoch 10/10, Loss: 0.0835873337984085\n",
            "Epoch 10/10, Loss: 0.08551406240463257\n",
            "Epoch 10/10, Loss: 0.08699369132518768\n",
            "Epoch 10/10, Loss: 0.08945171082019807\n",
            "Epoch 10/10, Loss: 0.09146820294857025\n",
            "Epoch 10/10, Loss: 0.0927958949804306\n",
            "Epoch 10/10, Loss: 0.09431883943080902\n",
            "Epoch 10/10, Loss: 0.09632916176319123\n",
            "Epoch 10/10, Loss: 0.09824121141433716\n",
            "Epoch 10/10, Loss: 0.09994985806941986\n",
            "Epoch 10/10, Loss: 0.10146377623081207\n",
            "Epoch 10/10, Loss: 0.10327560579776764\n",
            "Epoch 10/10, Loss: 0.10481175696849823\n",
            "Epoch 10/10, Loss: 0.10631160914897919\n",
            "Epoch 10/10, Loss: 0.10800474679470062\n",
            "Epoch 10/10, Loss: 0.10928872561454774\n",
            "Epoch 10/10, Loss: 0.11060463154315948\n",
            "Epoch 10/10, Loss: 0.11195853006839752\n",
            "Epoch 10/10, Loss: 0.11334821784496307\n",
            "Epoch 10/10, Loss: 0.11539781129360199\n",
            "Epoch 10/10, Loss: 0.11676625156402588\n",
            "Epoch 10/10, Loss: 0.11822120583057404\n",
            "Epoch 10/10, Loss: 0.11970012950897217\n",
            "Epoch 10/10, Loss: 0.12131992721557618\n",
            "Epoch 10/10, Loss: 0.12316111898422241\n",
            "Epoch 10/10, Loss: 0.12480301451683044\n",
            "Epoch 10/10, Loss: 0.12657927203178407\n",
            "Epoch 10/10, Loss: 0.12829301178455352\n",
            "Epoch 10/10, Loss: 0.13007189238071443\n",
            "Epoch 10/10, Loss: 0.13165248799324036\n",
            "Epoch 10/10, Loss: 0.13317892456054686\n",
            "Epoch 10/10, Loss: 0.13505731678009034\n",
            "Epoch 10/10, Loss: 0.13706964015960693\n",
            "Epoch 10/10, Loss: 0.13840663492679595\n",
            "Epoch 10/10, Loss: 0.14004147732257843\n",
            "Epoch 10/10, Loss: 0.14172484648227693\n",
            "Epoch 10/10, Loss: 0.14318181753158568\n",
            "Epoch 10/10, Loss: 0.1449855080842972\n",
            "Epoch 10/10, Loss: 0.14657674753665925\n",
            "Epoch 10/10, Loss: 0.14812258338928222\n",
            "Epoch 10/10, Loss: 0.15013089561462403\n",
            "Epoch 10/10, Loss: 0.15183726954460144\n",
            "Epoch 10/10, Loss: 0.1534043205976486\n",
            "Epoch 10/10, Loss: 0.15476736009120942\n",
            "Epoch 10/10, Loss: 0.15660018610954285\n",
            "Epoch 10/10, Loss: 0.1578704788684845\n",
            "Epoch 10/10, Loss: 0.15959011495113373\n",
            "Epoch 10/10, Loss: 0.16095951473712922\n",
            "Epoch 10/10, Loss: 0.16226248228549958\n",
            "Epoch 10/10, Loss: 0.16380955851078033\n",
            "Epoch 10/10, Loss: 0.1653991541862488\n",
            "Epoch 10/10, Loss: 0.1670240262746811\n",
            "Epoch 10/10, Loss: 0.16867434906959533\n",
            "Epoch 10/10, Loss: 0.17026572966575623\n",
            "Epoch 10/10, Loss: 0.17185452246665955\n",
            "Epoch 10/10, Loss: 0.17332201886177062\n",
            "Epoch 10/10, Loss: 0.17495034885406494\n",
            "Epoch 10/10, Loss: 0.17658647310733794\n",
            "Epoch 10/10, Loss: 0.17839383792877198\n",
            "Epoch 10/10, Loss: 0.18070037937164307\n",
            "Epoch 10/10, Loss: 0.1822241871356964\n",
            "Epoch 10/10, Loss: 0.18392277002334595\n",
            "Epoch 10/10, Loss: 0.1855243138074875\n",
            "Epoch 10/10, Loss: 0.18690541017055512\n",
            "Epoch 10/10, Loss: 0.18860528767108917\n",
            "Epoch 10/10, Loss: 0.19022571551799775\n",
            "Epoch 10/10, Loss: 0.19197942638397217\n",
            "Epoch 10/10, Loss: 0.1935740588903427\n",
            "Epoch 10/10, Loss: 0.19512792587280273\n",
            "Epoch 10/10, Loss: 0.19665540611743926\n",
            "Epoch 10/10, Loss: 0.19821319806575774\n",
            "Epoch 10/10, Loss: 0.2005605071783066\n",
            "Epoch 10/10, Loss: 0.2018966077566147\n",
            "Epoch 10/10, Loss: 0.20362408721446992\n",
            "Epoch 10/10, Loss: 0.20513935565948485\n",
            "Epoch 10/10, Loss: 0.2069847904443741\n",
            "Epoch 10/10, Loss: 0.20833961892127992\n",
            "Epoch 10/10, Loss: 0.20981448936462402\n",
            "Epoch 10/10, Loss: 0.2117950681447983\n",
            "Epoch 10/10, Loss: 0.21369660139083863\n",
            "Epoch 10/10, Loss: 0.21517039835453033\n",
            "Epoch 10/10, Loss: 0.2168170050382614\n",
            "Epoch 10/10, Loss: 0.21856617188453675\n",
            "Epoch 10/10, Loss: 0.22012862646579742\n",
            "Epoch 10/10, Loss: 0.2215191388130188\n",
            "Epoch 10/10, Loss: 0.22329790914058686\n",
            "Epoch 10/10, Loss: 0.22494390821456908\n",
            "Epoch 10/10, Loss: 0.22673491835594178\n",
            "Epoch 10/10, Loss: 0.2284122142791748\n",
            "Epoch 10/10, Loss: 0.23025026392936707\n",
            "Epoch 10/10, Loss: 0.23246876001358033\n",
            "Epoch 10/10, Loss: 0.2346982808113098\n",
            "Epoch 10/10, Loss: 0.23651571679115296\n",
            "Epoch 10/10, Loss: 0.23846549057960512\n",
            "Epoch 10/10, Loss: 0.24053030109405518\n",
            "Epoch 10/10, Loss: 0.24180621469020844\n",
            "Epoch 10/10, Loss: 0.2438098646402359\n",
            "Epoch 10/10, Loss: 0.24534008276462554\n",
            "Epoch 10/10, Loss: 0.24662814962863921\n",
            "Epoch 10/10, Loss: 0.24875401270389558\n",
            "Epoch 10/10, Loss: 0.25035905504226685\n",
            "Epoch 10/10, Loss: 0.2520292863845825\n",
            "Epoch 10/10, Loss: 0.2537539700269699\n",
            "Epoch 10/10, Loss: 0.2553069462776184\n",
            "Epoch 10/10, Loss: 0.2569051556587219\n",
            "Epoch 10/10, Loss: 0.25843142926692964\n",
            "Epoch 10/10, Loss: 0.2597854950428009\n",
            "Epoch 10/10, Loss: 0.2612630400657654\n",
            "Epoch 10/10, Loss: 0.26258937537670135\n",
            "Epoch 10/10, Loss: 0.2644122338294983\n",
            "Epoch 10/10, Loss: 0.26592859482765197\n",
            "Epoch 10/10, Loss: 0.26760815167427066\n",
            "Epoch 10/10, Loss: 0.2689594184160233\n",
            "Epoch 10/10, Loss: 0.27047794449329376\n",
            "Epoch 10/10, Loss: 0.27201620388031006\n",
            "Epoch 10/10, Loss: 0.2737281851768494\n",
            "Epoch 10/10, Loss: 0.2753099225759506\n",
            "Epoch 10/10, Loss: 0.27684225130081175\n",
            "Epoch 10/10, Loss: 0.2785999889373779\n",
            "Epoch 10/10, Loss: 0.28027024400234224\n",
            "Epoch 10/10, Loss: 0.2815544259548187\n",
            "Epoch 10/10, Loss: 0.28307318353652955\n",
            "Epoch 10/10, Loss: 0.28434910261631013\n",
            "Epoch 10/10, Loss: 0.2862716957330704\n",
            "Epoch 10/10, Loss: 0.2881431642770767\n",
            "Epoch 10/10, Loss: 0.2901159932613373\n",
            "Epoch 10/10, Loss: 0.2914328814744949\n",
            "Epoch 10/10, Loss: 0.29279514515399935\n",
            "Epoch 10/10, Loss: 0.2947105985879898\n",
            "Epoch 10/10, Loss: 0.29684625709056855\n",
            "Epoch 10/10, Loss: 0.298121720790863\n",
            "Epoch 10/10, Loss: 0.29991448867321013\n",
            "Epoch 10/10, Loss: 0.3026020704507828\n",
            "Epoch 10/10, Loss: 0.30403281903266904\n",
            "Epoch 10/10, Loss: 0.3055521354675293\n",
            "Epoch 10/10, Loss: 0.30747929668426516\n",
            "Epoch 10/10, Loss: 0.3095797019004822\n",
            "Epoch 10/10, Loss: 0.3110069634914398\n",
            "Epoch 10/10, Loss: 0.3131947944164276\n",
            "Epoch 10/10, Loss: 0.3154827525615692\n",
            "Epoch 10/10, Loss: 0.316947762966156\n",
            "Epoch 10/10, Loss: 0.3188491554260254\n",
            "Epoch 10/10, Loss: 0.3204263347387314\n",
            "Epoch 10/10, Loss: 0.3219961642026901\n",
            "Epoch 10/10, Loss: 0.32385672235488894\n",
            "Epoch 10/10, Loss: 0.3258889243602753\n",
            "Epoch 10/10, Loss: 0.32716846084594725\n",
            "Epoch 10/10, Loss: 0.3285771675109863\n",
            "Epoch 10/10, Loss: 0.33014908373355867\n",
            "Epoch 10/10, Loss: 0.33143455851078035\n",
            "Epoch 10/10, Loss: 0.3333467572927475\n",
            "Epoch 10/10, Loss: 0.3351167230606079\n",
            "Epoch 10/10, Loss: 0.33691253924369813\n",
            "Epoch 10/10, Loss: 0.3382428925037384\n",
            "Epoch 10/10, Loss: 0.33985075867176057\n",
            "Epoch 10/10, Loss: 0.3413645646572113\n",
            "Epoch 10/10, Loss: 0.34316443157196047\n",
            "Epoch 10/10, Loss: 0.3449287288188934\n",
            "Epoch 10/10, Loss: 0.3467477715015411\n",
            "Epoch 10/10, Loss: 0.3487610342502594\n",
            "Epoch 10/10, Loss: 0.3501142293214798\n",
            "Epoch 10/10, Loss: 0.35174289083480836\n",
            "Epoch 10/10, Loss: 0.35347383677959443\n",
            "Epoch 10/10, Loss: 0.3547945020198822\n",
            "Epoch 10/10, Loss: 0.3563103848695755\n",
            "Epoch 10/10, Loss: 0.3579212735891342\n",
            "Epoch 10/10, Loss: 0.3592949423789978\n",
            "Epoch 10/10, Loss: 0.3610849466323853\n",
            "Epoch 10/10, Loss: 0.36265300333499906\n",
            "Epoch 10/10, Loss: 0.36392112481594086\n",
            "Epoch 10/10, Loss: 0.3656389750242233\n",
            "Epoch 10/10, Loss: 0.36692653346061704\n",
            "Epoch 10/10, Loss: 0.36886246180534366\n",
            "Epoch 10/10, Loss: 0.3703072667121887\n",
            "Epoch 10/10, Loss: 0.3720783318281174\n",
            "Epoch 10/10, Loss: 0.3742853404283524\n",
            "Epoch 10/10, Loss: 0.37586965298652647\n",
            "Epoch 10/10, Loss: 0.37742232143878934\n",
            "Epoch 10/10, Loss: 0.3787358298301697\n",
            "Epoch 10/10, Loss: 0.38053693091869356\n",
            "Epoch 10/10, Loss: 0.3824498059749603\n",
            "Epoch 10/10, Loss: 0.3841385269165039\n",
            "Epoch 10/10, Loss: 0.3854479984045029\n",
            "Epoch 10/10, Loss: 0.38721777057647705\n",
            "Epoch 10/10, Loss: 0.3887401173114777\n",
            "Epoch 10/10, Loss: 0.39003177917003634\n",
            "Epoch 10/10, Loss: 0.39200763082504275\n",
            "Epoch 10/10, Loss: 0.3933550440073013\n",
            "Epoch 10/10, Loss: 0.3951143423318863\n",
            "Epoch 10/10, Loss: 0.397115376830101\n",
            "Epoch 10/10, Loss: 0.3989408903121948\n",
            "Epoch 10/10, Loss: 0.40021913290023803\n",
            "Epoch 10/10, Loss: 0.40179686844348905\n",
            "Epoch 10/10, Loss: 0.4037928169965744\n",
            "Epoch 10/10, Loss: 0.4050808359384537\n",
            "Epoch 10/10, Loss: 0.4066915855407715\n",
            "Epoch 10/10, Loss: 0.4079788905382156\n",
            "Epoch 10/10, Loss: 0.4095335865020752\n",
            "Epoch 10/10, Loss: 0.4112701526880264\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "epoch_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0\n",
        "  for batch_X, batch_y in trainloader:\n",
        "    optimizer.zero_grad()\n",
        "    predicted = model3(batch_X)                      # forward pass in batches\n",
        "    loss = criterion(predicted, batch_y)\n",
        "    loss.backward()                                  # gradient calculation with help of loss\n",
        "    optimizer.step()                                 # weight update\n",
        "\n",
        "    # Evaluation example\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # After processing all batches, calculate average loss for the epoch\n",
        "    average_loss = train_loss / len(trainloader)\n",
        "    epoch_loss.append(average_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {average_loss}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXqHy3ITUOYv"
      },
      "source": [
        "## Evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "xrrhY9grULl3"
      },
      "outputs": [],
      "source": [
        "# save the trained model\n",
        "PATH = './mymodel.pth'\n",
        "torch.save(model3.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iubpMiwURAw",
        "outputId": "5e607dd7-a247-451e-eaf2-a21fbc55ec3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the saved model\n",
        "model3 = Network()\n",
        "model3.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0KMzK5zqUSZe"
      },
      "outputs": [],
      "source": [
        "testdata = CustomDataset(X_test, y_test)\n",
        "\n",
        "testloader = DataLoader(testdata, batch_size=4, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWDCRQCaUTq6",
        "outputId": "ab79a2fa-eeb8-418f-de6d-11b0a76e66a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([ 0.6102, -1.8529, -0.3723,  0.7270,  0.7007,  0.6258, -1.2360,  1.4126,\n",
            "        -3.3738, -1.0379]), tensor(0))\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f220d01b1f0>\n"
          ]
        }
      ],
      "source": [
        "print(testdata[5])\n",
        "print(testloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "J54sO3oYUWXU"
      },
      "outputs": [],
      "source": [
        "# Function to perform inference\n",
        "def test_model(model, testloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for batch_X, batch_y in testloader:\n",
        "            outputs = model(batch_X)  # Perform forward pass\n",
        "            _, predicted = torch.max(outputs, 1)  # Max probability value discarded, index obtained in predicted. [since output = (probability, class), here \"class\" is the index]\n",
        "            total += batch_y.size(0)              # total no of samples so far\n",
        "            correct += (predicted == batch_y).sum().item()     # total correct prediction so far. [(predicted==batch_y) is a boolean tensor, .sums true value in tensor, .item() converts the sum to scalar to give total true values.\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECSXGFtaUX6_",
        "outputId": "d44edbc5-5d4d-452e-ca8e-ae05cdd89d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 88.30%\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "test_model(model3, testloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIZyNH-WUZIq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
